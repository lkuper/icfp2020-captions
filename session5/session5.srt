1
00:00:49,080 --> 00:00:54,080


2
00:04:20,920 --> 00:04:25,920


3
00:08:00,520 --> 00:08:05,520


4
00:13:01,920 --> 00:13:04,560
RICHARD EISENBERG: Hi everyone!

5
00:13:04,560 --> 00:13:08,160
Welcome to Technical
Session 5 of ICFP 2020.

6
00:13:08,160 --> 00:13:09,960
My name is Richard Eisenberg.

7
00:13:09,960 --> 00:13:13,000
I'm principal researcher
at Tweag I/O

8
00:13:13,000 --> 00:13:17,520
and a member of the ICFP
Program Committee.

9
00:13:17,520 --> 00:13:21,520
This session will be
streamed via YouTube

10
00:13:21,520 --> 00:13:24,600
but is also available via Clowdr.

11
00:13:24,600 --> 00:13:25,880
So, those of you who registered

12
00:13:25,880 --> 00:13:28,120
if you're on YouTube please
switch over to Clowdr.

13
00:13:28,120 --> 00:13:30,680
You should have gotten
a notification, an email

14
00:13:30,680 --> 00:13:32,120
how to do so.

15
00:13:32,120 --> 00:13:35,520
And that will give you access
to more interactive features

16
00:13:35,520 --> 00:13:37,440
of these presentations.

17
00:13:37,440 --> 00:13:42,840
Including links to the question
and answer sessions from the authors.

18
00:13:42,840 --> 00:13:44,720
So, first up in this session,

19
00:13:44,720 --> 00:13:47,120
we have Joe Cutler who will present

20
00:13:47,120 --> 00:13:50,400
how we can extract recurrent
relations from functions

21
00:13:50,400 --> 00:13:55,400
for amortized analysis.

22
00:13:55,880 --> 00:13:57,520
JOE CUTLER: Hi, I'm Joe Cutler.

23
00:13:57,520 --> 00:13:59,720
I'm an undergraduate at Wesleyan
University in Connecticut.

24
00:13:59,720 --> 00:14:01,880
And this is work with
my advisor Dan Licata

25
00:14:01,880 --> 00:14:03,920
and Norman Danner.

26
00:14:03,920 --> 00:14:06,200
So, if I handed you this
code for mergeSort

27
00:14:06,200 --> 00:14:07,840
and then asked you to
write down a recurrence

28
00:14:07,840 --> 00:14:09,880
for its worst-case execution cost,

29
00:14:09,880 --> 00:14:11,400
you'll probably end up writing down

30
00:14:11,400 --> 00:14:14,160
pretty much exactly this
expression right here.

31
00:14:14,160 --> 00:14:15,880
But the thing you did in
your head to get there

32
00:14:15,880 --> 00:14:18,400
is actually pretty subtle.

33
00:14:18,400 --> 00:14:21,760
It required knowing things about
the sizes of the results of functions.

34
00:14:21,760 --> 00:14:24,600
Like the fact that
split splits lists in half

35
00:14:24,600 --> 00:14:27,480
and abstracting list
down to just a length.

36
00:14:27,480 --> 00:14:30,040
And even more importantly
as a PL person,

37
00:14:30,040 --> 00:14:32,640
you might even have
an operational semantics

38
00:14:32,640 --> 00:14:34,600
for a language that this
function is written in,

39
00:14:34,600 --> 00:14:38,000
that precisely defines
the cost of program execution

40
00:14:38,000 --> 00:14:42,280
in terms of beta redexes
or some other cost metrics.

41
00:14:42,280 --> 00:14:44,520
And how do you know this
recurrence you wrote down

42
00:14:44,520 --> 00:14:48,200
has anything to do whatsoever
with those formal costs?

43
00:14:48,200 --> 00:14:49,760
So, in other words,

44
00:14:49,760 --> 00:14:54,280
how do we make this informal process
of recurrence extraction formal

45
00:14:54,280 --> 00:14:56,280
and ensure that it's correct?

46
00:14:56,280 --> 00:14:59,200
Well, at ICFP five years ago,

47
00:14:59,200 --> 00:15:02,120
the other two authors of this
paper and their co-author

48
00:15:02,120 --> 00:15:05,080
presented an answer to
exactly that question.

49
00:15:05,080 --> 00:15:07,280
And I guess rather than
keep you on suspense,

50
00:15:07,280 --> 00:15:10,160
I just show you what
they came up with.

51
00:15:10,160 --> 00:15:12,720
So, recurrence extraction
is formalized as

52
00:15:12,720 --> 00:15:14,960
a language-to-language translation.

53
00:15:14,960 --> 00:15:17,040
So, given the term in
some source language

54
00:15:17,040 --> 00:15:20,000
we can translate it into
a language that we call lambda C

55
00:15:20,000 --> 00:15:21,920
which is a language whose terms are

56
00:15:21,920 --> 00:15:24,480
syntactic recurrence relations.

57
00:15:24,480 --> 00:15:26,280
Now, the overall idea
of this translation

58
00:15:26,280 --> 00:15:29,160
which we often call formal
recurrence extraction

59
00:15:29,160 --> 00:15:33,200
is to translate terms to
output their evaluation cost

60
00:15:33,200 --> 00:15:35,360
along with the size of the result.

61
00:15:35,360 --> 00:15:39,280
So, a term of type A in a source
gets translated to a pair.

62
00:15:39,280 --> 00:15:41,600
Consisting of the cost
to evaluate that term

63
00:15:41,600 --> 00:15:43,600
along with the resulting value,

64
00:15:43,600 --> 00:15:46,000
the size of the resulting
value of type A.

65
00:15:46,000 --> 00:15:48,400
Now, this size translation
that I'm talking about

66
00:15:48,400 --> 00:15:50,960
is essentially
the identity on values

67
00:15:50,960 --> 00:15:53,240
but it shifts to a higher type

68
00:15:53,240 --> 00:15:56,920
where the size of a function is
a recurrence in a traditional sense.

69
00:15:56,920 --> 00:16:00,440
So, it's a function from
sizes to cost and sizes.

70
00:16:00,440 --> 00:16:02,600
Now, the real meat of this
recurrence extraction

71
00:16:02,600 --> 00:16:05,800
is the term translation which
is defined compositionally

72
00:16:05,800 --> 00:16:08,120
by recursion over raw syntax.

73
00:16:08,120 --> 00:16:09,640
So, given a term in the source,

74
00:16:09,640 --> 00:16:12,600
we can just translate it
to a term in lambda C.

75
00:16:12,600 --> 00:16:15,240
Now, if you're
a monadically minded person,

76
00:16:15,240 --> 00:16:17,600
you might recognize
all of this as being

77
00:16:17,600 --> 00:16:20,040
a monadic translation
into the writer monad.

78
00:16:20,040 --> 00:16:22,040
And all of this helps
motivate the definition

79
00:16:22,040 --> 00:16:23,480
of a size translation.

80
00:16:23,480 --> 00:16:26,000
Arrows in the source become
Kleisli arrows in lambda C.

81
00:16:27,360 --> 00:16:29,040
So we've extracted
recurrences

82
00:16:29,040 --> 00:16:30,800
from the source language
into lambda C.

83
00:16:30,800 --> 00:16:34,280
And now we need to prove that
this extraction procedure is correct.

84
00:16:34,280 --> 00:16:35,760
Otherwise all of this is no better

85
00:16:35,760 --> 00:16:38,840
than just extracting recurrences
the old informal way.

86
00:16:39,640 --> 00:16:42,800
So, the first step here is to fix
an operational semantics

87
00:16:42,800 --> 00:16:45,920
from the source language which
is indexed by execution cost.

88
00:16:45,920 --> 00:16:49,880
You can read this judgment as
M evaluates in n steps to V.

89
00:16:50,680 --> 00:16:54,040
And then we prove this following
theorem using a logical relation.

90
00:16:54,040 --> 00:16:59,000
So, for any closed term m of type A,
if m evaluates in n steps to v,

91
00:16:59,000 --> 00:17:03,280
then the extracted cost of running
M is an upper bound on n,

92
00:17:03,280 --> 00:17:06,880
and also the usual logical relations
value relation nonsense.

93
00:17:06,880 --> 00:17:10,280
But anyway, this theorem is the
crucial step, it formally connects

94
00:17:10,280 --> 00:17:13,640
the operational cost model
to the extracted recurrences.

95
00:17:14,880 --> 00:17:17,520
One important piece of the
informal analysis

96
00:17:17,520 --> 00:17:20,800
that this translation doesn't take
into account is size abstraction,

97
00:17:20,800 --> 00:17:24,000
like say, analyzing a function
which takes a list in terms

98
00:17:24,000 --> 00:17:27,120
of its length or a tree function
by depth or something like that.

99
00:17:27,120 --> 00:17:30,440
Now, we do this abstraction
using a denotational semantics

100
00:17:30,440 --> 00:17:32,520
from lambda c into
partially ordered sets.

101
00:17:32,520 --> 00:17:35,000
And this means that the
objects that we get out

102
00:17:35,000 --> 00:17:38,040
are the standard objects you'd
recognize as recurrences.

103
00:17:38,040 --> 00:17:41,400
You can compute closed forms and
asymptotics and stuff like that.

104
00:17:42,160 --> 00:17:45,120
OK, that was a lot of technical
detail, I acknowledge that

105
00:17:45,120 --> 00:17:48,040
but luckily all this has all
been done before.

106
00:17:48,040 --> 00:17:50,240
And in the year since this
technique was developed,

107
00:17:50,240 --> 00:17:53,080
it's been employed to formalize
recurrence extraction

108
00:17:53,080 --> 00:17:55,880
for increasingly realistic languages
from the simply typed

109
00:17:55,880 --> 00:17:59,600
lambda calculus to inductive
types all the way up to PCF

110
00:17:59,600 --> 00:18:03,760
and ML-style let polymorphism.
But all this prior work

111
00:18:03,760 --> 00:18:06,760
is limited in the kinds of analyses
they can handle.

112
00:18:06,760 --> 00:18:10,280
Only ever worst case cost
analysis and in many cases,

113
00:18:10,280 --> 00:18:12,760
worst case cost analysis
simply isn't enough

114
00:18:12,760 --> 00:18:15,840
to get tight bounds, we,
and a technique like say,

115
00:18:15,840 --> 00:18:19,280
amortized analysis is required
to get better bounds.

116
00:18:19,280 --> 00:18:22,360
And so in this work we extend
the recurrence extraction technique

117
00:18:22,360 --> 00:18:24,520
to handle amortized analysis.

118
00:18:25,560 --> 00:18:28,880
So, in the paper we go over
two standard examples

119
00:18:28,880 --> 00:18:32,080
of amortized analysis to
demonstrate the effectiveness

120
00:18:32,080 --> 00:18:34,920
of our technique. And the first
is a simple binary counter

121
00:18:34,920 --> 00:18:37,880
and the second is splay trees
as presented in Okasaki.

122
00:18:38,640 --> 00:18:41,760
The splay trees are notable because
they aren't really well handled

123
00:18:41,760 --> 00:18:44,040
by automated amortized
analysis techniques

124
00:18:44,040 --> 00:18:45,760
such as resource aware ML.

125
00:18:45,760 --> 00:18:50,040
And since the analysis of splay trees
is really complicated,

126
00:18:50,040 --> 00:18:51,640
I only have like
10 more minutes.

127
00:18:52,280 --> 00:18:55,200
I'll use the binary counter as
a running example to motivate the way

128
00:18:55,200 --> 00:18:57,680
that we made our papers
main contribution.

129
00:18:57,680 --> 00:19:01,000
So, consider a binary number
represented as a list of bits

130
00:19:01,000 --> 00:19:03,360
with the least significant
bit at the head,

131
00:19:03,360 --> 00:19:05,920
we can define an increment function
in the standard way.

132
00:19:05,920 --> 00:19:07,880
Note that if the least
significant bit is zero,

133
00:19:08,600 --> 00:19:09,760
the increment is
constant time

134
00:19:09,760 --> 00:19:11,200
but if the least
significant bit is one,

135
00:19:11,200 --> 00:19:13,280
we have to make a recursive
call down the tail.

136
00:19:14,400 --> 00:19:16,480
Now we can define a set function
which just sets the counter

137
00:19:16,480 --> 00:19:19,720
to a natural number value by
looping increments end times.

138
00:19:20,240 --> 00:19:22,560
So, here are the recurrences
that you'd expect to get

139
00:19:22,560 --> 00:19:24,880
from these functions using
the informal method.

140
00:19:24,880 --> 00:19:27,480
By the way, for the rest of the
talk will fix the cost model,

141
00:19:27,480 --> 00:19:30,360
the cons operations are
the only cost the operation,

142
00:19:30,360 --> 00:19:33,800
in each incurring costs.
So, these two recurrences here solve

143
00:19:33,800 --> 00:19:37,240
to T inc being linear,
and T set being n log n.

144
00:19:37,760 --> 00:19:41,880
But this actually isn't a tight bound
for set,it's actually linear.

145
00:19:41,880 --> 00:19:44,600
Intuitively, this is because
the constant time case

146
00:19:44,600 --> 00:19:47,680
of the increment happens way
more often while you're looping.

147
00:19:47,680 --> 00:19:50,440
But you have to take the max between
the two non empty branches

148
00:19:50,440 --> 00:19:53,240
when you extract the recurrence,
which makes it look like

149
00:19:53,240 --> 00:19:56,240
increment traverses the
whole list every single time.

150
00:19:56,240 --> 00:19:59,000
And unfortunately, this is exactly
what the prior work in

151
00:19:59,000 --> 00:20:01,720
formal recurrence extraction
would do to this example.

152
00:20:01,720 --> 00:20:03,240
So, motivate some
stuff down the line,

153
00:20:03,240 --> 00:20:04,920
let's take a look at
how that would go.

154
00:20:05,800 --> 00:20:07,720
So, we can take our increment
function written

155
00:20:07,720 --> 00:20:09,560
in the source language
and pass it through

156
00:20:09,560 --> 00:20:11,800
the recurrence extraction
translation to get a function

157
00:20:11,800 --> 00:20:14,960
into a pair c cross which
computes the cost of running

158
00:20:14,960 --> 00:20:16,680
an increment along with
the size of the result.

159
00:20:16,680 --> 00:20:19,680
Now, the first component of
this pair is the important one,

160
00:20:19,680 --> 00:20:23,280
it's the execution cost. So I'll
just write down the subscripts c.

161
00:20:23,280 --> 00:20:25,440
Now this syntactic
recurrence that you get out

162
00:20:25,440 --> 00:20:28,600
is exactly what I hope
You'd expect. The key cases,

163
00:20:28,600 --> 00:20:31,400
when the least significant bit
is one,it costs one for the cons

164
00:20:31,400 --> 00:20:33,240
plus the cost of
recursing down the tail.

165
00:20:33,960 --> 00:20:36,680
Now, after this, we use
the denotational semantics

166
00:20:36,680 --> 00:20:39,840
to interpret this recurrence
as a map between Posets.

167
00:20:39,840 --> 00:20:43,080
Most importantly, we interpret the
bit list type as the natural numbers

168
00:20:43,080 --> 00:20:46,240
which abstracts a list to its
length in the traditional way.

169
00:20:46,760 --> 00:20:50,120
Now the monotone maps of
posets we get in the semantics

170
00:20:50,120 --> 00:20:52,200
represent the exact
same loose bounds

171
00:20:52,200 --> 00:20:53,480
that we talked about before .

172
00:20:54,240 --> 00:20:56,800
If you carry it all through
for set you still get O(n log n).

173
00:20:57,400 --> 00:21:00,400
So, we've seen how the
previous work comes up short.

174
00:21:00,400 --> 00:21:02,520
Now let's take a look at how
the amortized analysis

175
00:21:02,520 --> 00:21:06,160
of the binary counter actually
should work to help inform

176
00:21:06,160 --> 00:21:08,080
how to change the
recurrence extraction

177
00:21:08,080 --> 00:21:10,440
to get the tight amortized
bounds we are looking for.

178
00:21:11,400 --> 00:21:13,960
So, here's a quick refresher
on amortized analysis

179
00:21:13,960 --> 00:21:16,600
for those of you who've forgotten
about what the big deal was

180
00:21:16,600 --> 00:21:17,920
since you last
learned about it.

181
00:21:17,920 --> 00:21:20,560
So, amortized analysis comes
in a few different forms

182
00:21:20,560 --> 00:21:24,120
but all of them boil down
to essentially smoothing out cost

183
00:21:24,120 --> 00:21:26,080
across program execution.

184
00:21:26,080 --> 00:21:29,320
So, that when you analyse repeated
executions of the same function,

185
00:21:29,320 --> 00:21:32,200
they don't all look like
the most costly one

186
00:21:32,200 --> 00:21:34,960
and instead look like say
the average one.

187
00:21:34,960 --> 00:21:38,000
And the approach to amortized
analysis that we take in the paper,

188
00:21:38,480 --> 00:21:41,600
which is usually called the bankers
method is to maintain

189
00:21:41,600 --> 00:21:45,280
an invariant, the imaginary credits
on the nodes of a data structure.

190
00:21:45,280 --> 00:21:48,240
So, these credits
incur costs to create,

191
00:21:48,240 --> 00:21:50,840
but then you can spend
them to decrease cost.

192
00:21:50,840 --> 00:21:53,040
And by passing these credits
around the program,

193
00:21:53,040 --> 00:21:54,680
you can rearrange costs.

194
00:21:55,320 --> 00:21:57,720
The credit invariant for
the binary counter is that

195
00:21:57,720 --> 00:22:01,760
we keep one credit on each
cons cell, which holds a one bit.

196
00:22:02,240 --> 00:22:04,400
So, here's an example to
illustrate how this works.

197
00:22:04,400 --> 00:22:06,480
We start with an empty count
with no credits.

198
00:22:06,480 --> 00:22:08,800
And then the first increment
flips the first bit

199
00:22:08,800 --> 00:22:11,880
and we spawn a credit to put on the
one, total amortized cost here

200
00:22:11,880 --> 00:22:15,640
is one cons plus one created
credit equals two amortized cost.

201
00:22:15,640 --> 00:22:18,880
And then the next call to
increment flips the first bit

202
00:22:18,880 --> 00:22:21,120
from one to zero and the
second bit from zero to one.

203
00:22:21,120 --> 00:22:23,680
And then to reestablish the credit
invariant we spawn a credit

204
00:22:23,680 --> 00:22:26,080
to put on the new one bit,
which incurs a cost of one

205
00:22:26,080 --> 00:22:29,040
but then that's canceled out by
spending the credit which used

206
00:22:29,040 --> 00:22:32,440
to be on the least significant bit.
Again total two amortized costs.

207
00:22:33,480 --> 00:22:36,200
This is the essence of the analysis
and you can convince yourself

208
00:22:36,200 --> 00:22:40,000
that in every case, the amortized
cost of increment is two.

209
00:22:40,000 --> 00:22:43,920
So, in order to do this kind of
analysis by recurrence extraction,

210
00:22:43,920 --> 00:22:46,040
we need to make
these credit policies

211
00:22:46,040 --> 00:22:48,480
expressible in the
source language.

212
00:22:48,480 --> 00:22:50,920
So, to that end, we extend
our source language

213
00:22:50,920 --> 00:22:53,640
with credits to make a language
that we called lambda A.

214
00:22:54,280 --> 00:22:57,520
So, lambda A internalizes
everything you need to

215
00:22:57,520 --> 00:22:59,840
express credit invariants
in your code,

216
00:22:59,840 --> 00:23:01,680
and it comes in
three main parts.

217
00:23:01,680 --> 00:23:03,840
So, first we need
to be able to track

218
00:23:03,840 --> 00:23:05,800
the credits using
the type system.

219
00:23:05,800 --> 00:23:09,480
So, this new typing judgment
says that m is a term of Type A

220
00:23:09,480 --> 00:23:12,760
and context gamma, which
can use up to C credits.

221
00:23:12,760 --> 00:23:15,520
Now, since we don't want
credits to be duplicated

222
00:23:15,520 --> 00:23:18,520
and then the same credit
spent twice, the type system

223
00:23:18,520 --> 00:23:22,280
is Affine which means that variables
can be used at most once.

224
00:23:22,280 --> 00:23:25,680
So, next we need to be able
to attach these credits

225
00:23:25,680 --> 00:23:28,040
to terms like say
the elements of a list.

226
00:23:28,040 --> 00:23:31,200
And to do this we add a
modality to the language.

227
00:23:31,200 --> 00:23:35,480
The terms of type bang, cA,
are morally terms of Type A

228
00:23:35,480 --> 00:23:37,760
that carry c extra credits.

229
00:23:37,760 --> 00:23:40,440
And the intro/elim rules
for this modality

230
00:23:40,440 --> 00:23:41,720
are actually pretty intuitive.

231
00:23:41,720 --> 00:23:44,640
The intro rule says that
we have a term m of Type A

232
00:23:44,640 --> 00:23:46,800
and we have C Extra
Credits lying around,

233
00:23:46,800 --> 00:23:50,440
and we don't need to construct,
that we don't need to construct m,

234
00:23:50,440 --> 00:23:54,320
we can just attach them to M
and get a term type bang cA.

235
00:23:54,320 --> 00:23:57,600
And the elim rule says that if
we have a term of type bang cA,

236
00:23:57,600 --> 00:24:00,240
we can peel the credits off
the term and allow them

237
00:24:00,240 --> 00:24:04,320
and the value to be used in
a continuation to make more stuff.

238
00:24:04,320 --> 00:24:07,240
So, finally, we need the ability
to create and spend credits,

239
00:24:07,240 --> 00:24:10,560
of course, and the create rule
here says that if we have a term

240
00:24:10,560 --> 00:24:13,880
which uses a plus c credits,
we can simply create C of them

241
00:24:13,880 --> 00:24:17,600
and duly the spend rule says
that if we have A plus c credits,

242
00:24:17,600 --> 00:24:19,880
we can spend c of them
and then use a term

243
00:24:19,880 --> 00:24:21,520
which only requires
A credits.

244
00:24:21,520 --> 00:24:25,000
Now, all of these changes I just
described are completely local

245
00:24:25,000 --> 00:24:27,800
to lambda A. We don't change
the recurrence language at all,

246
00:24:27,800 --> 00:24:30,680
it's still fully structural
and has no modality.

247
00:24:30,680 --> 00:24:33,400
So, we've extended the source
language with the features

248
00:24:33,400 --> 00:24:36,880
that let us formally specify these
credit invariants in our code.

249
00:24:36,880 --> 00:24:39,120
Now let's take a look at how
the binary counter example

250
00:24:39,120 --> 00:24:41,240
would actually get
encoded into this language.

251
00:24:42,000 --> 00:24:46,600
So, in lambda a, we redefine the
bit type to be unit plus bang one

252
00:24:46,600 --> 00:24:50,040
unit, a bit is either in-left of
a unit which represents a zero,

253
00:24:50,040 --> 00:24:52,560
or in-right of a unit
with one credit attached,

254
00:24:52,560 --> 00:24:55,440
which represents one. And
then the code for increment

255
00:24:55,440 --> 00:24:57,200
is the exact same
behavior as before,

256
00:24:57,200 --> 00:25:01,040
it just maintains the credit policy.
If the list is empty or has zeros,

257
00:25:01,040 --> 00:25:03,800
in the least significant bit,
we create a credit and make

258
00:25:03,800 --> 00:25:06,440
a one bit value by attaching
that credit to a unit value

259
00:25:06,440 --> 00:25:09,240
and then returning that
as the least significant bit.

260
00:25:09,240 --> 00:25:11,480
And it's the least significant bit
as one, we detach

261
00:25:11,480 --> 00:25:15,200
the credit using transfer, spend it
and then make a recursive call

262
00:25:15,200 --> 00:25:17,440
on the tail and cons as zero
one to the result.

263
00:25:17,440 --> 00:25:21,080
So, all good lambda A can
express the credit policy

264
00:25:21,080 --> 00:25:22,800
that we were looking for,
but now we need

265
00:25:22,800 --> 00:25:24,880
to actually extract
recurrences from it.

266
00:25:26,040 --> 00:25:28,800
But the recurrence extraction for
lambda A is actually quite simple.

267
00:25:28,800 --> 00:25:31,400
So, let's briefly recall that
the recurrence extraction

268
00:25:31,400 --> 00:25:35,000
from a term consists of a pair
of the cost to compute

269
00:25:35,000 --> 00:25:36,640
the term along with its size.

270
00:25:36,640 --> 00:25:40,160
So, since creating A credits
incurs a cost of A

271
00:25:40,160 --> 00:25:44,000
just add A to the extracted cost
and dually we subtract A

272
00:25:44,000 --> 00:25:45,720
from the cost
component when we spent.

273
00:25:46,240 --> 00:25:49,960
Now, since the credits as
I mentioned earlier aren't present

274
00:25:49,960 --> 00:25:52,680
in the recurrence language,
the extraction has to erase

275
00:25:52,680 --> 00:25:54,720
the modality and its
corresponding form.

276
00:25:54,720 --> 00:25:58,360
So, save gets erased completely and
transfer gets turned into a let.

277
00:25:59,440 --> 00:26:01,920
All good. So, now that we have
the recurrence extraction

278
00:26:01,920 --> 00:26:05,400
in place for lambda a,
let's extract terms recurrences

279
00:26:05,400 --> 00:26:07,960
for the binary counter,
solve them and hopefully

280
00:26:07,960 --> 00:26:09,600
get the tight boundary
we're looking for.

281
00:26:10,320 --> 00:26:13,400
So, above line here is the term
for inc that we wrote earlier

282
00:26:13,400 --> 00:26:15,960
and below I've written the cost
component of the recurrence

283
00:26:15,960 --> 00:26:18,400
you get in lambda C
for this term.

284
00:26:18,400 --> 00:26:20,760
So, when the argument to
this recurrence is empty,

285
00:26:20,760 --> 00:26:22,800
or has zero is the least
significant bit,

286
00:26:22,800 --> 00:26:26,240
increment costs two, one from
the cons one from creating credit

287
00:26:26,240 --> 00:26:29,680
When is the least significant bit
one, the cost we extract is simply

288
00:26:29,680 --> 00:26:31,760
the cost of the
recursive call.

289
00:26:31,760 --> 00:26:34,600
Now, why is this?
Well, we do one cons operation

290
00:26:34,600 --> 00:26:37,000
which costs one and
then we spend a credit

291
00:26:37,000 --> 00:26:39,640
from the least significant bit,
which subtracts one.

292
00:26:39,640 --> 00:26:42,920
So, we're just left with the cost
of the recursive call down the tail.

293
00:26:42,920 --> 00:26:46,000
It's super easy to see that this
function is two everywhere.

294
00:26:46,000 --> 00:26:48,680
And indeed the monotone map
of posets that it denotes

295
00:26:48,680 --> 00:26:51,000
is in fact the constant
function two.

296
00:26:51,000 --> 00:26:53,040
So, carrying this
all through for set,

297
00:26:53,040 --> 00:26:55,880
we get to the corresponding
mathematical recurrence

298
00:26:55,880 --> 00:27:00,640
is order n as desired. So we have
a recurrence extraction procedure

299
00:27:00,640 --> 00:27:03,200
that extracts tight bounds
for amortized analyses.

300
00:27:03,200 --> 00:27:05,600
And the final step here
is to prove a theorem

301
00:27:05,600 --> 00:27:07,360
like the bounding theorem
for prior work

302
00:27:07,360 --> 00:27:10,360
which connects these recurrences
to the operational semantics.

303
00:27:10,880 --> 00:27:13,120
And similar to how we
did it in prior work.

304
00:27:13,120 --> 00:27:16,560
We first define a big step
operational semantics for lambda a,

305
00:27:16,560 --> 00:27:20,080
but this time indexed by amortized
cost. So, this judgment reads

306
00:27:20,080 --> 00:27:24,160
that M evaluates to V
with an amortized cost.

307
00:27:24,160 --> 00:27:27,000
Then we use a logical relation to
prove the same bounding theorem

308
00:27:27,000 --> 00:27:30,640
as before but this time the cost
per bounding is an amortized cost.

309
00:27:31,200 --> 00:27:33,160
But the crucial consequence
of this theorem

310
00:27:33,160 --> 00:27:36,360
is that for closed terms, the
amortized cost that we get

311
00:27:36,360 --> 00:27:39,920
for the recurrence is a bound on
the actual execution cost.

312
00:27:39,920 --> 00:27:42,800
And so for example, the set
program isn't just linear

313
00:27:42,800 --> 00:27:45,400
in this strange cost metric
where we can subtract

314
00:27:45,400 --> 00:27:46,840
and add arbitrary costs.

315
00:27:46,840 --> 00:27:50,000
It actually does a linear amount
of cons operations.

316
00:27:50,520 --> 00:27:52,640
So, in summary,
we formalize the process

317
00:27:52,640 --> 00:27:55,480
of doing amortized analysis
by recurrence extraction.

318
00:27:55,480 --> 00:27:57,600
Given an amortized analysis
of a data structure

319
00:27:57,600 --> 00:28:00,760
such as a binary counter
or splay tree written in lambda a,

320
00:28:00,760 --> 00:28:04,520
we can automatically extract
correct recurrences into lambda c.

321
00:28:04,520 --> 00:28:06,880
This extraction procedure
has been proven correct

322
00:28:06,880 --> 00:28:08,440
and is expressive
enough to handle

323
00:28:08,440 --> 00:28:11,040
non trivial analyses
like splay trees.

324
00:28:11,040 --> 00:28:12,320
Thank you very much.

325
00:28:12,840 --> 00:28:20,960
AUDIENCE: Applause

326
00:28:20,960 --> 00:28:22,360
RICHARD: Thanks, Joe.

327
00:28:22,360 --> 00:28:26,440
Now in both the New York
and Asia time bands.

328
00:28:26,440 --> 00:28:30,480
You should be able to click a
link in the sidebar in Clowdr

329
00:28:30,480 --> 00:28:32,760
to be able to access
the Q&A sessions.

330
00:28:38,600 --> 00:28:42,920
OK, our next talk is featuring
Arthur Chargueraud

331
00:28:42,920 --> 00:28:46,080
about separation logic for
sequential programs.

332
00:28:49,040 --> 00:28:50,080
ARTHUR CHARGUERAUD: Separation logic

333
00:28:50,080 --> 00:28:52,120
was introduced in
the early 2000s.

334
00:28:52,120 --> 00:28:55,760
Since then, it has had tremendous
success at program verification.

335
00:28:56,880 --> 00:29:00,400
I believe that today, the ideas
of separation logic deserve

336
00:29:00,400 --> 00:29:02,240
to be taught much more widely.

337
00:29:03,760 --> 00:29:06,680
For this reason, I've been working
on a course on separation logic.

338
00:29:08,160 --> 00:29:10,680
The course focuses on
sequential programs.

339
00:29:10,680 --> 00:29:12,480
Concurrency is interesting
but it's much harder.

340
00:29:13,520 --> 00:29:17,520
Course is written in the style of
the Software Foundations series,

341
00:29:17,520 --> 00:29:19,760
where every definition
every lemma,

342
00:29:19,760 --> 00:29:23,200
and every exercise is formalized
using the Coq proof assistant.

343
00:29:24,800 --> 00:29:27,720
The course covers modern
features of Separation logic.

344
00:29:27,720 --> 00:29:30,600
These are features that will not
present in the original papers

345
00:29:30,600 --> 00:29:34,000
in separation logic, but have
since then proved very useful

346
00:29:34,000 --> 00:29:38,400
for developing practical tools.
The third contribution of this paper

347
00:29:38,400 --> 00:29:40,400
is an extensive
related work section,

348
00:29:40,880 --> 00:29:44,200
which I believe nicely completes
'Peter O'Hearn's' survey,

349
00:29:44,200 --> 00:29:47,280
which was published last year
by focusing specifically

350
00:29:47,280 --> 00:29:51,960
on the contributions to make a nice
presentations of separation logic.

351
00:29:54,000 --> 00:29:55,920
The course is organized
in 10 chapters.

352
00:29:56,400 --> 00:29:58,120
So, the first three
chapters focus on

353
00:29:58,120 --> 00:30:00,160
basic features of
separation logic,

354
00:30:00,160 --> 00:30:03,320
such as heap predicates,
triples, entailment.

355
00:30:04,720 --> 00:30:05,840
The next few chapters focus

356
00:30:05,840 --> 00:30:07,880
on the presentation
of reasoning rules,

357
00:30:07,880 --> 00:30:11,040
either in the form of triples in the
form of weakest preconditions,

358
00:30:11,040 --> 00:30:13,400
or in the form of
characteristic formulae,

359
00:30:13,400 --> 00:30:15,800
which are a form of weakest
precondition generator.

360
00:30:16,760 --> 00:30:19,680
The remaining chapters focus
on more advanced features

361
00:30:19,680 --> 00:30:22,840
such as a magic wand,
the treatment of affine predicates,

362
00:30:22,840 --> 00:30:27,240
and language extensions,
such as records, arrays

363
00:30:27,240 --> 00:30:29,800
loops or n-ary functions.

364
00:30:29,800 --> 00:30:32,160
Let me take a few minutes
to explain how the course

365
00:30:32,160 --> 00:30:33,640
is set up at a
very high level.

366
00:30:35,600 --> 00:30:38,840
Traditional courses on separation
logic consider while loop language.

367
00:30:40,160 --> 00:30:42,680
I consider instead like other
colleagues before me

368
00:30:42,680 --> 00:30:44,120
an imperative
lambda calculus.

369
00:30:45,200 --> 00:30:47,760
This choice leads to
major simplification.

370
00:30:47,760 --> 00:30:50,160
The fact that there are
no mutable variables lead

371
00:30:50,160 --> 00:30:52,000
to an elegant statement
of the frame rule

372
00:30:52,000 --> 00:30:53,240
without any side-condition.

373
00:30:54,600 --> 00:30:57,480
At the same time, using
lambda calculus means

374
00:30:57,480 --> 00:30:59,080
that every term
produces a value

375
00:30:59,080 --> 00:31:01,280
and this leads to a
minor complication.

376
00:31:01,840 --> 00:31:04,000
The fact that post conditions
need to describe

377
00:31:04,000 --> 00:31:06,160
not just an output state
but also an output value.

378
00:31:07,480 --> 00:31:11,000
Quickly, where preconditions
are the type of heap to prop

379
00:31:11,480 --> 00:31:14,600
post conditions of
type value to heap to prop.

380
00:31:15,760 --> 00:31:19,640
These set of operators such as the
star, the magic wand or entailment,

381
00:31:19,640 --> 00:31:23,640
this needs to be extended to a form
that operates on post condition.

382
00:31:25,160 --> 00:31:28,880
These are marked with a little
dot symbol below the operator.

383
00:31:31,720 --> 00:31:37,080
The semantics is described in the
standard way as a call by value,

384
00:31:37,080 --> 00:31:39,680
substitution based,
big step style semantics.

385
00:31:40,600 --> 00:31:43,080
The choice of a big step semantics
is well suited for reasoning

386
00:31:43,080 --> 00:31:45,680
about total correctness
of sequential programs.

387
00:31:45,680 --> 00:31:47,040
It makes the proof simpler.

388
00:31:47,840 --> 00:31:50,360
Both the syntax and
the semantics are described

389
00:31:50,360 --> 00:31:53,240
in standard ways following
the style of the presentation

390
00:31:53,240 --> 00:31:55,400
of the prior Software
Foundation volumes,

391
00:31:55,400 --> 00:31:57,440
which will make it easy
for students to follow.

392
00:32:00,040 --> 00:32:02,240
Separation logic is presented
in a shallow embedding.

393
00:32:03,480 --> 00:32:06,760
This means that the core heap
predicates of separation logic

394
00:32:06,760 --> 00:32:11,320
are defined as Coq functions
from heap to propositions.

395
00:32:13,160 --> 00:32:15,240
Entailment is defined
in the standard way

396
00:32:15,240 --> 00:32:18,400
as simplest possible way
as pointwise entailment.

397
00:32:19,520 --> 00:32:23,040
The remaining operators
are encoded in terms of

398
00:32:23,040 --> 00:32:26,360
these core operators.
For the details

399
00:32:26,360 --> 00:32:27,840
I'll just refer
to the course.

400
00:32:30,640 --> 00:32:35,760
Triples are defined in two stage.
First, the hoarse Triple H t Q

401
00:32:35,760 --> 00:32:39,400
asserts that for any state as
satisfying the precondition,

402
00:32:39,400 --> 00:32:42,680
the term t terminates on a value V
and an open state S prime

403
00:32:42,680 --> 00:32:45,560
that together satisfy
the post condition Q.

404
00:32:47,080 --> 00:32:50,160
A separation logic
Triple H t Q is defined

405
00:32:50,160 --> 00:32:52,640
by quantifying universally
on a predicate H prime

406
00:32:53,480 --> 00:32:56,240
that describes
the rest of the world.

407
00:32:56,240 --> 00:33:00,080
And asserting that the triple
made of H stop H prime

408
00:33:00,080 --> 00:33:05,240
and Q-Star Q H prime makes
up a valid or triple.

409
00:33:05,240 --> 00:33:07,720
The standard technique of
baking in the frame rule

410
00:33:07,720 --> 00:33:10,040
leads to simple proofs
for the structural rules,

411
00:33:10,040 --> 00:33:15,480
such as a frame rule
or the consequence rule.

412
00:33:15,480 --> 00:33:16,760
I'd like to take
the rest of the talk

413
00:33:16,760 --> 00:33:21,400
to present four features of
modern separation logic,

414
00:33:21,400 --> 00:33:24,240
which I believe should
be to interest

415
00:33:24,240 --> 00:33:28,000
to every ICFP researcher.

416
00:33:28,000 --> 00:33:30,840
Let me begin with
a ramified frame rule.

417
00:33:30,840 --> 00:33:33,240
The frame rule in its
most standard statement

418
00:33:33,240 --> 00:33:34,920
is almost never applicable.

419
00:33:34,920 --> 00:33:37,200
Indeed, you need both
the precondition

420
00:33:37,200 --> 00:33:44,280
and the post-condition to
syntactically feature a star H2.

421
00:33:44,280 --> 00:33:48,240
But in practice, one would
typically use a combined rule

422
00:33:48,240 --> 00:33:51,240
that integrates
the rule of consequence,

423
00:33:51,240 --> 00:33:55,480
and is stated as follows;
to prove this triple H T Q

424
00:33:55,480 --> 00:33:58,400
derivable from a triple H1 T Q1.

425
00:33:58,400 --> 00:34:01,640
One has to show that
the precondition H

426
00:34:01,640 --> 00:34:06,480
decomposes as H1 star H2,

427
00:34:06,480 --> 00:34:08,480
and then show that
the post-condition Q1

428
00:34:08,480 --> 00:34:13,480
when extended with H2 recovers
the post-condition Q.

429
00:34:13,480 --> 00:34:18,480
H2 can be computed as
a difference between H and H1.

430
00:34:18,480 --> 00:34:21,040
Computing this difference
can be well automated

431
00:34:21,040 --> 00:34:24,720
in a simple case, but
if H contains existentially

432
00:34:24,720 --> 00:34:27,080
quantified variables,
it can be quite tricky

433
00:34:27,080 --> 00:34:29,160
to figure out whether these
corresponding variables

434
00:34:29,160 --> 00:34:33,240
should get quantified
in H1 or in H2.

435
00:34:33,240 --> 00:34:36,920
The ramified frame rule avoid
this problem altogether

436
00:34:36,920 --> 00:34:40,200
by removing the need to
introduce H2 in the first place.

437
00:34:40,200 --> 00:34:44,080
It reformulates the premises
as a single entailment.

438
00:34:44,080 --> 00:34:47,440
H entails H1 star.

439
00:34:47,440 --> 00:34:49,440
Some predicates, such that

440
00:34:49,440 --> 00:34:53,400
when augmenting Q1
with it, we obtain Q.

441
00:34:53,400 --> 00:34:55,880
The operator at play
here is the magic wand

442
00:34:55,880 --> 00:34:58,640
for post-condition.

443
00:34:58,640 --> 00:35:00,800
Without going into further
detail, let me just point out

444
00:35:00,800 --> 00:35:04,960
that the ramified frame rule
has proved very practical

445
00:35:04,960 --> 00:35:10,320
for developing practical tools.

446
00:35:10,320 --> 00:35:12,120
The second point I'd
like to focus on is;

447
00:35:12,120 --> 00:35:13,880
the weakest preconditioned
presentation

448
00:35:13,880 --> 00:35:15,720
of separation logic.

449
00:35:15,720 --> 00:35:17,320
Just like in our logic,
weakest precondition

450
00:35:17,320 --> 00:35:20,080
is a key ingredient, but what
is the definition of WP

451
00:35:20,080 --> 00:35:21,160
in separation logic?

452
00:35:21,160 --> 00:35:22,720
And what is a statement
of a frame rule

453
00:35:22,720 --> 00:35:26,000
in weakest precondition style?

454
00:35:26,000 --> 00:35:28,760
So WP, just like in our
logic, can be defined

455
00:35:28,760 --> 00:35:32,280
as an equivalence
between a triple H D Q

456
00:35:32,280 --> 00:35:37,040
and the entailment from H to WP T Q.

457
00:35:37,040 --> 00:35:39,760
This equivalence is not
quite a definition.

458
00:35:39,760 --> 00:35:44,680
The two definitions that
can be considered;

459
00:35:44,680 --> 00:35:48,560
one is a low level definition
working in terms of heaps.

460
00:35:48,560 --> 00:35:51,840
Another one is based on an encoding

461
00:35:51,840 --> 00:35:57,400
using the operators
of separation logic.

462
00:35:57,400 --> 00:36:01,920
The weakest precondition frame
rule can be read as follows;

463
00:36:01,920 --> 00:36:05,640
if I own a resource state
in which I can execute

464
00:36:05,640 --> 00:36:10,280
the term T and obtain
the post-condition Q

465
00:36:10,280 --> 00:36:14,200
and separately, I own a piece
of state described by H.

466
00:36:14,200 --> 00:36:17,680
Then altogether,
I own a piece of state

467
00:36:17,680 --> 00:36:19,400
in which the execution
of T terminates

468
00:36:19,400 --> 00:36:21,960
and produces
a post-condition described

469
00:36:21,960 --> 00:36:26,040
by Q extended with H.

470
00:36:26,040 --> 00:36:28,880
Interestingly, the weakest
precondition frame rule

471
00:36:28,880 --> 00:36:32,280
can be combined with the ideas
of the ramified frame rule,

472
00:36:32,280 --> 00:36:35,440
leading to the rule shown at
the bottom of the slide here.

473
00:36:35,440 --> 00:36:38,320
This rule subsumes all
the structural rules

474
00:36:38,320 --> 00:36:43,120
of separation logic.

475
00:36:43,120 --> 00:36:44,800
The third feature is
the mixing of affine

476
00:36:44,800 --> 00:36:46,720
and linear predicates.

477
00:36:46,720 --> 00:36:50,080
A linear predicate describes
a resource that must remain

478
00:36:50,080 --> 00:36:53,040
accounted for throughout
the reasoning.

479
00:36:53,040 --> 00:36:54,480
It is essential, for example,

480
00:36:54,480 --> 00:36:57,240
to prove that every allocated data

481
00:36:57,240 --> 00:36:59,440
eventually gets deallocated,
or every file open

482
00:36:59,440 --> 00:37:02,360
eventually gets closed.

483
00:37:02,360 --> 00:37:04,400
And the affine heap predicate
on the contrary,

484
00:37:04,400 --> 00:37:07,040
describes a resource that
may be freely discarded

485
00:37:07,040 --> 00:37:09,160
in the reasoning.

486
00:37:09,160 --> 00:37:14,800
Typically,this notion of affine predicate

487
00:37:14,800 --> 00:37:17,720
is a must have for garbage
collected languages,

488
00:37:17,720 --> 00:37:21,360
which do not feature
explicit free operations.

489
00:37:21,360 --> 00:37:23,360
So it is straightforward to set up

490
00:37:23,360 --> 00:37:26,000
a linear separation logic,
and it is not much harder

491
00:37:26,000 --> 00:37:28,560
to set up an affine
separation logic,

492
00:37:28,560 --> 00:37:31,280
but what is a simple way to
set up a separation logic,

493
00:37:31,280 --> 00:37:36,880
where both linear and affine
predicates can coexist?

494
00:37:36,880 --> 00:37:38,760
By describing
the (INAUDIBLE) construction

495
00:37:38,760 --> 00:37:40,920
that is relatively simple,

496
00:37:40,920 --> 00:37:44,400
which relies on
the introduction of a predicate

497
00:37:44,400 --> 00:37:46,960
H affine, that can be
customized to define

498
00:37:46,960 --> 00:37:52,320
which heaps should be considered
affine as opposed to linear.

499
00:37:52,320 --> 00:37:55,840
Top of that can define
the notion of affine predicates

500
00:37:55,840 --> 00:37:59,400
that characterize only
heaps that are affine.

501
00:37:59,400 --> 00:38:03,040
And the notion of
affine top predicate

502
00:38:03,040 --> 00:38:08,200
which characterizes any affine heap.

503
00:38:08,200 --> 00:38:10,320
The definition of
separation logic triples

504
00:38:10,320 --> 00:38:14,000
can be generalized by
introducing the top,

505
00:38:14,000 --> 00:38:17,680
the affine top predicate
in the post-condition.

506
00:38:17,680 --> 00:38:20,800
Doing so preserved
the validity of all the prior

507
00:38:20,800 --> 00:38:26,240
rules of separation logic
and adds two additional rules.

508
00:38:26,240 --> 00:38:28,160
The first one asserts that any piece

509
00:38:28,160 --> 00:38:31,080
of precondition H prime
may be discarded

510
00:38:31,080 --> 00:38:36,080
from the precondition
provided that it's affine.

511
00:38:36,080 --> 00:38:38,240
The second rule allows to
extend the post-condition

512
00:38:38,240 --> 00:38:41,400
within the affine top predicate
reflecting on the fact

513
00:38:41,400 --> 00:38:45,040
that it is fine to
produce a post-condition

514
00:38:45,040 --> 00:38:51,760
that is bigger than the desired one.

515
00:38:51,760 --> 00:38:54,440
Fourth and last feature
I'd like to describe

516
00:38:54,440 --> 00:38:58,800
is a frame friendly rule
for reasoning about loops.

517
00:38:58,800 --> 00:39:00,400
So consider for
example, an operation

518
00:39:00,400 --> 00:39:02,480
that traverses a linked list,

519
00:39:02,480 --> 00:39:06,120
or traverses a tree data
structure recursively.

520
00:39:06,120 --> 00:39:10,200
If the separation is implemented
as a recursive function,

521
00:39:10,200 --> 00:39:14,000
one can invoke the frame rule
around the recursive calls

522
00:39:14,000 --> 00:39:18,240
to forget about the cells
or the parts of the trees

523
00:39:18,240 --> 00:39:21,080
that are passed by.

524
00:39:21,080 --> 00:39:23,680
On the contrary, if
the operation is implemented

525
00:39:23,680 --> 00:39:27,120
as a loop and specified using
a loop invariant,

526
00:39:27,120 --> 00:39:31,960
the loop invariant must describe
the list segment

527
00:39:31,960 --> 00:39:35,360
of cells that have already
been passed by or worse,

528
00:39:35,360 --> 00:39:38,680
in the case of a tree, must
describe the tree context

529
00:39:38,680 --> 00:39:43,040
associated with the past that
has already been traversed.

530
00:39:43,040 --> 00:39:45,880
The question here is how can
we reason about syntactic

531
00:39:45,880 --> 00:39:48,640
loop construct as easily

532
00:39:48,640 --> 00:39:52,040
as recursive function

533
00:39:52,040 --> 00:39:56,440
that will allow us to
exploit the frame rule?

534
00:39:56,440 --> 00:39:58,560
So what we're trying to achieve here

535
00:39:58,560 --> 00:40:02,680
is essentially simulate
an encoding of a while loop

536
00:40:02,680 --> 00:40:05,680
as a recursive function,
but without introducing

537
00:40:05,680 --> 00:40:11,520
the overheads of an actual
explicit encoding.

538
00:40:11,520 --> 00:40:14,040
The idea is to specify the while loop

539
00:40:14,040 --> 00:40:19,200
by relating the behavior of
the state at the given iteration

540
00:40:19,200 --> 00:40:23,880
with the state at
the last iteration.

541
00:40:23,880 --> 00:40:27,280
And such a specification
can be established

542
00:40:27,280 --> 00:40:32,320
by induction using Coq's
induction mechanism

543
00:40:32,320 --> 00:40:34,760
by applying the following
unrolling rule,

544
00:40:34,760 --> 00:40:39,840
which essentially unfolds
the while loop one iteration.

545
00:40:39,840 --> 00:40:42,600
This rule introduces
a conditional sequence

546
00:40:42,600 --> 00:40:47,040
and a recursive occurrence
of the while loop.

547
00:40:47,040 --> 00:40:48,880
When reasoning about this
recursive occurrence

548
00:40:48,880 --> 00:40:51,680
of the while loop, one may
apply the frame rule

549
00:40:51,680 --> 00:40:56,000
to frame the parts of
the list or of the tree

550
00:40:56,000 --> 00:41:01,000
that we are stepping over.

551
00:41:01,000 --> 00:41:03,080
So in the course,
I described an example

552
00:41:03,080 --> 00:41:06,400
illustrating this proof technique

553
00:41:06,400 --> 00:41:12,000
and compare it with the proof
based on the loop environment.

554
00:41:12,000 --> 00:41:15,120
In conclusion, you can
find on my web page

555
00:41:15,120 --> 00:41:18,440
the paper, including
its eight page appendix

556
00:41:18,440 --> 00:41:21,680
on all the course material,
both in Coq format

557
00:41:21,680 --> 00:41:24,640
and HTML format.

558
00:41:24,640 --> 00:41:26,600
This course is meant
to be released soon

559
00:41:26,600 --> 00:41:30,400
as a volume of the software
foundation series.

560
00:41:30,400 --> 00:41:31,840
I'm actually seeking for feedback,

561
00:41:31,840 --> 00:41:34,560
both from students
and teachers to further

562
00:41:34,560 --> 00:41:36,360
polish the material.

563
00:41:36,360 --> 00:41:39,400
So if you're interested,
please get in touch with me,

564
00:41:39,400 --> 00:41:42,200
and I will be happy to
serve as a TA for you

565
00:41:42,200 --> 00:41:43,720
if you are teaching the course.

566
00:41:43,720 --> 00:41:47,160
Thank you very much.

567
00:41:47,160 --> 00:41:55,000
(APPLAUSE)

568
00:41:55,000 --> 00:41:57,840
Richard: OK, thank you
very much, Arthur.

569
00:41:57,840 --> 00:41:59,960
He is now available in
the New York Time Band

570
00:41:59,960 --> 00:42:02,640
for question and answer,

571
00:42:02,640 --> 00:42:04,600
and we'll take a brief
pause to sync up

572
00:42:04,600 --> 00:42:13,600
with the advertised schedule.

573
00:43:00,320 --> 00:43:01,760
Now, we have Aaron Stump,

574
00:43:01,760 --> 00:43:03,680
presenting his "Strong
Functional Pearl:

575
00:43:03,680 --> 00:43:07,240
"Harper's Regular-Expression
Matcher in Cedille."

576
00:43:07,240 --> 00:43:08,800
AARON: Hello, I'm Aaron Stump,

577
00:43:08,800 --> 00:43:10,040
and today, I'm going to present to you

578
00:43:10,040 --> 00:43:11,200
our "Strong Functional Pearl:

579
00:43:11,200 --> 00:43:15,240
"Harper's Regular-Expression
Matcher in Cedille."

580
00:43:15,240 --> 00:43:16,560
Harper's regular-expression matcher

581
00:43:16,560 --> 00:43:18,520
is a continuation-based algorithm

582
00:43:18,520 --> 00:43:20,240
for regular-expression matching,

583
00:43:20,240 --> 00:43:23,600
introduced by Robert
Harper, in his JFP paper,

584
00:43:23,600 --> 00:43:25,360
titled, "Proof-Directed Debugging."

585
00:43:25,360 --> 00:43:26,920
And the basic idea of the algorithm

586
00:43:26,920 --> 00:43:29,480
is to decompose regular expressions.

587
00:43:29,480 --> 00:43:31,120
So, you're going to dig down
into the regular expression,

588
00:43:31,120 --> 00:43:33,400
building up a success continuation.

589
00:43:33,920 --> 00:43:34,920
If you detect a clash,

590
00:43:34,920 --> 00:43:36,280
where the regular
expression is asking

591
00:43:36,280 --> 00:43:38,400
for some character that's
not there in the string,

592
00:43:38,400 --> 00:43:40,160
then, you just return false.

593
00:43:40,160 --> 00:43:42,120
But, if you find a character match,

594
00:43:42,120 --> 00:43:43,600
then, you're gonna invoke
the continuation,

595
00:43:43,600 --> 00:43:46,720
to keep processing
the suffix of the string.

596
00:43:46,720 --> 00:43:49,280
And Harper's paper is, you know,

597
00:43:49,280 --> 00:43:50,640
he called it
"Proof-Directed Debugging,"

598
00:43:50,640 --> 00:43:53,000
because there's an interesting bug

599
00:43:53,000 --> 00:43:55,960
in the termination argument
for this algorithm,

600
00:43:55,960 --> 00:43:58,840
unless you use standard
regular expressions.

601
00:43:58,840 --> 00:44:00,200
So, your R star is allowed,

602
00:44:00,200 --> 00:44:02,480
only if the empty string, epsilon,

603
00:44:02,480 --> 00:44:04,360
is not in language of R.

604
00:44:04,360 --> 00:44:05,840
And here, in this talk,

605
00:44:05,840 --> 00:44:07,880
we're gonna enforce
that syntactically,

606
00:44:07,880 --> 00:44:11,200
using a typed StdReg, for
standard regular expressions.

607
00:44:11,200 --> 00:44:12,840
And this is an idea
we got from a paper,

608
00:44:12,840 --> 00:44:15,760
from Dan Licata and his students.

609
00:44:15,760 --> 00:44:19,040
So, here's a look at
the implementation in Haskell.

610
00:44:19,040 --> 00:44:20,720
We're going to move to
the implementation in Cedille,

611
00:44:20,720 --> 00:44:22,720
in a little bit, but to
start off in Haskell,

612
00:44:22,720 --> 00:44:24,800
so, it's a continuation-based
algorithm.

613
00:44:24,800 --> 00:44:27,160
So, we have a type K
for continuations.

614
00:44:27,160 --> 00:44:28,320
It takes strings to bools.

615
00:44:28,320 --> 00:44:29,800
They're going to take suffixes,

616
00:44:29,800 --> 00:44:31,560
tell us whether or not they match.

617
00:44:32,320 --> 00:44:34,120
And then, there's a type,
matchT, for convenience.

618
00:44:34,120 --> 00:44:37,320
It just says continuation
goes to bool.

619
00:44:37,320 --> 00:44:39,440
So here, we see the two helper-

620
00:44:39,440 --> 00:44:42,800
central helper functions
of the algorithm.

621
00:44:42,800 --> 00:44:44,120
There's matchh.

622
00:44:44,120 --> 00:44:46,840
It takes a string, in one of
these regular expressions,

623
00:44:46,840 --> 00:44:48,560
and a continuation.

624
00:44:48,560 --> 00:44:50,840
Now, the first equation
matchh you see,

625
00:44:50,840 --> 00:44:53,360
if that string is empty,
then, we just return false,

626
00:44:53,360 --> 00:44:55,240
'cause these are standard
regular expressions.

627
00:44:55,240 --> 00:44:57,960
They cannot match the empty string.

628
00:44:57,960 --> 00:44:59,320
Otherwise, we just split the string

629
00:44:59,320 --> 00:45:01,560
into its head, c, and tail, cs,

630
00:45:01,560 --> 00:45:05,080
and we invoke this second helper
function, called matchi.

631
00:45:05,080 --> 00:45:06,600
OK, and here, we see the semantics,

632
00:45:06,600 --> 00:45:09,040
in the form of these standard
regular expressions, too.

633
00:45:09,040 --> 00:45:11,040
So, matchi, this,

634
00:45:11,040 --> 00:45:12,560
you could match NoMatch
regular expression.

635
00:45:12,560 --> 00:45:14,120
That just says
nothing's gonna match.

636
00:45:14,120 --> 00:45:15,720
We just return false.

637
00:45:15,720 --> 00:45:18,400
There's MatchChar, with
a character of c prime,

638
00:45:18,400 --> 00:45:19,400
and then, you check,

639
00:45:19,400 --> 00:45:22,200
does c equal c prime?

640
00:45:22,200 --> 00:45:24,000
And if so, that's the success case,

641
00:45:24,000 --> 00:45:28,520
and you invoke this continuation,
k,on the suffix, cs,

642
00:45:28,520 --> 00:45:30,440
and otherwise, you return false.

643
00:45:30,440 --> 00:45:32,600
For an Or regular expression,
we just decompose,

644
00:45:32,600 --> 00:45:34,480
and just check disjunctively

645
00:45:34,480 --> 00:45:37,320
the two parts of
the regular expression.

646
00:45:37,320 --> 00:45:38,320
For the Plus case, though,

647
00:45:38,320 --> 00:45:41,040
this is where the interesting
termination issue comes up.

648
00:45:41,040 --> 00:45:43,080
So, if we have Plus r,

649
00:45:43,080 --> 00:45:45,360
we want to match r, to see
if it matches one time,

650
00:45:45,360 --> 00:45:48,680
'cause this is Plus r, of course,
being one or more matches.

651
00:45:48,680 --> 00:45:49,920
So, we match it one time,

652
00:45:49,920 --> 00:45:51,120
and then, in the continuation

653
00:45:51,120 --> 00:45:53,560
that starts out with
Lambda cs prime,

654
00:45:53,560 --> 00:45:54,560
we're gonna say, well,

655
00:45:54,560 --> 00:45:57,160
let's try just calling
the continuation k on cs prime.

656
00:45:57,160 --> 00:46:00,280
That would represent
exactly one match of r.

657
00:46:00,280 --> 00:46:01,280
But, if that fails,

658
00:46:01,280 --> 00:46:04,400
then, we want to go,
and we're supposed to recursively

659
00:46:04,400 --> 00:46:06,160
try the whole
regular-expression again,

660
00:46:06,160 --> 00:46:07,680
on the suffix, cs prime.

661
00:46:07,680 --> 00:46:10,080
So, that's what I've got
highlighted in red there.

662
00:46:10,080 --> 00:46:11,520
And the interesting thing
about that, of course,

663
00:46:11,520 --> 00:46:13,360
is that the regular expression

664
00:46:13,360 --> 00:46:15,520
is exactly the same
as we started with.

665
00:46:15,520 --> 00:46:17,720
Plus r, we just make
our recursive call with plus r.

666
00:46:17,720 --> 00:46:19,360
So, there's been no
structural decrease here,

667
00:46:19,360 --> 00:46:23,440
which is quite worrisome for
a termination situation.

668
00:46:23,440 --> 00:46:24,680
And then, concat,

669
00:46:24,680 --> 00:46:28,920
similarly decomposes
and updates the continuation.

670
00:46:28,920 --> 00:46:32,560
So, the problem of termination
for this algorithm

671
00:46:32,560 --> 00:46:33,600
has been actually studied,

672
00:46:33,600 --> 00:46:35,960
by a number of different
previous works.

673
00:46:35,960 --> 00:46:37,960
You can sort of see
the main problem right here,

674
00:46:37,960 --> 00:46:39,400
in this little part
that I pulled out

675
00:46:39,400 --> 00:46:41,520
from the code we were
just looking at.

676
00:46:41,520 --> 00:46:44,160
And so, we have a matchh,
that calls matchi,

677
00:46:44,160 --> 00:46:46,000
and matchi wants to
call back to matchh,

678
00:46:46,000 --> 00:46:48,920
in this red highlighted
call site there,

679
00:46:48,920 --> 00:46:50,680
but it's calling it
with the cs prime.

680
00:46:50,680 --> 00:46:54,200
It's just a value it got
from a continuation.

681
00:46:54,200 --> 00:46:55,400
It has no idea.

682
00:46:55,400 --> 00:46:58,240
You know, the code, there's no
obvious syntactic connection

683
00:46:58,240 --> 00:47:00,200
between cs prime and cs.

684
00:47:00,200 --> 00:47:01,920
So, it seems that you need
something like a proof

685
00:47:01,920 --> 00:47:03,360
that the length of cs prime

686
00:47:03,360 --> 00:47:04,600
is less than the length of cs,

687
00:47:04,600 --> 00:47:05,800
or something like this.

688
00:47:05,800 --> 00:47:08,920
And, in fact, previous works
the tackled termination

689
00:47:08,920 --> 00:47:10,720
of Harper's
regular-expression matcher,

690
00:47:10,720 --> 00:47:11,840
some of them anyway,

691
00:47:11,840 --> 00:47:14,520
used dependently typed continuations.

692
00:47:14,520 --> 00:47:16,880
And, in fact, one of these papers,

693
00:47:16,880 --> 00:47:18,560
a very nice paper
I highly recommend,

694
00:47:18,560 --> 00:47:20,400
by Bove, Krauss, and Sozeau,

695
00:47:20,400 --> 00:47:23,280
raise this challenge, that
there seems to be no easy way

696
00:47:23,280 --> 00:47:25,040
to achieve the same effect,

697
00:47:25,040 --> 00:47:28,560
without the use of dependent types.

698
00:47:28,560 --> 00:47:31,360
So, in this pearl, we're gonna show-

699
00:47:31,360 --> 00:47:32,560
we're showing Harper's matcher

700
00:47:32,560 --> 00:47:34,440
as a strong functional program,

701
00:47:34,440 --> 00:47:36,440
in the sense of David Turner.

702
00:47:36,440 --> 00:47:38,080
So, the idea is that we want,

703
00:47:38,080 --> 00:47:39,240
in strong functional programming,

704
00:47:39,240 --> 00:47:41,920
is we want to statically enforce
termination of programs,

705
00:47:41,920 --> 00:47:43,520
but the name of the game here is,

706
00:47:43,520 --> 00:47:44,880
you're only allowed to use

707
00:47:44,880 --> 00:47:47,800
the types of regular old
functional programming.

708
00:47:47,800 --> 00:47:50,640
Just polymorphic types.

709
00:47:50,640 --> 00:47:52,120
You can't use dependent types.

710
00:47:52,120 --> 00:47:53,280
You can't use sized types.

711
00:47:53,280 --> 00:47:55,080
You're not allowed to use
termination proofs,

712
00:47:55,080 --> 00:47:57,120
or variants, or metrics,
or well-founded orders.

713
00:47:57,120 --> 00:48:01,440
All that heavy duty machinery
of termination is not allowed,

714
00:48:01,440 --> 00:48:04,240
in the regime of strong
functional programming.

715
00:48:04,240 --> 00:48:07,120
And so, we are happy to report that,

716
00:48:07,120 --> 00:48:10,840
in fact, you can do this example
without dependent types.

717
00:48:10,840 --> 00:48:12,560
You can just use the tool box

718
00:48:12,560 --> 00:48:16,600
of well, advanced, strong
functional program.

719
00:48:16,600 --> 00:48:17,800
How so, how can you do that?

720
00:48:17,800 --> 00:48:18,880
That's what we're going to talk about,

721
00:48:18,880 --> 00:48:21,280
for the time we have here.

722
00:48:21,280 --> 00:48:25,720
And this is - we're using our
language, Cedille, for this.

723
00:48:25,720 --> 00:48:27,960
And so, strong functional
programming in Cedille.

724
00:48:27,960 --> 00:48:29,520
Cedille is a dependent type theory,

725
00:48:29,520 --> 00:48:32,560
kind of like Coq or Agda,

726
00:48:32,560 --> 00:48:36,280
and termination of programs
is enforced by typing.

727
00:48:36,280 --> 00:48:39,360
But here, Cedille has
some particularities

728
00:48:39,360 --> 00:48:41,240
that make it a bit different
from Coq and Agda,

729
00:48:41,240 --> 00:48:44,320
and one of them is how
termination is handled.

730
00:48:44,320 --> 00:48:47,080
So, and that's part of what we're going to have
a little tutorial about,

731
00:48:47,080 --> 00:48:48,160
in this talk.

732
00:48:48,160 --> 00:48:49,200
So, in Cedille,

733
00:48:49,200 --> 00:48:50,680
when you're writing
a recursive function,

734
00:48:50,680 --> 00:48:51,680
you're presented with something

735
00:48:51,680 --> 00:48:54,160
that we call a recursion universe.

736
00:48:54,160 --> 00:48:58,400
It's a typed interface for
doing the recursion, OK?

737
00:48:58,400 --> 00:49:00,480
So, you get an abstract type r.

738
00:49:00,480 --> 00:49:03,200
That's what you're sort of
thinking of as the universe,

739
00:49:03,200 --> 00:49:04,200
the recursion universe,

740
00:49:04,200 --> 00:49:05,720
and it has some operations on it,

741
00:49:05,720 --> 00:49:07,360
that help you write your recursion.

742
00:49:07,360 --> 00:49:10,480
You're not allowed to make
direct recursive calls,

743
00:49:10,480 --> 00:49:11,480
in your code.

744
00:49:11,480 --> 00:49:14,120
You have to kinda go
through this interface.

745
00:49:14,120 --> 00:49:15,600
And the language, though, provides-

746
00:49:15,600 --> 00:49:17,280
Cedille provides special syntax

747
00:49:17,280 --> 00:49:19,320
for these recursion universes,
that pretty much hide

748
00:49:20,000 --> 00:49:22,000
that hide
a lot of the machinery

749
00:49:22,000 --> 00:49:24,320
and make it feel
a quite idiomatic,

750
00:49:24,320 --> 00:49:26,400
way of writing
recursive functions.

751
00:49:26,400 --> 00:49:29,000
So, I want to emphasize
even though Cedille

752
00:49:29,000 --> 00:49:31,040
is this type theory
that lets you do proofs

753
00:49:31,040 --> 00:49:34,000
and things like that,
dependently typed programs

754
00:49:34,000 --> 00:49:35,680
here we're not using
any dependent types

755
00:49:35,680 --> 00:49:36,960
or any of that
kind of machinery,

756
00:49:36,960 --> 00:49:40,080
we're just using
strong functional programming.

757
00:49:40,080 --> 00:49:41,640
OK. So let's see
how this works

758
00:49:41,640 --> 00:49:44,560
in several steps.
So first, the first point,

759
00:49:44,560 --> 00:49:47,040
we're going to see
Cedille's recursion universes.

760
00:49:47,040 --> 00:49:48,720
So we start off
with something called

761
00:49:48,720 --> 00:49:51,160
a Mendler style
recursion universe.

762
00:49:51,160 --> 00:49:54,240
OK. And so this is kind
of like the underlying interface.

763
00:49:54,240 --> 00:49:55,520
And again, Cedille
provides sort of a little

764
00:49:55,520 --> 00:49:58,120
bit nicer surface language
access to this interface,

765
00:49:58,120 --> 00:50:00,720
but this is the basic interface
that we're relying on.

766
00:50:00,720 --> 00:50:04,120
So let's let F be a signature
function for a data type.

767
00:50:04,120 --> 00:50:06,640
So for example,
if your data type was list of A's,

768
00:50:06,640 --> 00:50:11,240
then your signature function
is FX equals unit or A cross X.

769
00:50:11,240 --> 00:50:15,000
OK. So, if you wanna
suppose you're trying

770
00:50:15,000 --> 00:50:17,040
to write a function.
that's gonna compute

771
00:50:17,040 --> 00:50:19,840
a value of type X
from your datatype D

772
00:50:19,840 --> 00:50:22,560
then what the recursion universe
provides you is

773
00:50:22,560 --> 00:50:24,560
this abstract type R
is mentioning.

774
00:50:24,560 --> 00:50:25,960
It's just some type.
You have no idea,

775
00:50:25,960 --> 00:50:28,080
anything about the type,
except what the interface

776
00:50:28,080 --> 00:50:30,600
gives you about it.
The first thing it gives you

777
00:50:30,600 --> 00:50:34,000
is an eval function
R to X, OK.

778
00:50:34,000 --> 00:50:35,640
And this is the function
you have to invoke.

779
00:50:35,640 --> 00:50:37,680
If you want to make
a recursive call,

780
00:50:37,680 --> 00:50:39,480
you gotta call eval,
but you can only call it

781
00:50:39,480 --> 00:50:40,920
on an R thing.
You can't call it

782
00:50:40,920 --> 00:50:43,440
on just some random
other D thing.

783
00:50:43,440 --> 00:50:45,880
And you're also given
a substructure, sub data structure

784
00:50:45,880 --> 00:50:49,920
of type F R, in other words,
you're given this little package

785
00:50:49,920 --> 00:50:53,400
That is presenting you,
the data that you have,

786
00:50:53,400 --> 00:50:57,840
except at the sort
of factorial position it's got

787
00:50:57,840 --> 00:51:01,840
The abstract type R.
And so the key thing

788
00:51:01,840 --> 00:51:03,560
about this is that,
and this is, this goes

789
00:51:03,560 --> 00:51:07,280
back to this idea
of Mendler style recursion

790
00:51:07,280 --> 00:51:10,760
is that you can only call
eval on sub data,

791
00:51:10,760 --> 00:51:12,360
because the only things
you have lying around

792
00:51:12,360 --> 00:51:14,520
of type R, are the things
that are hidden,

793
00:51:14,520 --> 00:51:16,680
you know, presented to you
in this sub data structure.

794
00:51:16,680 --> 00:51:19,840
And so that's the only stuff
you can call it eval on.

795
00:51:19,840 --> 00:51:21,640
So that ensures your,
you are doing

796
00:51:21,640 --> 00:51:24,040
a structural termination,
even though there's no

797
00:51:24,040 --> 00:51:27,160
sort of check about,
you know, is this variable,

798
00:51:27,160 --> 00:51:30,320
I'm making a quick call
on somehow structurally

799
00:51:30,320 --> 00:51:32,240
derived from my input
or anything like that.

800
00:51:32,240 --> 00:51:34,160
It's all done
through the typing.

801
00:51:34,160 --> 00:51:37,040
OK. So that's Mendler-Style.
Now Mendler-Style paramorphic

802
00:51:37,040 --> 00:51:40,160
recursion universe.
So this is the next sort

803
00:51:40,160 --> 00:51:44,080
of step that presenting
sort of a conceptual development

804
00:51:44,080 --> 00:51:47,000
of this idea of Cedille's
recursion universe.

805
00:51:47,000 --> 00:51:49,280
So for computing X,
from data type D

806
00:51:49,280 --> 00:51:52,200
now for paramorphic recursion,
the universe gives you

807
00:51:52,200 --> 00:51:54,800
the same stuff you had
before the recursion universe

808
00:51:54,800 --> 00:51:56,880
has all these same pieces,
but it also comes

809
00:51:56,880 --> 00:52:00,560
as, highlighted in black,
with a reveal function,

810
00:52:00,560 --> 00:52:06,520
R to D, so this unmasks R
as really being D, OK.

811
00:52:06,520 --> 00:52:09,080
So, but notice you can't
go the other way around.

812
00:52:09,080 --> 00:52:11,520
You cannot inject D into R
that would be totally

813
00:52:11,520 --> 00:52:13,440
unsound from
a termination perspective.

814
00:52:13,440 --> 00:52:15,760
'Cause you could just
stick random values into R

815
00:52:15,760 --> 00:52:18,160
and the recurse on them.
But here you,

816
00:52:18,160 --> 00:52:20,640
you are allowed to sort of
leave the recursion universe.

817
00:52:20,640 --> 00:52:22,880
You can sort of, you know,
I don't know wormhole

818
00:52:22,880 --> 00:52:24,080
out of there
if you want to,

819
00:52:24,080 --> 00:52:27,480
to have some data D,
and this is really important.

820
00:52:27,480 --> 00:52:28,800
If you wanna write
efficient accessor

821
00:52:28,800 --> 00:52:31,480
functions like predecessor
or tail or something.

822
00:52:32,040 --> 00:52:34,400
Now finally, what we need
for the Harper's

823
00:52:34,400 --> 00:52:36,680
regular expression matcher
is we need support

824
00:52:36,680 --> 00:52:39,440
for nested recursion.
So you have all the same stuff

825
00:52:39,440 --> 00:52:41,680
we had before,
except we change a little bit,

826
00:52:41,680 --> 00:52:44,200
the return type
of your function

827
00:52:44,200 --> 00:52:49,160
now couldn't- is now-
could depend on D.

828
00:52:49,160 --> 00:52:52,560
So you compute X of D
from a data type D

829
00:52:52,560 --> 00:52:55,200
and the, the reason
that's really important to shift,

830
00:52:55,200 --> 00:52:57,480
as I see it say below
my bullet points there

831
00:52:57,480 --> 00:53:00,720
from X of kind star,
to X of kind star to star,

832
00:53:00,720 --> 00:53:03,960
is it now the eval function,
you can write eval function

833
00:53:03,960 --> 00:53:06,960
that goes from R to X of R.
And that gives you

834
00:53:06,960 --> 00:53:09,360
the possibility of seeing
when you return something

835
00:53:09,360 --> 00:53:12,160
from your recursive call,
that it also has

836
00:53:12,160 --> 00:53:13,680
type R and hence,
you could make

837
00:53:13,680 --> 00:53:18,640
a recursive call again on that.
So a recursion is then allowed

838
00:53:18,640 --> 00:53:21,000
on values that
eval has returned.

839
00:53:21,000 --> 00:53:22,720
So that enables a form
of nested recursion,

840
00:53:22,720 --> 00:53:25,360
and the continuations
are basically hiding

841
00:53:25,360 --> 00:53:28,520
a form of nested recursion.
OK. So let's take

842
00:53:28,520 --> 00:53:30,120
a look at how we do
this in Cedille

843
00:53:30,120 --> 00:53:35,680
Harper's match in Cedille.
So, the critical part

844
00:53:35,680 --> 00:53:38,800
about the nested
recursion is here.

845
00:53:38,800 --> 00:53:44,680
So in,
In blue, I've had,

846
00:53:44,680 --> 00:53:48,480
it's Cedille syntax
for indicating the abstract type R,

847
00:53:48,480 --> 00:53:49,880
if we're doing
a recursion here,

848
00:53:49,880 --> 00:53:52,640
mu match, then mu
is for the sort of

849
00:53:52,640 --> 00:53:56,360
kicking off recursion
then type/match is R

850
00:53:56,360 --> 00:53:58,680
That's the abstract type
of the recursion universe.

851
00:53:58,680 --> 00:54:01,000
And the X, the type
we're computing here

852
00:54:01,000 --> 00:54:03,080
has to be given explicitly
Cedille can't

853
00:54:03,080 --> 00:54:07,120
infer this for you.
And it can mention this R,

854
00:54:07,120 --> 00:54:10,400
and that's just like XR, XD.
Now we have

855
00:54:10,400 --> 00:54:13,560
a way of mentioning
we're indicating

856
00:54:13,560 --> 00:54:17,600
the functoriality of X
and X does have to be

857
00:54:17,600 --> 00:54:21,760
positive for this to work.
And finally where

858
00:54:21,760 --> 00:54:24,800
the eval function
is conveniently called match.

859
00:54:24,800 --> 00:54:26,280
And so it looks,
this looks very much like

860
00:54:26,280 --> 00:54:27,680
an idiomatic recursion.
Now, here, we're doing

861
00:54:27,680 --> 00:54:30,520
something it'd be tough
to do in other type theories

862
00:54:30,520 --> 00:54:32,760
where passing the recursive

863
00:54:32,760 --> 00:54:36,320
the recursive function match
there to the helper

864
00:54:36,320 --> 00:54:38,200
function matchi,
but anyway,

865
00:54:38,200 --> 00:54:40,880
that match is the eval
and it has the type below

866
00:54:40,880 --> 00:54:42,840
it takes in the abstract type R
which here is denoted

867
00:54:42,840 --> 00:54:45,920
type/match and proceeds.

868
00:54:45,920 --> 00:54:49,000
OK. So that's sort
of the setup for the recursion,

869
00:54:49,000 --> 00:54:51,600
introducing, the recursion
universe pieces

870
00:54:51,600 --> 00:54:55,360
that we then need
to write this helper function

871
00:54:55,360 --> 00:54:58,400
and make that deal with that
one nasty recursive call.

872
00:54:58,400 --> 00:55:00,720
So you can see
highlighted here,

873
00:55:00,720 --> 00:55:02,560
it's transliterated
into Cedille.

874
00:55:02,560 --> 00:55:04,640
I hope you can sort of
still make out the structure.

875
00:55:04,640 --> 00:55:07,960
We have a plus R case
and that I pulled out here

876
00:55:07,960 --> 00:55:09,920
and we were gonna
make this recursive call

877
00:55:09,920 --> 00:55:14,920
to check one
occurrence of R,

878
00:55:14,920 --> 00:55:16,800
and, then in the end,
when we make

879
00:55:16,800 --> 00:55:19,760
the recursive call
we're calling and match cs prime,

880
00:55:19,760 --> 00:55:22,080
and that's great,
this match function is,

881
00:55:22,080 --> 00:55:24,840
is actually coming in to matchi
As you saw it,

882
00:55:24,840 --> 00:55:27,000
where matchi was invoked
in my previous slide,

883
00:55:27,000 --> 00:55:28,800
this match functions
coming into matchi

884
00:55:28,800 --> 00:55:31,080
and it's expecting type T
where T is actually

885
00:55:31,080 --> 00:55:32,280
just some abstract type.
You really don't need

886
00:55:32,280 --> 00:55:35,360
to know much about it,
except we can recurse on it.

887
00:55:35,360 --> 00:55:36,920
If you're looking
at the type signature

888
00:55:36,920 --> 00:55:39,120
of matchi at the top,
we can recurse on it.

889
00:55:39,120 --> 00:55:42,400
And the suffix comes
in at that time.

890
00:55:42,400 --> 00:55:46,080
And so when we call
match of CS prime, match

891
00:55:46,080 --> 00:55:49,040
is expecting input of type
T and CS prime has type T

892
00:55:49,040 --> 00:55:51,960
because CS prime
is the input to the continuation.

893
00:55:51,960 --> 00:55:53,800
And the continuation
is denoted there

894
00:55:53,800 --> 00:55:58,960
in blue has type KT,
which is T to bool.

895
00:55:58,960 --> 00:56:00,880
OK. So to conclude,
we've talked about

896
00:56:00,880 --> 00:56:02,840
Harper's regular
expression matcher,

897
00:56:02,840 --> 00:56:05,880
which uses continuations
for very elegant control flow

898
00:56:05,880 --> 00:56:08,720
that leads to complications
for termination reasoning.

899
00:56:08,720 --> 00:56:11,960
And we saw a solution now using
strong functional programming

900
00:56:11,960 --> 00:56:13,720
in Cedille,
it's type based.

901
00:56:13,720 --> 00:56:15,640
There's no fancy,
extra additions

902
00:56:15,640 --> 00:56:18,640
beyond regular types
of functional programming,

903
00:56:18,640 --> 00:56:20,080
no dependent types,
nothing like that,

904
00:56:20,080 --> 00:56:22,920
even though Cedille
does support those separately.

905
00:56:22,920 --> 00:56:26,360
So we use this idea
of a recursion universe that,

906
00:56:26,360 --> 00:56:29,920
that gives you a form
of type based nested

907
00:56:29,920 --> 00:56:32,280
paramorphic recursion.
And it turns out that

908
00:56:32,280 --> 00:56:34,760
Harper's matcher is just
a perfect fit for this.

909
00:56:34,760 --> 00:56:36,160
It's very easy.
We don't have time

910
00:56:36,160 --> 00:56:37,720
to wade through
all the code,

911
00:56:37,720 --> 00:56:40,920
but it's really pretty much
a direct port of the

912
00:56:40,920 --> 00:56:42,920
Haskell code
because of the way

913
00:56:42,920 --> 00:56:45,600
that the abstract type
for the eval function

914
00:56:45,600 --> 00:56:47,640
matches up perfectly
with the inputs

915
00:56:47,640 --> 00:56:51,520
for the continuation.
So thanks very much to funders,

916
00:56:51,520 --> 00:56:54,000
and I'd really like to thank
the very dedicated

917
00:56:54,000 --> 00:56:55,880
anonymous ICFP reviewers
who gave us

918
00:56:55,880 --> 00:56:58,360
outstanding really
above and beyond

919
00:56:58,360 --> 00:57:00,760
kind of criticism
and help getting this

920
00:57:00,760 --> 00:57:04,280
into good shape
and shepherd Richard Eisenberg

921
00:57:04,280 --> 00:57:06,160
and colleagues
here at Iowa.

922
00:57:06,160 --> 00:57:08,440
And thanks to you
for watching.

923
00:57:08,440 --> 00:57:15,760
(APPLAUSE)

924
00:57:16,440 --> 00:57:17,520
RICHARD: Great,
thanks, Aaron.

925
00:57:17,520 --> 00:57:20,120
And now in the New York
and Asia time band,

926
00:57:20,120 --> 00:57:22,360
we have authors available
for question and answer.

927
00:57:22,360 --> 00:57:24,000
So please follow
the links via Clowdr,

928
00:57:24,000 --> 00:57:26,040
if you wish to join them.
We'll take a brief pause

929
00:57:26,040 --> 00:57:29,880
until the next talk at 11:45
in the New York time band.

930
00:58:01,960 --> 00:58:05,880
Next up we have Nandor Licker
who will present Duplo,

931
00:58:05,880 --> 00:58:10,040
a framework for
Ocaml post-link optimization.

932
00:58:10,680 --> 00:58:12,560
NANDOR LICKER: Hello,
my name is Nandor Licker

933
00:58:12,560 --> 00:58:15,160
And today I'm going to talk
about Duplo a framework

934
00:58:15,160 --> 00:58:18,960
for OCaml post link optimization.
I built with Dr. Tim Jones

935
00:58:18,960 --> 00:58:21,320
at the university of Cambridge.

936
00:58:21,320 --> 00:58:23,240
Why build a post link optimizer?

937
00:58:23,240 --> 00:58:26,200
And why build one
for OCaml in particular.

938
00:58:26,200 --> 00:58:28,640
OCaml is a high level language,
which means that it is not

939
00:58:28,640 --> 00:58:31,800
well suited for expressing
specific low-level constructs

940
00:58:31,800 --> 00:58:34,400
as an example,
we consider a simple type

941
00:58:34,400 --> 00:58:36,840
to represent complex numbers,
composed of an imaginary

942
00:58:36,840 --> 00:58:39,160
and a real component.
If we declare

943
00:58:39,160 --> 00:58:40,760
a complex number
as a pair of floats,

944
00:58:40,760 --> 00:58:43,040
we end up
with many allocations,

945
00:58:43,040 --> 00:58:45,160
even though we only care
about eight bytes of data

946
00:58:45,160 --> 00:58:48,160
to store two 32 bit floats,
the object will be spread

947
00:58:48,160 --> 00:58:51,560
across 56 bytes
and three distinct allocations

948
00:58:51,560 --> 00:58:54,840
producing three distinct blocks
on the heap.

949
00:58:54,840 --> 00:58:57,320
Ideally, we'd like
a single block with a header

950
00:58:57,320 --> 00:59:00,760
and eight bytes of data.
So how do we get there?

951
00:59:00,760 --> 00:59:03,240
We can write a lot of code,
possibly fiddling with bits

952
00:59:03,240 --> 00:59:04,920
and express the type
in OCaml.

953
00:59:04,920 --> 00:59:08,160
However, the solution
is unlikely to be elegant.

954
00:59:08,160 --> 00:59:11,040
We could also change
the runtime or the compiler.

955
00:59:11,040 --> 00:59:13,320
However, this is not
practical nor scalable,

956
00:59:13,320 --> 00:59:15,800
and could become
a maintenance nightmare.

957
00:59:15,800 --> 00:59:17,400
We choose to instead
use an existing

958
00:59:17,400 --> 00:59:20,800
language feature namely
the foreign function interface,

959
00:59:20,800 --> 00:59:24,520
the FFI allows us to use
the right tool for the right job.

960
00:59:24,520 --> 00:59:26,320
We can express
low level constructs

961
00:59:26,320 --> 00:59:29,360
in C and call them
from OCaml.

962
00:59:29,920 --> 00:59:33,040
The FFI of OCaml
is particularly powerful.

963
00:59:33,040 --> 00:59:37,000
Thanks to its minimal runtime
and simple object representation.

964
00:59:37,000 --> 00:59:39,200
This allows us
to read arbitrary objects

965
00:59:39,200 --> 00:59:41,560
from the OCaml heap
and also to allocate

966
00:59:41,560 --> 00:59:44,320
arbitrary objects
on the OCaml heap.

967
00:59:44,320 --> 00:59:47,120
Such features allow the FFI
to be flexible enough

968
00:59:47,120 --> 00:59:50,760
to build a wide variety
of types and functions.

969
00:59:50,760 --> 00:59:53,640
However, the FFI
does not come for free.

970
00:59:53,640 --> 00:59:56,720
Besides the implicit trampolines
that heap management code

971
00:59:56,720 --> 00:59:59,000
requires to invoke
C from OCaml,

972
00:59:59,000 --> 01:00:02,480
we pay the cost of missed
optimization opportunities.

973
01:00:02,480 --> 01:00:05,600
In the typical build, OCaml
and C meet in assembly

974
01:00:05,600 --> 01:00:08,640
and the linker unfortunately
high level transformations

975
01:00:08,640 --> 01:00:11,000
in machine code
are far from trivial.

976
01:00:11,000 --> 01:00:14,600
We need a common form,
more suitable for analysis,

977
01:00:14,600 --> 01:00:17,200
ideally, a representation
that has not yet gone

978
01:00:17,200 --> 01:00:21,080
through register allocation
nor instruction selection.

979
01:00:21,080 --> 01:00:25,400
So the goal of Duplo is to
ameliorate this FFI overhead

980
01:00:25,400 --> 01:00:27,760
and to lose the overhead,
we want to optimize

981
01:00:27,760 --> 01:00:30,520
across the language boundary.
However, on one side,

982
01:00:30,520 --> 01:00:32,920
we have a OCaml
and C on the other.

983
01:00:32,920 --> 01:00:35,680
Two very different languages
with different memory models

984
01:00:35,680 --> 01:00:39,120
and different run times.
In order to apply

985
01:00:39,120 --> 01:00:41,160
any optimizations,
we need to first find

986
01:00:41,160 --> 01:00:43,160
a common denominator
capable of capturing

987
01:00:43,160 --> 01:00:46,480
the semantics of both
C and OCaml.

988
01:00:46,480 --> 01:00:48,440
Duplo achieves this
with representation,

989
01:00:48,440 --> 01:00:52,200
we call LLIR, low level
intermediate representation.

990
01:00:52,200 --> 01:00:54,400
The framework provides
a custom version

991
01:00:54,400 --> 01:00:57,520
of Clang to LLIR
a custom version

992
01:00:57,520 --> 01:01:00,640
of the OCaml compiler
to lower OCaml to LLIR

993
01:01:00,640 --> 01:01:03,160
and an entire tool chain
turning LLIR modules

994
01:01:03,160 --> 01:01:06,560
into a single module containing
whole program information.

995
01:01:06,560 --> 01:01:09,520
And finally an optimizer,
which we call LLIR opt

996
01:01:09,520 --> 01:01:11,680
to analyze and transform
the program

997
01:01:11,680 --> 01:01:15,120
before lowering it
to executable machine code.

998
01:01:15,960 --> 01:01:19,000
LLIR is the central
component of our project.

999
01:01:19,000 --> 01:01:21,800
It is built to be easy to optimize.
It is in SSA form

1000
01:01:21,800 --> 01:01:24,400
has virtual registers,
simple instructions

1001
01:01:24,400 --> 01:01:27,400
and primitive types
with direct hardware equivalents,

1002
01:01:27,400 --> 01:01:30,680
even though it is not LLVM IR
it's in their representation

1003
01:01:30,680 --> 01:01:34,520
in C++ is almost
identical to LLVM IR.

1004
01:01:34,520 --> 01:01:37,640
Mirroring several useful methods
and we're using support libraries,

1005
01:01:37,640 --> 01:01:39,760
including valuable functions
and iterators

1006
01:01:39,760 --> 01:01:43,160
such as dominator reconstruction
and graph traversals.

1007
01:01:43,160 --> 01:01:45,480
We find a cool style
of our transformations

1008
01:01:45,480 --> 01:01:48,000
to be quite similar
to that of LLVM.

1009
01:01:48,000 --> 01:01:50,680
And this familiarity
is highly valuable.

1010
01:01:50,680 --> 01:01:54,840
Despite some similarities LLVM
is definitely not LLVM IR.

1011
01:01:54,840 --> 01:01:58,520
It is less abstract
and a lot closer to hardware.

1012
01:01:59,400 --> 01:02:01,800
Why is LLIR low level?

1013
01:02:01,800 --> 01:02:04,000
This choice is almost
entirely constrained

1014
01:02:04,000 --> 01:02:06,240
by the OCaml compiler
and the intermediate

1015
01:02:06,240 --> 01:02:09,360
representations found in clang.
Ocaml first compiles

1016
01:02:09,360 --> 01:02:10,880
to Lambda form
with closures,

1017
01:02:10,880 --> 01:02:13,320
which have no direct
equivalent in LLVM.

1018
01:02:13,320 --> 01:02:16,920
Requires to step one level down
in the OCaml compiler.

1019
01:02:16,920 --> 01:02:19,280
The next step is CMM,
which still has

1020
01:02:19,280 --> 01:02:20,920
some high level features.
So we need to go

1021
01:02:20,920 --> 01:02:23,560
down another level.
We are left with building

1022
01:02:23,560 --> 01:02:26,520
LLIR from Mach Linear,
which are essentially

1023
01:02:26,520 --> 01:02:28,320
equivalent to machine code,

1024
01:02:28,320 --> 01:02:31,120
LLVM IR is more
abstract than that.

1025
01:02:31,120 --> 01:02:33,520
We need to lower it
a bit in LLVM.

1026
01:02:33,520 --> 01:02:38,520
Conveniently, we defined LLIR
to be almost like machine code.

1027
01:02:38,520 --> 01:02:41,640
We lower LLVM IR
to LLIR to the target

1028
01:02:41,640 --> 01:02:44,040
specific back end of LLVM
passing through

1029
01:02:44,040 --> 01:02:47,560
selection DAG and LLIR
specific machine IR.

1030
01:02:47,560 --> 01:02:49,960
On the OCaml side.
We generate LLIR

1031
01:02:49,960 --> 01:02:52,640
from linear as the mapping
is almost one to one.

1032
01:02:52,640 --> 01:02:54,360
We are essentially
cross-compiling

1033
01:02:54,360 --> 01:02:57,800
to a new target
in both compilers.

1034
01:02:57,800 --> 01:03:01,680
Getting to LLIR
is only the start of the journey.

1035
01:03:01,680 --> 01:03:04,000
We also need to generate
machine code from LLIR.

1036
01:03:04,000 --> 01:03:06,520
And this step is not trivial.
Code generators

1037
01:03:06,520 --> 01:03:09,240
tend to be complex
pieces of code

1038
01:03:09,240 --> 01:03:11,640
and OCaml has run time
and a garbage collector

1039
01:03:11,640 --> 01:03:13,760
that we do not wish
to alter that imposes

1040
01:03:13,760 --> 01:03:16,440
further constraints.
Since our tool chain

1041
01:03:16,440 --> 01:03:18,520
already includes clang
We decided to use

1042
01:03:18,520 --> 01:03:21,040
the LLVM code generator
from its selection diagram

1043
01:03:21,040 --> 01:03:22,520
presentation onwards.

1044
01:03:22,520 --> 01:03:24,360
We have added
custom instructions

1045
01:03:24,360 --> 01:03:27,320
required by the OCaml GC
we're using the entirety

1046
01:03:27,320 --> 01:03:30,520
of the target-specific
machine IR pipeline.

1047
01:03:31,080 --> 01:03:34,160
Since we cross compile C to LLIR,
we have a mapping

1048
01:03:34,160 --> 01:03:37,360
from a selection DAG to LLIR,
in our back end,

1049
01:03:37,360 --> 01:03:41,960
we map LLIR to selection DAG
due to the circular dependency.

1050
01:03:41,960 --> 01:03:44,720
Most of the LLIR instructions
have a direct correspondent

1051
01:03:44,720 --> 01:03:46,760
in selection DAG
with the exception

1052
01:03:46,760 --> 01:03:48,480
of some more
complex constructs,

1053
01:03:48,480 --> 01:03:51,240
such as function calls
and 80 bits floats.

1054
01:03:51,240 --> 01:03:56,200
Simple instructions such as
add, subtract, multiply, and so on.

1055
01:03:56,200 --> 01:03:57,880
Do not really warrant
a custom mapping

1056
01:03:57,880 --> 01:04:00,120
from selection DAG
to LLIR, nor from

1057
01:04:00,120 --> 01:04:04,760
LLIR to selection DAG.
So, we simply map selection DAG,

1058
01:04:04,760 --> 01:04:07,800
to LLIR instruction,
and LLIR instruction

1059
01:04:07,800 --> 01:04:10,520
back to selection DAG
but function calls

1060
01:04:10,520 --> 01:04:16,200
are specific to each target.
And even in all of LLVM's back end,

1061
01:04:16,200 --> 01:04:20,440
they require a custom mapping
from LLVM IR to selection DAG,

1062
01:04:20,440 --> 01:04:23,440
and we have that mapping
for LLIR as well.

1063
01:04:23,440 --> 01:04:25,400
And because
of these constraints,

1064
01:04:25,400 --> 01:04:26,760
we actually had
very little freedom

1065
01:04:26,760 --> 01:04:30,880
when designing
our new representation.

1066
01:04:32,160 --> 01:04:33,840
Probably the most
pressing question

1067
01:04:33,840 --> 01:04:37,920
is why not lower to LLVM IR.
Previous attempts

1068
01:04:37,920 --> 01:04:39,840
made some progress
albeit with

1069
01:04:39,840 --> 01:04:41,880
a run time cost,
Ocaml is not

1070
01:04:41,880 --> 01:04:44,080
an easy language
to map to LLVM

1071
01:04:44,080 --> 01:04:46,240
it's exception handling
requires explicit

1072
01:04:46,240 --> 01:04:48,920
stack manipulation
Ocaml also has

1073
01:04:48,920 --> 01:04:50,560
a shadow stack
for root pointers.

1074
01:04:50,560 --> 01:04:52,920
This aspect is crucial
for performance

1075
01:04:52,920 --> 01:04:55,720
while LLVM offers some support
for garbage collectors.

1076
01:04:55,720 --> 01:04:58,080
It does not get
a specific to the needs

1077
01:04:58,080 --> 01:05:01,160
of OCaml. Through Duplo
we made significant progress though.

1078
01:05:01,160 --> 01:05:03,160
We have added
the required instruction

1079
01:05:03,160 --> 01:05:06,680
to a single portion
of the target specific pipeline

1080
01:05:06,680 --> 01:05:09,120
in order to handle
the frame tables used by the,

1081
01:05:09,120 --> 01:05:13,280
the default OCaml
garbage collector and run time.

1082
01:05:13,280 --> 01:05:16,480
However, to achieve
the same ideal at the LLVM IR level,

1083
01:05:16,480 --> 01:05:19,720
extensive changes
are required.

1084
01:05:21,840 --> 01:05:24,720
To illustrate LLIR,
we can look at the simple methods

1085
01:05:24,720 --> 01:05:26,920
in LLIR form,
which confused the magnitude

1086
01:05:26,920 --> 01:05:30,240
of a vector allocated
on your OCaml heap.

1087
01:05:30,240 --> 01:05:32,640
It reads an argument,
a pointer to a pair

1088
01:05:32,640 --> 01:05:34,920
using an arg instruction.
Individual elements

1089
01:05:34,920 --> 01:05:37,520
are loaded using LD
from offsets computed

1090
01:05:37,520 --> 01:05:40,920
using basic pointer arithmetic,
the elements of squared

1091
01:05:40,920 --> 01:05:44,320
using the multiply instruction,
are passed on to the sqrt function

1092
01:05:44,320 --> 01:05:46,000
before being boxed
into a heap allocated.

1093
01:05:46,640 --> 01:05:49,000
block, we can
notice the annotations,

1094
01:05:49,000 --> 01:05:50,960
the OCaml runtime requires us

1095
01:05:50,960 --> 01:05:53,120
to identify all heap pointers,

1096
01:05:53,120 --> 01:05:54,760
which we do using @camel_value.

1097
01:05:54,760 --> 01:05:57,200
And emit frame tables
describing those which are live

1098
01:05:57,200 --> 01:05:59,280
across call sites that can allocate,

1099
01:05:59,280 --> 01:06:03,160
all marked through @caml_frame.

1100
01:06:03,160 --> 01:06:04,840
Finding a common representation

1101
01:06:04,840 --> 01:06:06,800
is the only part of the problem.

1102
01:06:06,800 --> 01:06:10,080
In this representation,
optimizations,

1103
01:06:10,080 --> 01:06:13,160
opportunities to prefer
optimizations should be scarce

1104
01:06:13,160 --> 01:06:15,520
OCaml should have
already optimized everything

1105
01:06:15,520 --> 01:06:18,680
LLVM should have also
optimized everything,

1106
01:06:18,680 --> 01:06:20,720
and we are left with
the nontrivial task

1107
01:06:20,720 --> 01:06:22,600
of identifying optimization
opportunities

1108
01:06:22,600 --> 01:06:24,600
that are exposed only when both OCaml

1109
01:06:24,600 --> 01:06:27,160
and C sources are present.

1110
01:06:27,160 --> 01:06:28,840
Naturally, the first step is

1111
01:06:28,840 --> 01:06:32,000
to create optimization
opportunities for other steps.

1112
01:06:32,000 --> 01:06:33,600
Inlining achieves this,

1113
01:06:33,600 --> 01:06:35,440
unfortunately, inlining
in this context

1114
01:06:35,440 --> 01:06:39,040
is not as simple as copying
a body into a callee.

1115
01:06:39,040 --> 01:06:42,040
The world is split into two
realms: OCaml and C,

1116
01:06:42,040 --> 01:06:43,840
the two communicate through a trampoline

1117
01:06:43,840 --> 01:06:47,480
recording vital information
for the runtime.

1118
01:06:47,480 --> 01:06:49,480
When we inline C into OCaml,

1119
01:06:49,480 --> 01:06:51,480
we actually create
a bigger OCaml function

1120
01:06:51,480 --> 01:06:53,280
and push the trampolines down.

1121
01:06:53,280 --> 01:06:56,080
Even though G initially called
F without a trampoline,

1122
01:06:56,080 --> 01:07:00,280
after in-lining, it requires
one, each kind of allocation,

1123
01:07:00,280 --> 01:07:03,120
doesn't need one, in OCaml FF,

1124
01:07:03,120 --> 01:07:04,480
we are left with a mess.

1125
01:07:04,480 --> 01:07:07,080
The function now tracks heap
roots using both OCaml values

1126
01:07:07,080 --> 01:07:12,080
and the specific root measurement
mechanism specific to C.

1127
01:07:12,080 --> 01:07:14,240
In the future, we aim to
address this problem

1128
01:07:14,240 --> 01:07:16,680
by moving roots over to
the OCaml value mechanism.

1129
01:07:16,680 --> 01:07:18,440
This is a great improvement

1130
01:07:18,440 --> 01:07:21,560
since it eliminates the upfront
cost of root management,

1131
01:07:21,560 --> 01:07:24,400
that's necessary in C.

1132
01:07:24,400 --> 01:07:26,400
As your global information,
in the future,

1133
01:07:26,400 --> 01:07:28,000
we will also build to
move C functions,

1134
01:07:28,000 --> 01:07:30,280
which allocate over
to the OCaml realm.

1135
01:07:30,280 --> 01:07:32,600
First simplify manual
root management,

1136
01:07:32,600 --> 01:07:36,080
and possibly ending up on
using OCaml value annotations

1137
01:07:36,080 --> 01:07:38,680
to identify roots.

1138
01:07:38,680 --> 01:07:40,920
While you're not there
yet, at some point,

1139
01:07:40,920 --> 01:07:43,120
we will be able to not care
about whether our function

1140
01:07:43,120 --> 01:07:46,280
is OCaml specific or C
specific erasing the boundary

1141
01:07:46,280 --> 01:07:48,880
between the two heaps.

1142
01:07:48,880 --> 01:07:50,200
To illustrate some more problems,

1143
01:07:50,200 --> 01:07:52,600
we consider a function that
increments of double,

1144
01:07:52,600 --> 01:07:56,080
unboxing a value, adding one
to it and boxing it back,

1145
01:07:56,080 --> 01:07:58,560
sending it back to the OCaml heap.

1146
01:07:58,560 --> 01:07:59,760
After in-lining,

1147
01:07:59,760 --> 01:08:02,640
we see that the boxing
Ocaml copy double call

1148
01:08:02,640 --> 01:08:06,440
is now annotated,
and it has a trampoline.

1149
01:08:06,440 --> 01:08:10,080
But what happens if we inline
two consecutive invocations?

1150
01:08:10,080 --> 01:08:14,120
Unfortunately, we get the chain
of boxing unboxing steps.

1151
01:08:14,120 --> 01:08:16,520
This is expected
and quite unfortunate.

1152
01:08:16,520 --> 01:08:19,080
We are now faced with
the nontrivial task

1153
01:08:19,080 --> 01:08:22,920
of understanding pointers
and blocks on the heap

1154
01:08:22,920 --> 01:08:25,160
to perform even
a simple transformation,

1155
01:08:25,160 --> 01:08:29,120
such as constant propagation.

1156
01:08:29,120 --> 01:08:31,680
And the stage we are at in
the pipeline does not help,

1157
01:08:31,680 --> 01:08:32,840
the common ground between C

1158
01:08:32,840 --> 01:08:34,400
and OCaml memory
representations

1159
01:08:34,400 --> 01:08:36,360
is not particularly verbose.

1160
01:08:36,360 --> 01:08:38,720
We can work only with the assumption

1161
01:08:38,720 --> 01:08:40,200
that pointers have stored

1162
01:08:40,200 --> 01:08:43,520
at addresses aligned to eight bytes.

1163
01:08:43,520 --> 01:08:45,480
We essentially have no
type of information.

1164
01:08:45,480 --> 01:08:48,320
And if, even if we have
type information,

1165
01:08:48,320 --> 01:08:50,800
due to the fact that C has unions,

1166
01:08:50,800 --> 01:08:53,400
the type information you could
possibly keep would be minimal,

1167
01:08:53,400 --> 01:08:58,040
and the only piece of
information we'll be left with

1168
01:08:58,040 --> 01:09:02,480
is this fact that pointers
are aligned to eight bytes,

1169
01:09:02,480 --> 01:09:06,120
which allows us to model
the heap as bags of pointers

1170
01:09:06,120 --> 01:09:08,960
aligned as specific addresses.

1171
01:09:08,960 --> 01:09:11,800
As a proof of concept, we have
built two transformations

1172
01:09:11,800 --> 01:09:13,080
working with pointers.

1173
01:09:13,080 --> 01:09:17,200
A local analysis to propagate
value store to memory

1174
01:09:17,200 --> 01:09:18,680
to loads reading them

1175
01:09:18,680 --> 01:09:21,640
and a global analysis to
identify indirect call targets

1176
01:09:21,640 --> 01:09:25,200
essential for dead code elimination.

1177
01:09:25,200 --> 01:09:27,440
Presently, our optimizer
implements a number

1178
01:09:27,440 --> 01:09:29,400
of classic transformations
which are required

1179
01:09:29,400 --> 01:09:31,680
mostly to eliminate inefficiencies

1180
01:09:31,680 --> 01:09:35,520
in the C, OCaml, LLIR lowering.

1181
01:09:35,520 --> 01:09:38,000
While we have exposed
many new opportunities,

1182
01:09:38,000 --> 01:09:40,400
our goal for the moment
was to implement

1183
01:09:40,400 --> 01:09:42,440
the minimal battery of
transformations required

1184
01:09:42,440 --> 01:09:45,560
to emit code as fast as,

1185
01:09:45,560 --> 01:09:49,800
at least as fast as
it was emitted by,

1186
01:09:49,800 --> 01:09:52,960
OCaml and C previously.

1187
01:09:52,960 --> 01:09:55,320
The other transformations
include sparse condition

1188
01:09:55,320 --> 01:09:56,480
and constant propagation,

1189
01:09:56,480 --> 01:09:59,760
unreachable code elimination
control flow simplification

1190
01:09:59,760 --> 01:10:01,400
and these are schedules
after inlining

1191
01:10:01,400 --> 01:10:04,160
and simplify
the program quite a bit.

1192
01:10:04,160 --> 01:10:05,720
Tail recursion elimination is vital.

1193
01:10:05,720 --> 01:10:08,200
LLVM cannot lower direct tail recursion

1194
01:10:08,200 --> 01:10:09,960
as effectively as OCaml,

1195
01:10:09,960 --> 01:10:12,760
but it generates identical code

1196
01:10:12,760 --> 01:10:16,760
if these functions are
turned into loops instead.

1197
01:10:16,760 --> 01:10:19,200
An interesting transformation
specific to this context

1198
01:10:19,200 --> 01:10:20,760
is trampoline elimination.

1199
01:10:20,760 --> 01:10:22,960
OCaml requires a trampoline

1200
01:10:22,960 --> 01:10:24,840
to switch from
the OCaml calling convention

1201
01:10:24,840 --> 01:10:26,840
to the C calling
convention when invoking

1202
01:10:26,840 --> 01:10:28,600
a C method indirectly.

1203
01:10:28,600 --> 01:10:30,760
Since you have global information,

1204
01:10:30,760 --> 01:10:32,720
we can change the calling
convention of C method

1205
01:10:32,720 --> 01:10:34,320
to the OCaml one,

1206
01:10:34,320 --> 01:10:37,720
completely eliminating
these trampolines.

1207
01:10:37,720 --> 01:10:38,840
These transformations,

1208
01:10:38,840 --> 01:10:42,600
along with the LLVM code generator
provide some benefit.

1209
01:10:42,600 --> 01:10:44,760
We benchmarked our optimizer
on a subset of benchmarks

1210
01:10:44,760 --> 01:10:47,160
from operf-macro
and operf-micro.

1211
01:10:47,160 --> 01:10:49,720
Unfortunately, we did not yet
implement some optimizations

1212
01:10:49,720 --> 01:10:51,160
performed by the OCaml compiler

1213
01:10:51,160 --> 01:10:54,040
which we have disabled in
our reference version.

1214
01:10:54,040 --> 01:10:56,960
We do not cache some
globals in registers

1215
01:10:56,960 --> 01:10:59,960
and we do not inline allocations.

1216
01:10:59,960 --> 01:11:01,960
Despite these mitigations,

1217
01:11:01,960 --> 01:11:05,560
the relative results should
still be relevant.

1218
01:11:05,560 --> 01:11:07,640
Our linker delivers
a significant reduction

1219
01:11:07,640 --> 01:11:09,480
in code size while LLVM manages

1220
01:11:09,480 --> 01:11:12,600
to select better instructions
providing some improvement

1221
01:11:12,600 --> 01:11:13,880
on large informations

1222
01:11:13,880 --> 01:11:16,400
and we're quite happy
about these results.

1223
01:11:16,400 --> 01:11:19,360
When comparing
the fact of the small set

1224
01:11:19,360 --> 01:11:21,360
of transformations we
have implemented,

1225
01:11:21,360 --> 01:11:23,440
the effect is moderate.

1226
01:11:23,440 --> 01:11:25,080
There are not many opportunities

1227
01:11:25,080 --> 01:11:26,880
for cross-language optimization

1228
01:11:26,880 --> 01:11:29,000
as our inlining heuristics
and points-to analyses

1229
01:11:29,000 --> 01:11:30,440
are quite primitive.

1230
01:11:30,440 --> 01:11:33,360
As an example, we currently
inline aggressively functions

1231
01:11:33,360 --> 01:11:35,040
which box or
deconstruct arguments

1232
01:11:35,040 --> 01:11:38,640
in the hope of propagating
constants through them.

1233
01:11:38,640 --> 01:11:41,120
In the future, we have to
build better heuristics

1234
01:11:41,120 --> 01:11:42,720
to base our inline decision on how,

1235
01:11:42,720 --> 01:11:44,600
for the purpose of this project,

1236
01:11:44,600 --> 01:11:46,560
the difficultly was in figuring out

1237
01:11:46,560 --> 01:11:50,000
how to do inlining
in the first place.

1238
01:11:50,000 --> 01:11:51,680
We do gain in performance a bit

1239
01:11:51,680 --> 01:11:53,800
by mitigating inefficiencies
in both back ends

1240
01:11:53,800 --> 01:11:56,280
and inlining some small
methods through the

1241
01:11:56,280 --> 01:12:00,360
battery of transformations
we have presented earlier.

1242
01:12:00,360 --> 01:12:02,440
To sum up, we have presented
a Duplo framework

1243
01:12:02,440 --> 01:12:03,840
to inline C into OCaml,

1244
01:12:03,840 --> 01:12:06,880
analyzing and transforming
the resulting code.

1245
01:12:06,880 --> 01:12:09,120
We built a new representation
to capture both

1246
01:12:09,120 --> 01:12:12,280
OCaml and C which we call LLIR.

1247
01:12:12,280 --> 01:12:14,160
Finally, as our code generator,

1248
01:12:14,160 --> 01:12:16,960
we assure how OCaml can be lowered

1249
01:12:16,960 --> 01:12:18,520
to part of the LLVM pipeline,

1250
01:12:18,520 --> 01:12:21,760
yielding and improving in
code size and performance.

1251
01:12:21,760 --> 01:12:29,720
(CROWN APPLAUDS)

1252
01:12:29,720 --> 01:12:31,960
RICHARD: Thanks very much, Nandor,

1253
01:12:31,960 --> 01:12:34,600
who's now available in
the New York Time Band

1254
01:12:34,600 --> 01:12:36,480
for question and answer,

1255
01:12:36,480 --> 01:12:44,680
you can click the link in Clowdr.

1256
01:13:01,280 --> 01:13:03,640
Now we have Vikraman Choudhury

1257
01:13:03,640 --> 01:13:06,800
discussing how to embed
purity in an impure language

1258
01:13:06,800 --> 01:13:12,360
via the use of a comonad.

1259
01:13:12,360 --> 01:13:16,320
VIKRAMAN: Hello, I'm
Vikraman Choudhury,

1260
01:13:16,320 --> 01:13:18,080
and I'm going to be presenting

1261
01:13:18,080 --> 01:13:22,360
some joint work with Neel
Krishnaswami on our paper,

1262
01:13:22,360 --> 01:13:28,680
'Recovering Purity With
Comonads and Capabilities'.

1263
01:13:28,680 --> 01:13:32,520
The basic problem in this
paper is the following,

1264
01:13:32,520 --> 01:13:35,200
we've known for a while
now how to extend

1265
01:13:35,200 --> 01:13:38,360
a purely functional
language with monads

1266
01:13:38,360 --> 01:13:41,760
to encode side effects.

1267
01:13:41,760 --> 01:13:46,360
However, what if we already have
an impure functional language?

1268
01:13:46,360 --> 01:13:48,360
Can we extend the type system

1269
01:13:48,360 --> 01:13:52,760
to encode the absence of effects?

1270
01:13:52,760 --> 01:13:54,480
So let's try to motivate

1271
01:13:54,480 --> 01:14:01,120
why such a thing might be
useful with some examples.

1272
01:14:01,120 --> 01:14:04,640
Consider a strict call
by value language,

1273
01:14:04,640 --> 01:14:06,640
which allows side effects

1274
01:14:06,640 --> 01:14:11,000
and let's consider
the map functional.

1275
01:14:11,000 --> 01:14:13,000
So map is a higher order function,

1276
01:14:13,000 --> 01:14:15,080
which takes the function f

1277
01:14:15,080 --> 01:14:17,600
and applies it to every
element of the list recursively.

1278
01:14:17,600 --> 01:14:20,080
There are at least two
different ways of writing map.

1279
01:14:24,560 --> 01:14:25,560
In map1,

1280
01:14:25,560 --> 01:14:29,320
we recursively call map1
on the tail of the list

1281
01:14:29,320 --> 01:14:32,960
before evaluating f on
the head of the list.

1282
01:14:32,960 --> 01:14:34,600
In map2,

1283
01:14:34,600 --> 01:14:37,200
we evaluate f on
the head of the list

1284
01:14:37,200 --> 01:14:42,360
before recursively calling
map2 on the tail of the list.

1285
01:14:42,360 --> 01:14:45,920
So these two functions
should be equal,

1286
01:14:45,920 --> 01:14:48,720
but in an impure language like this,

1287
01:14:48,720 --> 01:14:51,440
they can have different
side effects.

1288
01:14:51,440 --> 01:14:53,200
So, as an example,

1289
01:14:53,200 --> 01:14:58,080
let's evaluate map1 and map2
 on the following inputs.

1290
01:14:58,080 --> 01:15:01,480
So here f is the identity
function on strings,

1291
01:15:01,480 --> 01:15:02,880
but it's impure.

1292
01:15:02,880 --> 01:15:05,760
It prints its input
string to standard out

1293
01:15:05,760 --> 01:15:09,680
before returning it.

1294
01:15:09,680 --> 01:15:13,160
So let's try to use map1.

1295
01:15:13,160 --> 01:15:15,960
We expand the let and we observe

1296
01:15:15,960 --> 01:15:19,160
that it reaches the end of the list

1297
01:15:19,160 --> 01:15:23,360
when it tries to evaluate
f on the string right.

1298
01:15:23,360 --> 01:15:26,600
So this prints right.

1299
01:15:26,600 --> 01:15:30,880
Reducing more we see
that it calls f on to,

1300
01:15:30,880 --> 01:15:32,720
which prints to,

1301
01:15:32,720 --> 01:15:37,200
and then it calls f on left,
which prints left

1302
01:15:37,200 --> 01:15:41,360
and finally it
returns the original list.

1303
01:15:41,360 --> 01:15:47,720
So it produced a side effect
where it printed right to left.

1304
01:15:47,720 --> 01:15:51,880
If however, we reduce map2
 on the same inputs,

1305
01:15:51,880 --> 01:15:56,880
we notice that it evaluates
f first on the string left,

1306
01:15:56,880 --> 01:16:01,200
then on the string to
and then on right,

1307
01:16:01,200 --> 01:16:04,680
and finally returns
the original list.

1308
01:16:04,680 --> 01:16:07,040
So even though both of
these map functions

1309
01:16:07,040 --> 01:16:08,920
return the same output,

1310
01:16:08,920 --> 01:16:15,120
they produce different side effects.

1311
01:16:15,120 --> 01:16:19,040
Alternatively consider
these two functions f and g

1312
01:16:19,040 --> 01:16:20,720
from Int to Int.

1313
01:16:21,480 --> 01:16:23,560
They perform different side effects

1314
01:16:23,560 --> 01:16:26,840
before returning their output.

1315
01:16:26,840 --> 01:16:30,120
Now, suppose we map g on xs,

1316
01:16:30,120 --> 01:16:34,200
followed by mapping f on the output.

1317
01:16:34,200 --> 01:16:38,080
An optimizing compiler
could perform map fusion

1318
01:16:38,080 --> 01:16:40,080
on this expression.

1319
01:16:40,080 --> 01:16:42,800
But in this impure language,

1320
01:16:42,800 --> 01:16:46,080
if we mapped f compose g on xs,

1321
01:16:46,080 --> 01:16:49,080
that would produce
a different side effect.

1322
01:16:49,080 --> 01:16:54,080
So this map fusion optimization
pass would be invalid.

1323
01:16:55,400 --> 01:16:59,120
So I hope these examples
are motivating

1324
01:16:59,120 --> 01:17:02,800
and they show that it
would indeed be useful

1325
01:17:02,800 --> 01:17:06,000
to be able to encode
the absence of effects

1326
01:17:06,000 --> 01:17:07,840
in an impure language.

1327
01:17:07,840 --> 01:17:11,120
So first let's see how
effects are being encoded

1328
01:17:11,120 --> 01:17:16,640
in this language with some examples.

1329
01:17:16,640 --> 01:17:19,480
Let's consider this function g.

1330
01:17:19,480 --> 01:17:23,680
It takes a channel c, a string s,

1331
01:17:23,680 --> 01:17:28,840
prints s on the channel
c, then returns s.

1332
01:17:28,840 --> 01:17:31,720
So this function is impure
because it uses the channel

1333
01:17:31,720 --> 01:17:33,600
it was passed to,

1334
01:17:33,600 --> 01:17:37,200
to perform an effect.

1335
01:17:37,200 --> 01:17:42,320
Now consider this function
f from String to String.

1336
01:17:42,320 --> 01:17:44,680
This is also impure,

1337
01:17:44,680 --> 01:17:50,240
but it is defined by partially
applying g to standard out.

1338
01:17:50,240 --> 01:17:52,800
Now standard out is
a channel that's available

1339
01:17:52,800 --> 01:17:56,560
in the global environment.

1340
01:17:56,560 --> 01:18:00,240
So there's a difference
between f and g.

1341
01:18:00,240 --> 01:18:02,360
g isn't safe,

1342
01:18:02,360 --> 01:18:03,680
because it uses the channel

1343
01:18:03,680 --> 01:18:06,200
that was passed as an argument.

1344
01:18:06,200 --> 01:18:10,080
But f uses standard
out which is used

1345
01:18:10,080 --> 01:18:12,960
from the global environment.

1346
01:18:12,960 --> 01:18:15,960
So, this gives us an idea,

1347
01:18:15,960 --> 01:18:18,400
we should be able to
control the use of effects

1348
01:18:18,400 --> 01:18:20,680
by controlling the channel variables

1349
01:18:20,680 --> 01:18:24,680
that a function has access
to in its environment.

1350
01:18:24,680 --> 01:18:26,960
So we were interested in purity,

1351
01:18:26,960 --> 01:18:33,400
but we can refine this to
a different notion of safety.

1352
01:18:33,400 --> 01:18:38,640
So we formalize this idea using
the notion of capabilities.

1353
01:18:38,640 --> 01:18:40,560
Capabilities are
a well known thing

1354
01:18:40,560 --> 01:18:44,600
in the systems
programming community.

1355
01:18:44,600 --> 01:18:48,600
A capability is defined to
be a token of authority,

1356
01:18:48,600 --> 01:18:51,280
for example, a security token

1357
01:18:51,280 --> 01:18:54,480
or a permission.

1358
01:18:54,480 --> 01:18:56,360
In our impure language,

1359
01:18:56,360 --> 01:18:59,200
a channel variable, for
example, standard out

1360
01:18:59,200 --> 01:19:02,680
is a capability.

1361
01:19:02,680 --> 01:19:06,480
Then we define the notion
of capability safety.

1362
01:19:06,480 --> 01:19:09,160
We say that a program
is capability safe

1363
01:19:09,160 --> 01:19:12,840
when it only performs effects
using the capabilities

1364
01:19:12,840 --> 01:19:15,840
it has access to at runtime.

1365
01:19:15,840 --> 01:19:20,440
So for example, this function
g is capability safe,

1366
01:19:20,440 --> 01:19:24,480
but f is not.

1367
01:19:24,480 --> 01:19:28,680
Ah, but g is not pure, it's only safe.

1368
01:19:28,680 --> 01:19:30,080
So, we'll come back to how

1369
01:19:30,080 --> 01:19:33,440
we could encode purity using safety.

1370
01:19:33,440 --> 01:19:39,760
But for now, let's try to extend
our language with safety.

1371
01:19:39,760 --> 01:19:41,440
So first, we want to distinguish

1372
01:19:41,440 --> 01:19:43,760
between two kinds of variables,

1373
01:19:43,760 --> 01:19:47,480
safe variables and impure variables.

1374
01:19:47,480 --> 01:19:52,040
Safe variables, are capability
safe and impure variables

1375
01:19:52,040 --> 01:19:55,840
are just arbitrary
variables in our language.

1376
01:19:55,840 --> 01:20:01,320
So we use these qualifiers,
s or i in blue and red

1377
01:20:01,320 --> 01:20:06,240
to annotate the bindings of
variables in our context.

1378
01:20:06,240 --> 01:20:10,400
Then, we would like to
internalize the notion of safety

1379
01:20:10,400 --> 01:20:13,520
using a safe type constructor.

1380
01:20:13,520 --> 01:20:16,720
We add a box type constructor

1381
01:20:16,720 --> 01:20:22,200
and we add two new term
formers, box and let box.

1382
01:20:22,200 --> 01:20:25,040
So, these are expressions and

1383
01:20:25,040 --> 01:20:28,760
we distinguish between
expressions and values.

1384
01:20:28,760 --> 01:20:33,120
Boxing an expression
turns it into a value.

1385
01:20:33,120 --> 01:20:36,800
We also have a new safety judgment

1386
01:20:36,800 --> 01:20:41,040
which we use to judge
whether a term e of type A

1387
01:20:41,040 --> 01:20:46,520
is safe in context Gamma.

1388
01:20:46,520 --> 01:20:49,600
So now we want to ensure
that safe variables

1389
01:20:49,600 --> 01:20:53,080
can only depend on
other safe variables.

1390
01:20:53,080 --> 01:20:57,760
So we add a substructural
operation on our contexts.

1391
01:20:57,760 --> 01:21:00,480
So this purify operation

1392
01:21:00,480 --> 01:21:05,560
drops all the impure
variables in our context.

1393
01:21:05,560 --> 01:21:09,720
Then, if we know that
a term is well typed

1394
01:21:09,720 --> 01:21:13,960
in a safe context, we
know that it's safe.

1395
01:21:13,960 --> 01:21:17,040
And if we know that a term is safe,

1396
01:21:17,040 --> 01:21:19,160
we can put it inside the box

1397
01:21:19,160 --> 01:21:22,840
using our box introduction rule.

1398
01:21:23,880 --> 01:21:28,040
And finally, we also have
a let box elimination rule

1399
01:21:28,040 --> 01:21:30,840
for our box type constructor.

1400
01:21:30,840 --> 01:21:33,440
So if e1 is a boxed term,

1401
01:21:33,440 --> 01:21:36,520
we can bind
the underlying value x.

1402
01:21:36,520 --> 01:21:39,160
And we can eliminate to e2

1403
01:21:39,160 --> 01:21:42,960
if it is well typed in
the extended context,

1404
01:21:42,960 --> 01:21:49,000
where x is marked with
a safe annotation.

1405
01:21:49,000 --> 01:21:53,120
So these were the additional
typing rules for the box type.

1406
01:21:53,120 --> 01:21:58,200
Let's also take a quick look
at the other typing rules.

1407
01:21:58,200 --> 01:22:03,240
The rules for variables
remains essentially unchanged.

1408
01:22:03,240 --> 01:22:05,960
For functions, to introduce a lambda

1409
01:22:05,960 --> 01:22:09,120
we allow the bound
variable to be impure

1410
01:22:09,120 --> 01:22:12,640
so that we can substitute
arbitrary expressions.

1411
01:22:13,320 --> 01:22:18,280
The Lambda application
rule remains the same.

1412
01:22:18,280 --> 01:22:23,080
And then finally, the rule for
print is just as expected.

1413
01:22:23,080 --> 01:22:26,720
It's simply a unit.

1414
01:22:27,760 --> 01:22:30,480
Now, at this point we note
that this box constructor

1415
01:22:30,480 --> 01:22:32,920
is a comonad.

1416
01:22:32,920 --> 01:22:37,640
It has a co-unit,
and it has a co-multiplication.

1417
01:22:37,640 --> 01:22:40,480
And in fact, our syntax
for adding comonads

1418
01:22:40,480 --> 01:22:44,800
to our type theory is inspired
by the classic Pfenning Davies

1419
01:22:44,800 --> 01:22:51,000
paper on the judgmental
reconstruction of S4 modal logic.

1420
01:22:51,000 --> 01:22:53,040
Instead of dual contexts,

1421
01:22:53,040 --> 01:22:56,440
we use annotations on our variables.

1422
01:22:56,440 --> 01:23:03,280
And for more details on
the syntax, see our paper.

1423
01:23:03,280 --> 01:23:06,160
So coming back to our
original motivation,

1424
01:23:06,160 --> 01:23:10,360
can we used this notion of
safety to recover purity?

1425
01:23:10,360 --> 01:23:15,600
Well, we say that a function
is pure if it is safe

1426
01:23:15,600 --> 01:23:20,520
and it has access to zero
capabilities in its environment.

1427
01:23:20,520 --> 01:23:25,240
So box of A arrow B is
the type of safe functions

1428
01:23:25,240 --> 01:23:28,200
but they're not necessarily pure.

1429
01:23:28,200 --> 01:23:34,560
And box of box of A arrow
B is the the type of pure functions.

1430
01:23:34,560 --> 01:23:37,400
So going back to our
original example,

1431
01:23:37,400 --> 01:23:39,320
we can now write a pure variant

1432
01:23:39,320 --> 01:23:44,040
of our map1
and map2 functions.

1433
01:23:44,040 --> 01:23:47,800
We force the function
argument to be pure

1434
01:23:47,800 --> 01:23:54,280
and it should be only applied
to a list of safe expressions.

1435
01:23:54,280 --> 01:23:57,840
We can use box and let
box to rewrite the body

1436
01:23:57,840 --> 01:24:00,720
of map1 and map2.

1437
01:24:00,720 --> 01:24:05,800
So using this new definition
for map1 and map2,

1438
01:24:05,800 --> 01:24:09,040
we can show that they
are provably equal

1439
01:24:09,040 --> 01:24:13,640
in the extended calculus.

1440
01:24:13,640 --> 01:24:17,320
Finally to conclude, let me
point out some of the main

1441
01:24:17,320 --> 01:24:20,400
takeaways from this talk.

1442
01:24:20,400 --> 01:24:26,080
First, we can use this notion
of capabilities to encode effects,

1443
01:24:26,080 --> 01:24:30,640
and to control the usage of effects.

1444
01:24:30,640 --> 01:24:34,960
This idea of capability
safety or denying the usage

1445
01:24:34,960 --> 01:24:38,320
of a capability from
the environment can be captured

1446
01:24:38,320 --> 01:24:43,360
using a comonadic
type discipline.

1447
01:24:43,360 --> 01:24:45,840
Let me also highlight some
of the technical results

1448
01:24:45,840 --> 01:24:48,320
in our paper.

1449
01:24:48,320 --> 01:24:52,440
We formalize an effectful
calculus and we extend it

1450
01:24:52,440 --> 01:24:57,000
with a safety modality and we
show how to do substitution

1451
01:24:57,000 --> 01:25:00,120
and so on.

1452
01:25:00,120 --> 01:25:04,680
We show how to recover purity
from capability safety.

1453
01:25:04,680 --> 01:25:09,240
And we also discuss how
to do capability taming,

1454
01:25:09,240 --> 01:25:13,000
that is, how to incrementally
add safety to an already

1455
01:25:13,000 --> 01:25:17,240
impure language or a library.

1456
01:25:17,240 --> 01:25:21,080
We give a categorical
semantics for this language.

1457
01:25:21,080 --> 01:25:24,320
It is given by a cartesian
closed category

1458
01:25:24,320 --> 01:25:26,200
with a monad
and a comonad

1459
01:25:26,200 --> 01:25:29,560
which interact with each other.

1460
01:25:29,560 --> 01:25:33,240
We give a construction of
capability spaces,

1461
01:25:33,240 --> 01:25:36,400
which give us
a denotational semantics,

1462
01:25:36,400 --> 01:25:40,640
which satisfies the axiomatic
categorical structure

1463
01:25:40,640 --> 01:25:44,320
that we need for our semantics.

1464
01:25:44,320 --> 01:25:46,520
The idea of this
construction in fact

1465
01:25:46,520 --> 01:25:50,640
comes from an old paper
of Martin Hofmann.

1466
01:25:50,640 --> 01:25:53,480
Also capability spaces
have more structure

1467
01:25:53,480 --> 01:25:57,080
than we actually use
for our semantics.

1468
01:25:57,080 --> 01:26:00,640
It can be used to give
a model of linear type theory,

1469
01:26:00,640 --> 01:26:04,560
which we discuss in the paper.

1470
01:26:04,560 --> 01:26:08,320
We also give the full equational
theory for this calculus

1471
01:26:08,320 --> 01:26:11,000
and we use our
denotational semantics

1472
01:26:11,000 --> 01:26:14,280
to prove its soundness.

1473
01:26:14,280 --> 01:26:17,040
Finally, we also show how to embed

1474
01:26:17,040 --> 01:26:19,560
the pure simply-typed
lambda calculus

1475
01:26:19,560 --> 01:26:25,120
into our language, using
this comonadic modality.

1476
01:26:25,120 --> 01:26:27,680
This embedding, preserves
the equational theory

1477
01:26:27,680 --> 01:26:30,640
of the pure STLC.

1478
01:26:30,640 --> 01:26:34,000
Also, we show that our language
is a conservative extension

1479
01:26:34,000 --> 01:26:36,840
of the STLC.

1480
01:26:36,840 --> 01:26:40,200
This is a requirement for
a purely functional language

1481
01:26:40,200 --> 01:26:45,880
that Amr Sabry told us
about a long time ago.

1482
01:26:45,880 --> 01:26:49,560
And yeah, that's all I had to say.

1483
01:26:49,560 --> 01:26:52,120
Please see the extended
version of our paper

1484
01:26:52,120 --> 01:26:53,480
if you're interested

1485
01:26:53,480 --> 01:26:58,400
and thank you very much
for listening.

1486
01:26:58,400 --> 01:27:06,760
(AUDIENCE CLAPS)

1487
01:27:06,760 --> 01:27:08,440
RICHARD: Thanks Vikraman.

1488
01:27:08,440 --> 01:27:12,440
He is now available in both
the New York and Asia time bands

1489
01:27:12,440 --> 01:27:14,400
for question and answer.

1490
01:27:14,400 --> 01:27:17,000
I will take a brief pause
for the next talk at 12:15

1491
01:27:17,000 --> 01:27:23,120
in the New York time band.

1492
01:28:01,280 --> 01:28:04,120
Next up, we have Xiaohong
Chen to present

1493
01:28:04,120 --> 01:28:09,120
a general approach to define
binders using matching logic.

1494
01:28:09,120 --> 01:28:10,280
XIAOHONG CHEN: Hello everyone.

1495
01:28:10,280 --> 01:28:11,920
Welcome to this video.

1496
01:28:11,920 --> 01:28:13,680
My name is Xiaohong Chen.

1497
01:28:13,680 --> 01:28:14,920
My talk today is about

1498
01:28:14,920 --> 01:28:18,880
a general approach to define
binders using matching logic.

1499
01:28:18,880 --> 01:28:21,280
This is a joint work
with my supervisor

1500
01:28:21,280 --> 01:28:23,080
Professor Grigore Rosu.

1501
01:28:23,080 --> 01:28:26,000
Our papers can be found
at the links below.

1502
01:28:26,000 --> 01:28:28,440
The technical report includes
all the proof details

1503
01:28:28,440 --> 01:28:30,640
in the appendix.

1504
01:28:30,640 --> 01:28:32,400
The main motivation of the work

1505
01:28:32,400 --> 01:28:34,400
is the K formal semantic framework

1506
01:28:34,400 --> 01:28:37,440
and its logical foundation,
matching logic.

1507
01:28:37,440 --> 01:28:40,640
The K framework allows users to
define the formal semantics

1508
01:28:40,640 --> 01:28:43,120
of any programming languages.

1509
01:28:43,120 --> 01:28:46,920
As shown in the picture, once
the semantics is defined,

1510
01:28:46,920 --> 01:28:48,320
all language tools

1511
01:28:48,320 --> 01:28:50,440
such as parsers, interpreters,

1512
01:28:50,440 --> 01:28:55,000
and verifiers are
automatically generated by K.

1513
01:28:55,000 --> 01:28:57,600
In practice, K has been used
to define

1514
01:28:57,600 --> 01:29:00,120
the formal semantics
of many real world languages,

1515
01:29:00,120 --> 01:29:03,480
such as C, Java
and JavaScript.

1516
01:29:03,480 --> 01:29:04,880
K allows users to define

1517
01:29:04,880 --> 01:29:08,440
the common language features
easily, including binders.

1518
01:29:08,440 --> 01:29:11,280
For example, the following K
definition defines the syntax

1519
01:29:11,280 --> 01:29:13,120
of lambda calculus.

1520
01:29:13,120 --> 01:29:16,600
An expression is either a variable
or an expression applied

1521
01:29:16,600 --> 01:29:17,880
to another expression

1522
01:29:17,880 --> 01:29:22,320
or has the form lambda var.e.

1523
01:29:22,320 --> 01:29:24,040
The binder is an attribute,

1524
01:29:24,040 --> 01:29:27,600
which will trigger the internal
substitution mechanism in K

1525
01:29:27,600 --> 01:29:29,560
to provide the correct implementation

1526
01:29:29,560 --> 01:29:31,600
of capture-avoiding substitution.

1527
01:29:31,600 --> 01:29:34,360
Currently, this only works
for a restricted form

1528
01:29:34,360 --> 01:29:36,800
of binders but with this paper,

1529
01:29:36,800 --> 01:29:38,440
we can give the binding attributes

1530
01:29:38,440 --> 01:29:40,800
a clean, mathematical explanation,

1531
01:29:40,800 --> 01:29:43,600
which will make K to support
more general binders

1532
01:29:43,600 --> 01:29:45,440
in the future.

1533
01:29:45,440 --> 01:29:47,960
Mathematically speaking,
every K definition

1534
01:29:47,960 --> 01:29:50,440
is a logical theory of matching logic

1535
01:29:50,440 --> 01:29:53,480
where the syntax and semantics
of a programming language

1536
01:29:53,480 --> 01:29:56,760
are defined using
matching logic axioms.

1537
01:29:56,760 --> 01:29:58,800
So our job in this paper

1538
01:29:58,800 --> 01:30:04,360
is to show how to define binders
as matching logic theories.

1539
01:30:04,360 --> 01:30:06,760
Matching logic is expressive.

1540
01:30:06,760 --> 01:30:09,200
Many important logical
systems can be defined

1541
01:30:09,200 --> 01:30:11,080
as matching logic theories.

1542
01:30:11,080 --> 01:30:13,240
These include first-order logic,

1543
01:30:13,240 --> 01:30:17,160
separation logic, Hoare
logic for verification,

1544
01:30:17,160 --> 01:30:19,720
temporal logics,
and many others.

1545
01:30:19,720 --> 01:30:23,840
However no logical systems where
binding plays a major role

1546
01:30:23,840 --> 01:30:26,760
have been thoroughly
defined in matching logic

1547
01:30:26,760 --> 01:30:28,560
until this paper.

1548
01:30:28,560 --> 01:30:30,440
In this paper, we studied
the typical systems

1549
01:30:30,440 --> 01:30:33,440
that include binders,
such as lambda calculus

1550
01:30:33,440 --> 01:30:35,120
and type systems.

1551
01:30:35,120 --> 01:30:37,680
We defined them as
matching logic theories

1552
01:30:37,680 --> 01:30:40,480
and we proved that our
definitions are correct

1553
01:30:40,480 --> 01:30:43,120
by showing that they
preserve the syntax,

1554
01:30:43,120 --> 01:30:48,400
semantics and formal reasoning
of the target system.

1555
01:30:48,400 --> 01:30:50,720
Here is an overview of
the main contribution

1556
01:30:50,720 --> 01:30:52,160
of the paper.

1557
01:30:52,160 --> 01:30:54,520
Firstly, we simplify matching logic

1558
01:30:54,520 --> 01:30:57,040
and propose a variant
that is more suitable

1559
01:30:57,040 --> 01:30:58,360
to define binders.

1560
01:30:58,360 --> 01:31:01,840
Secondly, we use lambda
calculus as an example

1561
01:31:01,840 --> 01:31:05,600
and define it as a machine
logic theory, Gamma lambda.

1562
01:31:05,600 --> 01:31:08,440
The key observation
is that lambda X.E

1563
01:31:08,440 --> 01:31:10,440
does two things at a time.

1564
01:31:10,440 --> 01:31:12,680
It creates the binding of X in E

1565
01:31:12,680 --> 01:31:15,680
and also builds a term,
lambda X.E.

1566
01:31:15,680 --> 01:31:18,920
In matching logic, we
separate these two.

1567
01:31:18,920 --> 01:31:21,400
We first use [X]E

1568
01:31:21,400 --> 01:31:23,680
to define the graph of the function

1569
01:31:23,680 --> 01:31:25,560
that maps X to E.

1570
01:31:25,560 --> 01:31:28,760
Here, <X,E> is the pair of X and E

1571
01:31:28,760 --> 01:31:31,600
and the matching logic
exists means union.

1572
01:31:31,600 --> 01:31:35,800
So this pattern is
the set of all pairs <X,E>

1573
01:31:35,800 --> 01:31:38,960
where X ranges over the variables.

1574
01:31:38,960 --> 01:31:42,600
The intention packs
the graph set into one element,

1575
01:31:42,600 --> 01:31:46,600
which then includes all
the binding information.

1576
01:31:46,600 --> 01:31:50,840
Then we define lambda X.E by
applying the construct lambda

1577
01:31:50,840 --> 01:31:53,080
to the graph set [X]E.

1578
01:31:53,080 --> 01:31:55,640
The constructor builds the term.

1579
01:31:55,640 --> 01:31:59,760
In short, we use the built-in
exist binder of matching logic

1580
01:31:59,760 --> 01:32:02,600
to build a graph set
and create a binding

1581
01:32:02,600 --> 01:32:06,880
and then use constructors
to build the term.

1582
01:32:06,880 --> 01:32:11,080
To show that the above
definition is indeed correct,

1583
01:32:11,080 --> 01:32:13,160
we prove the following theorems.

1584
01:32:13,160 --> 01:32:16,600
We proof that Gamma lambda
is a conservative extension

1585
01:32:16,600 --> 01:32:18,000
of lambda calculus.

1586
01:32:18,000 --> 01:32:21,000
So any equations between
two lambda expressions

1587
01:32:21,000 --> 01:32:23,400
are provable in lambda calculus

1588
01:32:23,400 --> 01:32:27,080
if and only if they are probable
in theory Gamma lambda.

1589
01:32:27,080 --> 01:32:29,040
We also show that Gamma lambda

1590
01:32:29,040 --> 01:32:30,440
is sound and complete.

1591
01:32:30,440 --> 01:32:34,560
So semantic validity
and provability coincide.

1592
01:32:34,560 --> 01:32:37,200
One important advantage
of our approach

1593
01:32:37,200 --> 01:32:40,680
is that the theory Gamma
lambda automatically

1594
01:32:40,680 --> 01:32:43,360
gives a class of
matching logic models

1595
01:32:43,360 --> 01:32:46,760
and the following two theorems
connect those models

1596
01:32:46,760 --> 01:32:48,480
to lambda calculus.

1597
01:32:48,480 --> 01:32:51,560
The representative
completeness states

1598
01:32:51,560 --> 01:32:54,000
that for any lambda theory, T,

1599
01:32:54,000 --> 01:32:57,000
which is a set of lambda equations,

1600
01:32:57,000 --> 01:33:01,080
there is a model M_T of
theory Gamma lambda,

1601
01:33:01,080 --> 01:33:05,160
such that M_T captures
precisely the lambda theory T.

1602
01:33:05,160 --> 01:33:08,920
That is E1 equals E2
is provable in T,

1603
01:33:08,920 --> 01:33:12,360
if and only if it holds in M_T.

1604
01:33:12,360 --> 01:33:13,800
The last theorem states

1605
01:33:13,800 --> 01:33:16,520
that for any lambda
calculus model A,

1606
01:33:16,520 --> 01:33:18,840
there is a matching logic model, M_A,

1607
01:33:18,840 --> 01:33:22,360
of the theory Gamma
lambda that captures A.

1608
01:33:22,360 --> 01:33:24,480
Lambda calculus has
many different models.

1609
01:33:24,480 --> 01:33:26,520
In this paper, we
consider the models

1610
01:33:26,520 --> 01:33:28,840
that are concrete Cartesian
closed categories

1611
01:33:28,840 --> 01:33:31,600
or called concrete CCC models.

1612
01:33:31,600 --> 01:33:34,720
The technical details can
be found in the paper.

1613
01:33:34,720 --> 01:33:37,720
To sum up, our definition
of lambda calculus

1614
01:33:37,720 --> 01:33:39,960
that first creates the binding

1615
01:33:39,960 --> 01:33:42,760
and then, builds the term is correct

1616
01:33:42,760 --> 01:33:45,200
and captures lambda
calculus faithfully.

1617
01:33:45,200 --> 01:33:47,480
Then at the end of the paper,

1618
01:33:47,480 --> 01:33:49,960
we generalize it to other systems

1619
01:33:49,960 --> 01:33:52,160
with binders, such as system F,

1620
01:33:52,160 --> 01:33:56,000
pure type systems, et cetera.

1621
01:33:56,000 --> 01:33:58,200
In the following, I'll give
a high-level overview

1622
01:33:58,200 --> 01:33:59,440
of matching logic,

1623
01:33:59,440 --> 01:34:01,560
including the syntax semantics

1624
01:34:01,560 --> 01:34:03,800
and then as an example,

1625
01:34:03,800 --> 01:34:06,560
I'll define lambda calculus
in matching logic.

1626
01:34:06,560 --> 01:34:07,560
Due to time limit,

1627
01:34:07,560 --> 01:34:10,440
I won't discuss the generalization
part in this talk.

1628
01:34:10,440 --> 01:34:15,520
However, I'll be happy to
address it in the Q&A session.

1629
01:34:15,520 --> 01:34:18,560
Matching logic is a very
simple and minimal

1630
01:34:18,560 --> 01:34:20,480
and yet, expressive logic.

1631
01:34:20,480 --> 01:34:23,840
It is the logical foundation
of the K framework.

1632
01:34:23,840 --> 01:34:25,840
Its syntax defines formulas,

1633
01:34:25,840 --> 01:34:27,520
which we call patterns,

1634
01:34:27,520 --> 01:34:29,640
using only 7 constructs.

1635
01:34:29,640 --> 01:34:32,160
They are element variables,

1636
01:34:32,160 --> 01:34:34,480
which are like first-order logic variables

1637
01:34:34,480 --> 01:34:36,520
that range over elements.

1638
01:34:36,520 --> 01:34:39,680
Set variables, which are like
propositional variables

1639
01:34:39,680 --> 01:34:42,880
in modal logic that
range over subsets.

1640
01:34:42,880 --> 01:34:45,640
Symbols are like set variables,

1641
01:34:45,640 --> 01:34:47,920
except that their
interpretations are fixed

1642
01:34:47,920 --> 01:34:50,760
by the models and do not change.

1643
01:34:50,760 --> 01:34:53,560
Phi one, phi two is
the application value

1644
01:34:53,560 --> 01:34:55,680
where we apply phi one to phi two.

1645
01:34:55,680 --> 01:34:58,000
if phi one is
a functional constructor,

1646
01:34:58,000 --> 01:35:00,560
then the result is like
a first-order logic term.

1647
01:35:00,560 --> 01:35:02,000
If phi one is a predicate,

1648
01:35:02,000 --> 01:35:05,520
then the result is like
a first-order logic atomic formula

1649
01:35:05,520 --> 01:35:08,000
but in general, any
pattern can be applied

1650
01:35:08,000 --> 01:35:09,440
to any other pattern,

1651
01:35:09,440 --> 01:35:12,840
giving matching logic
maximum flexibility.

1652
01:35:12,840 --> 01:35:16,560
Next two constructs,
bottom and implication

1653
01:35:16,560 --> 01:35:19,480
are standard propositional
constructs.

1654
01:35:19,480 --> 01:35:23,760
Other propositional operations
can be defined from these two.

1655
01:35:23,760 --> 01:35:25,880
Finally exists x phi

1656
01:35:25,880 --> 01:35:29,720
is the existential quantification
like in first-order logic.

1657
01:35:29,720 --> 01:35:32,200
Now, we move on to the semantics.

1658
01:35:32,200 --> 01:35:34,960
The semantics of matching
logic is one sentence,

1659
01:35:34,960 --> 01:35:38,600
a pattern phi is interpreted
as the set of elements

1660
01:35:38,600 --> 01:35:42,600
that match it, in other
words, patterns mean sets.

1661
01:35:42,600 --> 01:35:46,000
Formally, a model M has
a non-empty carrier set

1662
01:35:46,000 --> 01:35:49,720
and binary application
function from M x M

1663
01:35:49,720 --> 01:35:51,640
to the power set of M,

1664
01:35:51,640 --> 01:35:54,880
which will be used to interpret
the application pattern.

1665
01:35:54,880 --> 01:35:58,920
Each symbol is also
interpreted as a subset of M.

1666
01:35:58,920 --> 01:36:02,600
Now, given a valuation that
assigns element variables

1667
01:36:02,600 --> 01:36:05,920
to elements and set
variables to sets,

1668
01:36:05,920 --> 01:36:10,920
the pattern interpretation
phi rho is defined like this,

1669
01:36:10,920 --> 01:36:14,560
for example, x evaluates
to the elements rho of x,

1670
01:36:14,560 --> 01:36:18,360
sigma evaluates to
the interpretation, sigma M,

1671
01:36:18,360 --> 01:36:20,680
bottom evaluates to the empty set,

1672
01:36:20,680 --> 01:36:23,720
meaning that no
elements can match it.

1673
01:36:23,720 --> 01:36:27,520
Application is interpreted
using the application function,

1674
01:36:27,520 --> 01:36:30,680
but now phi one
and phi two are also sets,

1675
01:36:30,680 --> 01:36:34,120
we range over their
inhabitants by a1 and a2

1676
01:36:34,120 --> 01:36:36,400
and take the union
of all the results,

1677
01:36:36,400 --> 01:36:39,640
this is called
the pointwise extension.

1678
01:36:39,640 --> 01:36:42,560
Finally, exists x phi is interpreted

1679
01:36:42,560 --> 01:36:45,480
by ranging over all elements a in M,

1680
01:36:45,480 --> 01:36:48,560
and taking the big union of
all the resulting sets,

1681
01:36:48,560 --> 01:36:53,880
in short, exists means big
union in matching logic.

1682
01:36:53,880 --> 01:36:54,880
In matching logic,

1683
01:36:54,880 --> 01:36:57,680
we use theories to capture
the target logical systems

1684
01:36:57,680 --> 01:36:59,680
such as lambda calculus,

1685
01:36:59,680 --> 01:37:02,040
a theory consists of two components,

1686
01:37:02,040 --> 01:37:04,400
a set of symbols
and a set of axioms

1687
01:37:04,400 --> 01:37:07,320
that define the behavior
of the symbols.

1688
01:37:07,320 --> 01:37:11,080
We also define notations which
are just syntactic sugar.

1689
01:37:11,080 --> 01:37:12,880
With the proper notations,

1690
01:37:12,880 --> 01:37:15,400
we can capture the formulas
of those expressions

1691
01:37:15,400 --> 01:37:19,760
in the target system as is
without any translation.

1692
01:37:19,760 --> 01:37:23,280
We say that a model M is
a model of the theory, Gamma,

1693
01:37:23,280 --> 01:37:25,600
if all axioms hold in a model,

1694
01:37:25,600 --> 01:37:29,040
that is they are interpreted
to the total sets,

1695
01:37:29,040 --> 01:37:33,120
then for each theory, we have
a class of these models

1696
01:37:33,120 --> 01:37:35,800
whose properties can be
expressed by patterns

1697
01:37:35,800 --> 01:37:39,080
for Gamma to correctly
capture the target system,

1698
01:37:39,080 --> 01:37:41,560
we need to establish an equivalence

1699
01:37:41,560 --> 01:37:43,120
between the models of Gamma

1700
01:37:43,120 --> 01:37:45,280
and the models of the target system.

1701
01:37:45,280 --> 01:37:48,840
Then, we can use matching
logic patterns to express

1702
01:37:48,840 --> 01:37:54,080
and reason about the properties
about the target system.

1703
01:37:54,080 --> 01:37:56,120
In section four of the paper,

1704
01:37:56,120 --> 01:37:58,080
we define the matching logic theories

1705
01:37:58,080 --> 01:38:01,720
that captures many important
mathematical concepts

1706
01:38:01,720 --> 01:38:04,360
such as equality, membership,

1707
01:38:04,360 --> 01:38:07,080
the concept of sorts
and many sorted functions,

1708
01:38:07,080 --> 01:38:09,760
pairs and the power
sort 2 to the s,

1709
01:38:09,760 --> 01:38:13,360
whose elements are the sets
of elements of sort s.

1710
01:38:13,360 --> 01:38:16,280
Based on these
infrastructure definitions,

1711
01:38:16,280 --> 01:38:20,560
we now define binders
in matching logic.

1712
01:38:20,560 --> 01:38:22,840
We define the theory Gamma lambda

1713
01:38:22,840 --> 01:38:24,760
that captures lambda calculus

1714
01:38:24,760 --> 01:38:28,360
due to time limits I'll only
show intuitive ideas.

1715
01:38:28,360 --> 01:38:32,160
Intuitively, the lambda
abstraction, lambda X.e,

1716
01:38:32,160 --> 01:38:36,160
creates infinitely many alpha
equivalent representations,

1717
01:38:36,160 --> 01:38:39,320
lambda X1 e, X1 substitutes for X, etc.

1718
01:38:40,000 --> 01:38:44,520
Our idea is to
capture these representations.

1719
01:38:44,520 --> 01:38:47,480
To do that we construct
an argument value pair

1720
01:38:47,480 --> 01:38:49,920
where the first argument
is the variable

1721
01:38:49,920 --> 01:38:53,120
and the second is
the substituted expression.

1722
01:38:53,120 --> 01:38:56,160
Then we define this
existential pattern.

1723
01:38:56,160 --> 01:39:00,720
Recall that semantics of existing
matching logic means set union.

1724
01:39:00,720 --> 01:39:03,720
So this pattern means
the union of all pairs.

1725
01:39:03,720 --> 01:39:07,240
In other words this pattern
is the graph of the function

1726
01:39:07,240 --> 01:39:10,040
that maps x into e.

1727
01:39:10,040 --> 01:39:14,480
To wrap up the definition we take
the intension of the graph set.

1728
01:39:14,480 --> 01:39:17,440
The intension of a set
equals to the set itself

1729
01:39:17,440 --> 01:39:21,280
but the set is no longer regarded
as a collection of elements

1730
01:39:21,280 --> 01:39:24,640
but as a single element
in a power set domain.

1731
01:39:24,640 --> 01:39:28,240
This way we can avoid
the pointwise extension

1732
01:39:28,240 --> 01:39:31,640
when we apply the constructor
lambda to the set.

1733
01:39:31,640 --> 01:39:36,760
We introduce a notation [X]E to
mean the intension of the graph set

1734
01:39:36,760 --> 01:39:38,200
which is one element

1735
01:39:38,200 --> 01:39:42,160
that includes all the binding
information of x into e.

1736
01:39:42,160 --> 01:39:46,600
And finally, we define lambda x.e
 as the application pattern

1737
01:39:46,600 --> 01:39:51,560
where the constructed lambda is
applied to the graph set [X]E.

1738
01:39:51,560 --> 01:39:54,520
In short, to define
lambda abstraction

1739
01:39:54,520 --> 01:39:58,920
we first use the building binder
exist to define the graph set

1740
01:39:58,920 --> 01:40:01,200
and capture the binding behavior

1741
01:40:01,200 --> 01:40:04,160
then we build the lambda
term using the constructor

1742
01:40:04,160 --> 01:40:08,040
and matching logic application.

1743
01:40:08,040 --> 01:40:10,720
The correctness of
the definition means that

1744
01:40:10,720 --> 01:40:13,640
the encoding from lambda
calculus to matching logic

1745
01:40:13,640 --> 01:40:17,320
is faithful with respect
to the syntax semantics

1746
01:40:17,320 --> 01:40:20,000
and the formal reasoning
of Lambda Calculus.

1747
01:40:20,000 --> 01:40:21,640
In terms of the syntax,

1748
01:40:21,640 --> 01:40:25,400
matching logic captures
lambda expressions verbatim.

1749
01:40:25,400 --> 01:40:31,800
The lambda abstraction
becomes this pattern

1750
01:40:31,800 --> 01:40:34,040
where we use lambda to build a term

1751
01:40:34,040 --> 01:40:38,840
and use [X]E which is
the graph set to build the binding.

1752
01:40:38,840 --> 01:40:42,360
In lambda calculus formal reasoning
means equational reasoning

1753
01:40:42,360 --> 01:40:44,280
plus the beta-reduction rule.

1754
01:40:44,280 --> 01:40:46,520
To capture that we
add beta reduction

1755
01:40:46,520 --> 01:40:49,680
as an axiom schema to
the theory Gamma lambda

1756
01:40:49,680 --> 01:40:52,200
then we can prove that Gamma lambda

1757
01:40:52,200 --> 01:40:55,120
indeed captures lambda
calculus faithfully

1758
01:40:55,120 --> 01:40:57,680
in the sense that lambda
calculus reasoning

1759
01:40:57,680 --> 01:41:00,880
is equivalent to the matching
logic formal reasoning

1760
01:41:00,880 --> 01:41:02,760
within the theory Gamma lambda

1761
01:41:02,760 --> 01:41:06,000
and is also equivalent to
the semantic validity

1762
01:41:06,000 --> 01:41:09,120
within the theory Gamma lambda.

1763
01:41:09,120 --> 01:41:11,120
To conclude in this paper

1764
01:41:11,120 --> 01:41:15,320
we gave a generic way to dealing
with binding using matching logic.

1765
01:41:15,320 --> 01:41:18,400
Since matching logic is
the logical foundation of K

1766
01:41:18,400 --> 01:41:22,120
our work is useful to
the development of K.

1767
01:41:22,120 --> 01:41:25,400
In the paper we proposed
a simple matching logic variant

1768
01:41:25,400 --> 01:41:27,920
that has only 7 syntactic constructs

1769
01:41:27,920 --> 01:41:31,680
and we fully developed
models and proof theories.

1770
01:41:31,680 --> 01:41:35,120
Then we defined the matching
logic theory for lambda calculus

1771
01:41:35,120 --> 01:41:40,200
where lambda abstraction is defined
by applying the construct lambda

1772
01:41:40,200 --> 01:41:42,200
to the graph set [X]E.

1773
01:41:42,200 --> 01:41:44,760
We proved the correctness
of this definition

1774
01:41:44,760 --> 01:41:47,400
by showing the equivalence
among the validity

1775
01:41:47,400 --> 01:41:52,000
and the provability of lambda
calculus and matching logic.

1776
01:41:52,000 --> 01:41:56,600
Finally, we extended our approach
to all binders in a systematic way

1777
01:41:56,600 --> 01:41:59,320
where the details can
be found in the paper.

1778
01:41:59,320 --> 01:42:00,720
Due to the time limit,

1779
01:42:00,720 --> 01:42:04,120
I cannot fully cover all
technical details in this talk.

1780
01:42:04,120 --> 01:42:08,400
The paper and the companion technical
report contain all the details.

1781
01:42:08,400 --> 01:42:13,040
I am happy to answer any questions
about the work in the Q&A session.

1782
01:42:13,040 --> 01:42:18,040
Thank you for watching.

1783
01:42:24,560 --> 01:42:26,440
RICHARD: Thanks very much Xiaohong,

1784
01:42:26,440 --> 01:42:29,720
who is now available in both
the New York and Asian time bands

1785
01:42:29,720 --> 01:42:34,720
for question and answer.

1786
01:43:01,960 --> 01:43:07,280
Next up we have Pierce Darragh
presenting Parsing with Zippers,

1787
01:43:07,280 --> 01:43:12,280
a Functional Pearl.

1788
01:43:14,920 --> 01:43:17,240
PIERCE DARRAGH: Hello, thank you
for watching my talk.

1789
01:43:17,240 --> 01:43:18,480
My name is Pierce Darragh

1790
01:43:18,480 --> 01:43:20,360
and I will be walking you
through parsing with zippers,

1791
01:43:20,360 --> 01:43:23,320
a functional pearl written
by Michael D. Adams and me.

1792
01:43:23,320 --> 01:43:25,200
So let's get to it.

1793
01:43:25,200 --> 01:43:27,400
Parsing with zippers is
an extension of the earlier work

1794
01:43:27,400 --> 01:43:31,000
of parsing with derivatives
which we'll call PwD for short.

1795
01:43:31,000 --> 01:43:33,720
We address an underlying
inefficiency in PwD's algorithm

1796
01:43:33,720 --> 01:43:36,280
by implementing a new
generalized form of the zipper

1797
01:43:36,280 --> 01:43:38,560
and using it to manage derivations.

1798
01:43:38,560 --> 01:43:40,480
Our resulting algorithm
is significantly faster

1799
01:43:40,480 --> 01:43:42,480
than the original
PwD implementation,

1800
01:43:42,480 --> 01:43:45,080
measurably faster than
the optimized PwD implementation

1801
01:43:45,080 --> 01:43:46,920
of Adams et al in 2015

1802
01:43:46,920 --> 01:43:51,720
and it is only around 100 lines of
OCaml cord when fully implemented.

1803
01:43:51,720 --> 01:43:54,120
The primary contribution of
parsing with derivatives

1804
01:43:54,120 --> 01:43:55,800
was the generalized
Brzozowski's derivatives

1805
01:43:55,800 --> 01:43:58,440
from regular expressions
to context free grammars.

1806
01:43:58,440 --> 01:44:00,080
This is not trivial.

1807
01:44:00,080 --> 01:44:02,720
The Brzozowski derivative
uses two eager algorithms

1808
01:44:02,720 --> 01:44:05,840
perfectly fine for regular
languages but not so with CFGs

1809
01:44:05,840 --> 01:44:07,520
which have recursion.

1810
01:44:07,520 --> 01:44:11,120
PwD introduced laziness to fix most
of for one of those algorithms

1811
01:44:11,120 --> 01:44:14,400
and memoization
and fixed points for the other.

1812
01:44:14,400 --> 01:44:16,440
The result is
an algorithm that is elegant

1813
01:44:16,440 --> 01:44:17,960
in its theoretical simplicity

1814
01:44:17,960 --> 01:44:19,720
and which can also
produce full parse trees

1815
01:44:19,720 --> 01:44:23,400
instead of only recognizing inputs
as Brzozowski derivatives do.

1816
01:44:23,400 --> 01:44:27,280
Let's see a brief example
of PwD in action.

1817
01:44:27,280 --> 01:44:30,040
We start with a grammar consisting
of a root concatenation node

1818
01:44:30,040 --> 01:44:32,840
shown with the circle-like
concatenation operator

1819
01:44:32,840 --> 01:44:34,760
that has two children
concatenation nodes

1820
01:44:34,760 --> 01:44:37,120
each of which has two
children token nodes.

1821
01:44:37,120 --> 01:44:40,560
This grammar parses only
the string a,b,c,d.

1822
01:44:40,560 --> 01:44:42,160
We will attempt to parse that string

1823
01:44:42,160 --> 01:44:46,000
shown disassembled in
the tokens at the top.

1824
01:44:46,000 --> 01:44:49,240
We take the first token from
the input string labeling it current

1825
01:44:49,240 --> 01:44:52,800
then we derive the grammar
beginning at the root node.

1826
01:44:52,800 --> 01:44:54,640
At each step a pattern match is run

1827
01:44:54,640 --> 01:44:56,920
to determine what to do
with the current node.

1828
01:44:56,920 --> 01:44:58,720
The derivative proceeds
down the left side

1829
01:44:58,720 --> 01:45:00,000
of each concatenation node

1830
01:45:00,000 --> 01:45:01,760
as it attempts to find
a token node.

1831
01:45:01,760 --> 01:45:04,840
Eventually it finds
the first token node, a.

1832
01:45:04,840 --> 01:45:06,280
When the algorithm
comes to a token node

1833
01:45:06,280 --> 01:45:08,680
it attempts to match that
token to the current token

1834
01:45:08,680 --> 01:45:11,200
that we are taking the derivative
of the grammar with respect to,

1835
01:45:11,200 --> 01:45:14,200
in this case also a.

1836
01:45:14,200 --> 01:45:15,520
Because the tokens match

1837
01:45:15,520 --> 01:45:17,480
we replace the token grammar
expression with a lambda

1838
01:45:17,480 --> 01:45:20,040
indicating a successful parse.

1839
01:45:20,040 --> 01:45:22,640
Since we have successfully derived
the grammar with respect to a

1840
01:45:22,640 --> 01:45:26,680
let's proceed and attempt to derive
the result with respect to b.

1841
01:45:26,680 --> 01:45:29,400
Again we being our search
at the root of the grammar

1842
01:45:29,400 --> 01:45:31,360
exploring each node and sequence

1843
01:45:31,360 --> 01:45:33,840
until we find
an unmatched token node.

1844
01:45:33,840 --> 01:45:36,600
In this case, the traversal down to
the lambda will be thrown away

1845
01:45:36,600 --> 01:45:39,400
so I am only showing the traversal
down towards the b node.

1846
01:45:39,400 --> 01:45:42,320
Because the current token
and the found token node matched

1847
01:45:42,320 --> 01:45:46,120
we replace the node with a lambda
node and return a new grammar.

1848
01:45:46,120 --> 01:45:48,360
We'll stop the parsing
example at this point.

1849
01:45:48,360 --> 01:45:50,080
The take away is that
each derivative

1850
01:45:50,080 --> 01:45:52,960
will start at the root of
the previous derivative result grammar.

1851
01:45:52,960 --> 01:45:56,040
In this simple example, this
only means one extra step

1852
01:45:56,040 --> 01:45:57,960
but with more complex
grammar with many layers

1853
01:45:57,960 --> 01:45:59,520
it can be very many steps.

1854
01:45:59,520 --> 01:46:01,000
This is wasted effort.

1855
01:46:01,000 --> 01:46:04,000
Most often subsequent derivatives
need to essentially resume

1856
01:46:04,000 --> 01:46:06,400
from where the previous
derivative left off.

1857
01:46:06,400 --> 01:46:08,160
The PwD algorithm does
not account for this

1858
01:46:08,160 --> 01:46:10,920
and so it spends a lot of time
on redundant traversals.

1859
01:46:10,920 --> 01:46:14,240
To avoid this we'll
employ the zipper.

1860
01:46:14,240 --> 01:46:16,760
The zipper is a functional data
structure for navigating trees

1861
01:46:16,760 --> 01:46:19,200
introduced by Huet in 1997.

1862
01:46:19,200 --> 01:46:21,440
We start with a simple definition
for a tree data type

1863
01:46:21,440 --> 01:46:23,720
consisting of either
an item which is leaf node

1864
01:46:23,720 --> 01:46:26,640
or a section which is a branch.

1865
01:46:26,640 --> 01:46:28,400
Next we introduce the path.

1866
01:46:28,400 --> 01:46:30,320
A path is essentially a context.

1867
01:46:30,320 --> 01:46:31,520
It represents the part of the tree

1868
01:46:31,520 --> 01:46:32,960
which has essentially been traversed

1869
01:46:32,960 --> 01:46:34,760
and also keeps track of
siblings of the subtree

1870
01:46:34,760 --> 01:46:37,320
currently being focused on.

1871
01:46:37,320 --> 01:46:40,760
Last we have the location which
is really the zipper itself.

1872
01:46:40,760 --> 01:46:43,360
This is a pair of a path
with the focused-on subtree

1873
01:46:43,360 --> 01:46:46,520
which is generally referred to
as the focus of the zipper.

1874
01:46:46,520 --> 01:46:50,480
To solidify this concept let
us look at the visualization.

1875
01:46:50,480 --> 01:46:53,240
In this diagram blue
squares represent paths,

1876
01:46:53,240 --> 01:46:55,080
yellow triangles
represent tree nodes

1877
01:46:55,080 --> 01:46:57,240
and the red circle is the zipper.

1878
01:46:57,240 --> 01:46:59,880
The red arrow going down from
the zipper is the focus expression

1879
01:46:59,880 --> 01:47:03,240
and the black arrow going
up points to the path.

1880
01:47:03,240 --> 01:47:04,600
I'll take a few steps with the tree.

1881
01:47:04,600 --> 01:47:08,720
We begin by doing down
through the subtree t1.

1882
01:47:08,720 --> 01:47:10,960
When we go down into
a concatenation node

1883
01:47:10,960 --> 01:47:12,600
a new node path will be created

1884
01:47:12,600 --> 01:47:15,720
representing the path back up
through that concatenation node.

1885
01:47:15,720 --> 01:47:17,120
This will be attached to the zipper

1886
01:47:17,120 --> 01:47:19,800
which will be focused on the first
child of the concatenation node

1887
01:47:19,800 --> 01:47:21,880
t2 in this case.

1888
01:47:21,880 --> 01:47:24,040
The remaining children t3 and t4

1889
01:47:24,040 --> 01:47:27,200
are in the node paths
right siblings list.

1890
01:47:27,200 --> 01:47:30,400
We are not limited traversing
the tree a specific order.

1891
01:47:30,400 --> 01:47:32,040
We can immediately move to right

1892
01:47:32,040 --> 01:47:34,000
without going at any
other direction.

1893
01:47:34,000 --> 01:47:36,120
Notice that a new node
path is constructed

1894
01:47:36,120 --> 01:47:38,160
which has t2 as the left
sibling of the focus

1895
01:47:38,160 --> 01:47:40,920
and t4 as the right sibling.

1896
01:47:40,920 --> 01:47:44,440
If we go down through t3 we'll
create a new node path c3

1897
01:47:44,440 --> 01:47:46,680
which itself points up to c2.

1898
01:47:46,680 --> 01:47:48,400
Because t3 had no siblings

1899
01:47:48,400 --> 01:47:52,600
the left and right sibling
lists in c3 are left empty.

1900
01:47:52,600 --> 01:47:56,920
Lastly, we can return up
through a path in this case c3.

1901
01:47:56,920 --> 01:47:59,520
When we do this we could create
a new node to replace t3

1902
01:47:59,520 --> 01:48:02,400
which is why I've underlined t3
in the diagram at this point.

1903
01:48:02,400 --> 01:48:04,480
In any case, a new
zipper is constructed

1904
01:48:04,480 --> 01:48:06,600
focusing on the tree node
we just traveled through

1905
01:48:06,600 --> 01:48:10,240
with the path back at
the root as always.

1906
01:48:10,240 --> 01:48:14,440
This brings us to parsing with
zippers or PwZ for short.

1907
01:48:14,440 --> 01:48:16,160
We use zippers to save the location

1908
01:48:16,160 --> 01:48:18,320
where the derivative
was successfully taken.

1909
01:48:18,320 --> 01:48:21,080
By doing this we eliminate
almost all of the traversals

1910
01:48:21,080 --> 01:48:24,200
in the original parsing
with derivatives algorithm.

1911
01:48:24,200 --> 01:48:27,680
We use a slightly modified zipper
definition compared to the original.

1912
01:48:27,680 --> 01:48:29,880
For one, we actually call
it a zipper in the code

1913
01:48:29,880 --> 01:48:31,240
instead of location.

1914
01:48:31,240 --> 01:48:34,720
We also rename tree to exp,
meaning grammar expansions

1915
01:48:34,720 --> 01:48:37,680
and path to cxt meaning contexts.

1916
01:48:37,680 --> 01:48:39,920
All context constructors
end with a capital C

1917
01:48:39,920 --> 01:48:43,080
to indicate their status.

1918
01:48:43,080 --> 01:48:46,400
Now there is a problem with our
zipper implementation which is this.

1919
01:48:46,400 --> 01:48:49,000
It can't yet handle all
context-free grammars.

1920
01:48:49,000 --> 01:48:52,200
It can work fine for concatenation
tokens but that's it.

1921
01:48:52,200 --> 01:48:54,200
Many CFGs have alternates and cycles

1922
01:48:54,200 --> 01:48:56,840
which are part of what
makes CFG so useful.

1923
01:48:56,840 --> 01:48:57,920
We'll have to improve our zipper

1924
01:48:57,920 --> 01:49:01,160
if we want to use it
for parsing CFGs.

1925
01:49:01,160 --> 01:49:03,400
First, we look at supporting
alternates in the grammar.

1926
01:49:03,400 --> 01:49:06,480
This requires adding an alternate
constructor to the exp type

1927
01:49:06,480 --> 01:49:10,960
and a corresponding alternate
context constructor to the cxt type.

1928
01:49:10,960 --> 01:49:13,000
Here we have a simple grammar.

1929
01:49:13,000 --> 01:49:15,400
The inverted triangle labeled
a1 is an alternate node

1930
01:49:15,400 --> 01:49:17,760
and t1 and t2 are its children.

1931
01:49:17,760 --> 01:49:20,720
They should be processed
independently of one another.

1932
01:49:20,720 --> 01:49:23,880
When the zipper moves through
the alternate node a1 it splits.

1933
01:49:23,880 --> 01:49:26,200
We get two context
nodes and two zippers

1934
01:49:26,200 --> 01:49:27,880
which is exactly what we wanted.

1935
01:49:27,880 --> 01:49:29,040
Now the children of
an alternate node

1936
01:49:29,040 --> 01:49:32,160
are handled in
a non-deterministic manner.

1937
01:49:32,160 --> 01:49:33,680
When we eventually traverse back up,

1938
01:49:33,680 --> 01:49:36,840
possibly modifying either or both
of the child nodes in the process

1939
01:49:36,840 --> 01:49:39,560
we create a new alternate
node for each child.

1940
01:49:39,560 --> 01:49:42,360
These are handled independently
by the algorithm.

1941
01:49:42,360 --> 01:49:45,280
For non zipper grammars
the solution is technically sufficient

1942
01:49:45,280 --> 01:49:48,200
however, there are
some inefficiencies.

1943
01:49:48,200 --> 01:49:51,600
Here we have a situation where
three expressions have been parsed

1944
01:49:51,600 --> 01:49:54,200
which all descend to
a share expression t1.

1945
01:49:54,200 --> 01:49:55,640
There are three separate zippers

1946
01:49:55,640 --> 01:49:58,040
sitting between each of
the associated context

1947
01:49:58,040 --> 01:50:00,840
and this t1 expression.

1948
01:50:00,840 --> 01:50:03,720
Parsing with the first zipper
creates a new context c4

1949
01:50:03,720 --> 01:50:07,040
and a new zipper z4 which
sits between the new context

1950
01:50:07,040 --> 01:50:09,960
and the child expression t2.

1951
01:50:09,960 --> 01:50:13,080
Parsing with the second zipper
creates an identical context and zipper

1952
01:50:13,080 --> 01:50:14,840
but they are separate
from the first ones

1953
01:50:14,840 --> 01:50:17,560
and the third zipper
will do the same.

1954
01:50:17,560 --> 01:50:20,880
We now have three identical
contexts all labeled c4

1955
01:50:20,880 --> 01:50:23,640
and three identical
zippers all labeled z4.

1956
01:50:23,640 --> 01:50:25,280
This is wasted effort.

1957
01:50:25,280 --> 01:50:28,080
Not only that but if we were to add
support for cycles at this point

1958
01:50:28,080 --> 01:50:29,720
the inefficiency could compound

1959
01:50:29,720 --> 01:50:31,760
and lead to the number of
zippers becoming exponential

1960
01:50:31,760 --> 01:50:33,240
with respect to
the length of the input.

1961
01:50:33,240 --> 01:50:35,600
We can't have that.

1962
01:50:35,600 --> 01:50:37,240
Here is the code as
we currently have it

1963
01:50:37,240 --> 01:50:40,320
but now we'll add memoization.

1964
01:50:40,320 --> 01:50:43,680
The memoization entries simply take
the place of the child contexts

1965
01:50:43,680 --> 01:50:47,800
within each context type as
well as within the zipper.

1966
01:50:47,800 --> 01:50:50,680
We now have a memoization
table shown on the left.

1967
01:50:50,680 --> 01:50:53,440
Although the zipper is now actually
pointing upwards to memoization entries

1968
01:50:53,440 --> 01:50:55,520
I've shown them here according
to the associated context

1969
01:50:55,520 --> 01:50:57,600
for visual simplicity.

1970
01:50:57,600 --> 01:51:00,520
When zipper z1 is attempts
to parse into expression t1

1971
01:51:00,520 --> 01:51:02,240
we'll add an entry to
the memoization table

1972
01:51:02,240 --> 01:51:05,000
so subsequent derivations
do not repeat work.

1973
01:51:05,000 --> 01:51:07,440
It is important that the table
entry is added before descending

1974
01:51:07,440 --> 01:51:11,240
or else you may still end up
with redundant traversals.

1975
01:51:11,240 --> 01:51:14,320
When the first zipper z1 descends
through the expression t1

1976
01:51:14,320 --> 01:51:16,560
a new entry is added to
the memoization table indexed

1977
01:51:16,560 --> 01:51:19,360
by a pair consisting of
the current input token position

1978
01:51:19,360 --> 01:51:22,440
which we will call p here
and the expression.

1979
01:51:22,440 --> 01:51:26,120
The Zipper's context c1 is
added to the parent's field.

1980
01:51:26,120 --> 01:51:29,360
When the derivative is complete
the result field will be populated

1981
01:51:29,360 --> 01:51:33,600
and we will continue parsing
upwards through the parents.

1982
01:51:33,600 --> 01:51:37,160
Now, when zipper z2 attempts to
parse through expression t1

1983
01:51:37,160 --> 01:51:39,240
it first checks
the memoization table.

1984
01:51:39,240 --> 01:51:41,640
If the current input
token position is still p

1985
01:51:41,640 --> 01:51:43,720
then the look up
the table will succeed.

1986
01:51:43,720 --> 01:51:45,400
Instead of doing any
additional processing

1987
01:51:45,400 --> 01:51:49,840
z2 will simply add its current
context c2 to the parent's field.

1988
01:51:49,840 --> 01:51:52,560
The last zipper z3 will do the same.

1989
01:51:52,560 --> 01:51:53,880
When the child zipper z4

1990
01:51:53,880 --> 01:51:56,480
eventually returns from parsing
through the t2 expression

1991
01:51:56,480 --> 01:51:58,440
and ascends up through
the context c4

1992
01:51:58,440 --> 01:52:00,480
it will split again to
return the computed result

1993
01:52:00,480 --> 01:52:04,360
to each of the parent
contexts c1, c2 and c3.

1994
01:52:04,360 --> 01:52:05,720
By using this memoization table

1995
01:52:05,720 --> 01:52:07,320
we have avoided creating
redundant zippers

1996
01:52:07,320 --> 01:52:08,960
when traversing down the grammar.

1997
01:52:08,960 --> 01:52:12,680
What happens when we
decide to traverse up?

1998
01:52:12,680 --> 01:52:14,920
Here is an example where a zipper
is focused on an alternate

1999
01:52:14,920 --> 01:52:16,480
which has not been parsed yet.

2000
01:52:16,480 --> 01:52:18,960
So it has no entry in
the memoization table.

2001
01:52:18,960 --> 01:52:20,520
The way the algorithm
worked at this point

2002
01:52:20,520 --> 01:52:22,320
causes two new zippers to be spanned

2003
01:52:22,320 --> 01:52:25,160
each pointing to a new
alt c context node.

2004
01:52:25,160 --> 01:52:27,720
Note that both contexts, c2 and c3,

2005
01:52:27,720 --> 01:52:29,600
point to the same memoization entry

2006
01:52:29,600 --> 01:52:32,800
for the a1 alternate expression
that we just passed through.

2007
01:52:32,800 --> 01:52:35,280
Let's say zipper z2
does its work first

2008
01:52:35,280 --> 01:52:38,000
and returns up through context c2.

2009
01:52:38,000 --> 01:52:40,440
A new alt node is constructed, a2.

2010
01:52:40,440 --> 01:52:42,840
This is added to the result
field of the old alternate node,

2011
01:52:42,840 --> 01:52:46,400
a1's memoization table entry.

2012
01:52:46,400 --> 01:52:49,760
When the second child zipper z3
passes up through context c3

2013
01:52:49,760 --> 01:52:52,160
it also creates a new alt node, a3.

2014
01:52:52,160 --> 01:52:55,280
This node is also added
to a1's result field.

2015
01:52:55,280 --> 01:52:56,960
Two alternate nodes
have been created

2016
01:52:56,960 --> 01:52:59,160
and each has spawned a zipper
that will navigate up

2017
01:52:59,160 --> 01:53:02,080
through parent context
c1, this is inefficient

2018
01:53:02,080 --> 01:53:04,880
and similar to the problem
with unshared expressions

2019
01:53:04,880 --> 01:53:06,880
the situation can lead to
an exponential blow-up

2020
01:53:06,880 --> 01:53:08,520
in the number of zippers spawned

2021
01:53:08,520 --> 01:53:11,320
when deriving over a cyclic grammar.

2022
01:53:11,320 --> 01:53:13,680
To address this we will
adjust our memoization code

2023
01:53:13,680 --> 01:53:17,600
to also memoize contexts in
addition to expressions.

2024
01:53:17,600 --> 01:53:19,920
Because the situation only
arises for alternate nodes

2025
01:53:19,920 --> 01:53:21,560
we simply extend the alt constructor

2026
01:53:21,560 --> 01:53:23,880
to have a mutable list
of child expressions.

2027
01:53:23,880 --> 01:53:26,680
This will allow us to incrementally
update a constructed alt nodes

2028
01:53:26,680 --> 01:53:30,320
list of children as new zippers
return up through the tree.

2029
01:53:30,320 --> 01:53:32,080
We also now only need
a single expression

2030
01:53:32,080 --> 01:53:34,920
to be saved in the result field
of the memoization entry.

2031
01:53:34,920 --> 01:53:37,720
Let's see how it works.

2032
01:53:37,720 --> 01:53:39,080
We start back at this point

2033
01:53:39,080 --> 01:53:41,920
where a zipper has just passed up
through an alt c alternate context

2034
01:53:41,920 --> 01:53:44,520
and created a new alternate node a2.

2035
01:53:44,520 --> 01:53:48,720
The node is added as the lone
result in the memoization table.

2036
01:53:48,720 --> 01:53:50,560
When the second zipper
moves up in the tree

2037
01:53:50,560 --> 01:53:53,120
it simply modifies
the existing a2 node.

2038
01:53:53,120 --> 01:53:54,840
Now, there is only
one zipper created

2039
01:53:54,840 --> 01:53:56,880
above the newly
constructed alternate node

2040
01:53:56,880 --> 01:53:59,520
and we have avoided
the potential exponential behavior

2041
01:53:59,520 --> 01:54:01,880
we would have had earlier.

2042
01:54:01,880 --> 01:54:04,120
While adding memoization to
reduce inefficiencies

2043
01:54:04,120 --> 01:54:06,280
I mentioned some concerns
about a hypothetical future

2044
01:54:06,280 --> 01:54:07,840
when we support cyclic grammars.

2045
01:54:07,840 --> 01:54:10,440
Have no fear, the time
has come to handle cycles

2046
01:54:10,440 --> 01:54:14,080
and behold, they are
handled already.

2047
01:54:14,080 --> 01:54:15,760
Where recursive descent
parsers face a dilemma

2048
01:54:15,760 --> 01:54:18,960
in choosing how times to
descend into a left recursion

2049
01:54:18,960 --> 01:54:21,120
we side step the issue entirely.

2050
01:54:21,120 --> 01:54:24,360
Our memoization strategy automatically
converts cycles in the grammar

2051
01:54:24,360 --> 01:54:26,400
into cycles in the contexts.

2052
01:54:26,400 --> 01:54:28,480
This allows us to postpone
making any choices

2053
01:54:28,480 --> 01:54:29,800
until further parsing reveals

2054
01:54:29,800 --> 01:54:32,280
how many repetitions
are actually needed.

2055
01:54:32,280 --> 01:54:33,920
The diagram for this
is a bit complex

2056
01:54:33,920 --> 01:54:35,760
so I suggest reading
section 7 of our paper

2057
01:54:35,760 --> 01:54:38,600
for more detailed explanation.

2058
01:54:38,600 --> 01:54:41,880
With that, the fundamental theory
of our algorithm is laid out

2059
01:54:41,880 --> 01:54:43,560
but there remains one more
advancement to discuss

2060
01:54:43,560 --> 01:54:46,240
prior to looking at the evaluation.

2061
01:54:46,240 --> 01:54:48,320
We can completely eliminate
the memoization tables

2062
01:54:48,320 --> 01:54:50,200
and I'll show why.

2063
01:54:50,200 --> 01:54:52,560
All our memoization are indexed
by the current input position

2064
01:54:52,560 --> 01:54:54,400
but we parse
the input consequentially

2065
01:54:54,400 --> 01:54:57,400
so there is never a need
to maintain old entries.

2066
01:54:57,400 --> 01:54:59,040
We eliminate
the position as an index

2067
01:54:59,040 --> 01:55:01,040
and instead make two
fields in the table entry

2068
01:55:01,040 --> 01:55:02,720
to denote the start
and end positions

2069
01:55:02,720 --> 01:55:06,000
corresponding to a successful parse.

2070
01:55:06,680 --> 01:55:08,440
However, since this table is now
indexed only by an expression

2071
01:55:08,440 --> 01:55:10,400
we can actually
eliminate it entirely

2072
01:55:10,400 --> 01:55:13,880
and simply put a memoization entry
inside each expression automatically

2073
01:55:14,880 --> 01:55:16,920
Here is the code as it existed
with the memoization table.

2074
01:55:18,160 --> 01:55:20,840
And here is the code which
eliminates memoization tables.

2075
01:55:21,360 --> 01:55:23,000
We now have a new definition of EXP

2076
01:55:23,000 --> 01:55:24,680
which contains a memoization record

2077
01:55:25,160 --> 01:55:27,960
and an EXP prime which was
previously called EXP.

2078
01:55:28,960 --> 01:55:32,240
The memoization entries now record
start and end positions associated

2079
01:55:32,760 --> 01:55:33,760
with successful parses

2080
01:55:34,240 --> 01:55:35,680
allowing the algorithm
to quickly discern

2081
01:55:35,680 --> 01:55:37,000
whether the entry has grown stale.

2082
01:55:37,720 --> 01:55:39,280
And with that, the work is done.

2083
01:55:40,760 --> 01:55:42,280
Let's briefly look at our
performance evaluation.

2084
01:55:43,520 --> 01:55:47,320
We ran parsing with Zippers over
the Python 3.4.3 standard library

2085
01:55:47,320 --> 01:55:49,160
and compared the results
to a few other parsers.

2086
01:55:49,880 --> 01:55:52,240
Parse derivatives
from Mike et al, 2011.

2087
01:55:53,000 --> 01:55:56,000
Optimized parse derivatives
from Adams et al, 2016.

2088
01:55:56,480 --> 01:55:58,920
Menhir an LRK parsing
generator from Ocaml

2089
01:55:59,680 --> 01:56:02,200
and Deep gen a GLR partial
generator for Ocaml.

2090
01:56:03,680 --> 01:56:05,200
The parsing with
derivatives implementation

2091
01:56:05,200 --> 01:56:06,800
had to be ported from
Racket to OCaml

2092
01:56:06,800 --> 01:56:09,760
and Menhir's LRK limitations
require that we reformulate

2093
01:56:09,760 --> 01:56:12,240
some of the Python grammar to
avoid potential ambiguities

2094
01:56:12,240 --> 01:56:13,640
that might distort our comparisons.

2095
01:56:14,400 --> 01:56:16,280
We run the benchmarks on
the digital ocean droplet

2096
01:56:16,280 --> 01:56:17,400
with a dedicated CPU

2097
01:56:17,400 --> 01:56:19,800
to ensure we did not introduce
any sources of invalidity

2098
01:56:19,800 --> 01:56:20,800
due to other tasks.

2099
01:56:22,560 --> 01:56:25,440
The results show that parsing with
zippers significantly outperform

2100
01:56:25,440 --> 01:56:27,240
the original parsing with
derivatives algorithm

2101
01:56:27,240 --> 01:56:29,640
and even outperforms
the optimized version of PwD

2102
01:56:29,640 --> 01:56:31,720
that relies on many low level
tunings and adjustments

2103
01:56:31,720 --> 01:56:32,720
to the app algorithm.

2104
01:56:33,480 --> 01:56:36,320
Although PWZ does not surpass
the pressure generators in our test

2105
01:56:36,320 --> 01:56:37,800
we believe the result
to be significant

2106
01:56:37,800 --> 01:56:39,520
because of the simplicity
of the algorithm.

2107
01:56:40,000 --> 01:56:41,480
At only around
a hundred lines of code,

2108
01:56:41,480 --> 01:56:43,680
parsing with zippers is simple
enough to be written by hand

2109
01:56:43,680 --> 01:56:45,960
and yet it is fairly competitive
as long as performance

2110
01:56:45,960 --> 01:56:47,160
is not absolutely crucial.

2111
01:56:47,680 --> 01:56:49,440
Additionally we claim
parsing with zippers

2112
01:56:49,440 --> 01:56:50,720
can handle all context free grammars

2113
01:56:50,720 --> 01:56:52,320
even with recursions
and ambiguities

2114
01:56:52,320 --> 01:56:53,600
which Menhir cannot do.

2115
01:56:54,560 --> 01:56:57,440
So we have introduced
parsing with zippers,

2116
01:56:57,440 --> 01:56:58,840
a new general parsing algorithm

2117
01:56:58,840 --> 01:56:59,840
that is simple to implement,

2118
01:56:59,840 --> 01:57:01,320
it provides surprisingly
good performance.

2119
01:57:02,560 --> 01:57:04,920
Parsing with derivatives generalize
the Bruzewski derivative

2120
01:57:04,920 --> 01:57:07,040
from regular expressions to
context free grammars.

2121
01:57:07,560 --> 01:57:09,840
We have generalized
the zipper from tree structures

2122
01:57:09,840 --> 01:57:11,040
to context free grammars.

2123
01:57:11,760 --> 01:57:13,880
We also consider
asymptotic complexity,

2124
01:57:13,880 --> 01:57:15,280
other divergences from PwD

2125
01:57:15,760 --> 01:57:17,080
and a few more points
in the discussion

2126
01:57:17,080 --> 01:57:18,720
and evaluation sections of the paper

2127
01:57:18,720 --> 01:57:19,720
which I would encourage you to read

2128
01:57:19,720 --> 01:57:21,360
if you have further questions
that I did not address.

2129
01:57:22,600 --> 01:57:24,840
I hope you have enjoyed this
presentation of parsing with Zippers,

2130
01:57:24,840 --> 01:57:27,360
a Function Pearl, by
Pierce Darragh, that's me

2131
01:57:27,360 --> 01:57:28,480
and Michael D. Adams.

2132
01:57:29,240 --> 01:57:30,360
Please feel free to contact us

2133
01:57:30,360 --> 01:57:32,040
if you have any additional
questions or comments.

2134
01:57:32,760 --> 01:57:33,760
Thank you.

2135
01:57:39,040 --> 01:57:44,040
(APPLAUSE)

2136
01:57:47,400 --> 01:57:48,400
RICHARD: Thanks very much Pierce.

2137
01:57:49,160 --> 01:57:50,520
In the New York time band,

2138
01:57:50,520 --> 01:57:53,480
the authors are now available
for question and answer.

2139
01:57:54,720 --> 01:57:56,160
I'm afraid not in
the Asia time band.

2140
01:58:02,400 --> 01:58:05,520
OK, last up in this session
we have Timothee Haudebourg

2141
01:58:05,520 --> 01:58:08,560
presenting Regular Language Type
Inference with Term Rewriting.

2142
01:58:10,560 --> 01:58:11,640
TIMOTHEE HAUDEBOURG: Hello everyone.

2143
01:58:11,640 --> 01:58:13,320
My name is Timothee Haudebourg.

2144
01:58:13,320 --> 01:58:16,560
And in this presentation we will
be talking about our latest work

2145
01:58:16,560 --> 01:58:20,000
on the automatic verification of
higher-order functional programs

2146
01:58:20,720 --> 01:58:23,960
where we designed a Regular
language Type Inference system

2147
01:58:23,960 --> 01:58:25,360
using term rewriting.

2148
01:58:26,600 --> 01:58:29,480
This is joint work with Thomas
Genet and Thomas Jensen

2149
01:58:29,480 --> 01:58:32,200
at the French institute for
research and computer science.

2150
01:58:33,200 --> 01:58:34,680
And the things that
we have developed

2151
01:58:34,680 --> 01:58:37,200
is dedicated to the fully
automatic verification

2152
01:58:37,200 --> 01:58:41,120
of higher-order functional program
processing algebraic data

2153
01:58:42,120 --> 01:58:44,200
types. What see on this
slide is an example

2154
01:58:44,200 --> 01:58:45,480
of what we are going to achieve.

2155
01:58:45,960 --> 01:58:48,040
On the left side is a user input.

2156
01:58:48,560 --> 01:58:50,760
It is a higher-order
functional program

2157
01:58:50,760 --> 01:58:54,400
written for the purpose of
the presentation in OCaml.

2158
01:58:54,400 --> 01:58:56,640
It defines a recursive function sort

2159
01:58:56,640 --> 01:58:59,280
that implements
an insertion sort algorithms

2160
01:58:59,280 --> 01:59:01,400
taking as parameters
a comparison function.

2161
01:59:02,400 --> 01:59:06,880
And also a sorted predicate that verifies
that the list is indeed sorted

2162
01:59:06,880 --> 01:59:09,120
according to the same
input comparison function.

2163
01:59:09,640 --> 01:59:13,880
The right side shows that we imagine
could be the user interface.

2164
01:59:13,880 --> 01:59:17,200
Here the user can type in the
property he wants to verify.

2165
01:59:17,680 --> 01:59:21,520
So here for example, we want
to verify that for all list l

2166
01:59:21,520 --> 01:59:24,520
the output of the sort
function is indeed sorted

2167
01:59:24,520 --> 01:59:26,800
according to the corresponding predicate.

2168
01:59:26,800 --> 01:59:31,120
Note that the property uses predicates
that are defined in the program

2169
01:59:31,120 --> 01:59:32,800
and without any more information

2170
01:59:32,800 --> 01:59:36,480
without annotation, we are able to
check if the property is verified

2171
01:59:37,000 --> 01:59:38,600
or find a counterexample.

2172
01:59:39,600 --> 01:59:42,600
If the property cannot be verified
with a regular abstraction

2173
01:59:42,600 --> 01:59:44,320
of the program, it will diverge.

2174
01:59:45,600 --> 01:59:47,960
To understand why our
contribution was necessary

2175
01:59:48,440 --> 01:59:51,000
in how it relates to
other higher-order functional

2176
01:59:51,000 --> 01:59:52,920
program verification techniques

2177
01:59:52,920 --> 01:59:55,480
I'd like to place every
technique on this diagram.

2178
01:59:56,000 --> 01:59:59,520
Many properties can be verified
by simplifying or abstracting

2179
01:59:59,520 --> 02:00:02,320
the behavior of the input
program in a certain way

2180
02:00:02,800 --> 02:00:05,000
so we can classify
verification techniques

2181
02:00:05,000 --> 02:00:08,200
according to the abstraction
they can generate.

2182
02:00:08,720 --> 02:00:11,880
On the bottom we put techniques
that can verify properties

2183
02:00:11,880 --> 02:00:13,720
with any regular abstraction.

2184
02:00:14,240 --> 02:00:16,960
On the top we put techniques
that can verify property

2185
02:00:16,960 --> 02:00:18,200
with any abstraction.

2186
02:00:18,960 --> 02:00:22,960
Overall, this axis measures a range
of properties that can be verified

2187
02:00:22,960 --> 02:00:23,960
by the technique.

2188
02:00:24,440 --> 02:00:28,040
On left to right, we measure
how much a user is active

2189
02:00:28,040 --> 02:00:29,280
during the verification.

2190
02:00:30,760 --> 02:00:33,560
On the left we have fully
automatic techniques

2191
02:00:33,560 --> 02:00:35,520
and on the right manual techniques.

2192
02:00:37,280 --> 02:00:40,440
Proof assistants such
as Coq, Isabelle HOL

2193
02:00:40,440 --> 02:00:41,920
can be placed in this region.

2194
02:00:42,440 --> 02:00:44,560
They can verify a large
range of properties

2195
02:00:45,560 --> 02:00:47,160
but require a lot of manual work.

2196
02:00:47,920 --> 02:00:49,760
Refinement type inference techniques

2197
02:00:49,760 --> 02:00:52,200
which can be seen in
the Liquid types or F star

2198
02:00:52,200 --> 02:00:53,720
require less manual work.

2199
02:00:53,720 --> 02:00:55,880
So the user only needs
to give some hints

2200
02:00:55,880 --> 02:00:58,480
usually as program type annotations

2201
02:00:58,480 --> 02:01:02,240
however the range of verifiable
properties is reduced.

2202
02:01:03,240 --> 02:01:05,360
Fully automated techniques
have been developed

2203
02:01:05,360 --> 02:01:09,040
around higher-order model checking,
predicate abstraction and more.

2204
02:01:10,040 --> 02:01:12,760
These techniques do not
require any annotations

2205
02:01:12,760 --> 02:01:15,400
but mainly focus on
relational properties

2206
02:01:15,400 --> 02:01:16,840
over numerical data types.

2207
02:01:17,600 --> 02:01:20,680
And are generally not good at
verifying regular properties

2208
02:01:20,680 --> 02:01:22,760
over algebraic data types.

2209
02:01:23,520 --> 02:01:26,680
There have been some attempts to
fully automatic verification

2210
02:01:26,680 --> 02:01:30,120
of regular properties including
our previous work in Timbuk

2211
02:01:30,640 --> 02:01:33,440
although these techniques are
incomplete and not scalable.

2212
02:01:34,200 --> 02:01:36,600
In our paper, we
propose a new technique

2213
02:01:36,600 --> 02:01:38,520
that is complete on
regular properties

2214
02:01:38,520 --> 02:01:40,840
and modular to scale better.

2215
02:01:41,600 --> 02:01:44,000
In our work we use term
rewriting systems

2216
02:01:44,520 --> 02:01:46,480
to model programs
and their semantics

2217
02:01:46,480 --> 02:01:49,040
and we use regular language
of terms as types.

2218
02:01:49,800 --> 02:01:52,960
Which we are able to
automatically infer using regular

2219
02:01:52,960 --> 02:01:54,400
invariant learning techniques.

2220
02:01:55,400 --> 02:01:58,000
Before I detail this type
inference technique

2221
02:01:58,000 --> 02:02:02,400
let's start by an introduction
of our theoretical frameworks.

2222
02:02:03,160 --> 02:02:05,600
Terms, language and rewriting systems.

2223
02:02:06,360 --> 02:02:10,040
The term is a labeled tree that we
usually represent like this.

2224
02:02:10,520 --> 02:02:14,440
It is composed of a symbol f
living in a ranked alphabet

2225
02:02:14,440 --> 02:02:17,080
and n subterms t1 to tn.

2226
02:02:17,560 --> 02:02:20,080
Where n is the arity of the symbol f.

2227
02:02:21,080 --> 02:02:24,720
We call a pattern a term that
has a variable in it.

2228
02:02:25,200 --> 02:02:28,760
Terms are useful to represent
functional language expressions.

2229
02:02:28,760 --> 02:02:32,880
This slide shows how OCaml expressions
can be translated into terms.

2230
02:02:33,640 --> 02:02:35,040
Constants do not change,

2231
02:02:35,040 --> 02:02:36,320
they have no subterms

2232
02:02:37,040 --> 02:02:40,400
unless it is a number in which
case we'll represent it by a tree

2233
02:02:40,400 --> 02:02:42,560
using Peano's number
representation.

2234
02:02:43,320 --> 02:02:45,400
Functions and applications
can be translated

2235
02:02:45,400 --> 02:02:50,040
into a term where the arity of
the symbol is the arity of the function

2236
02:02:50,040 --> 02:02:51,600
and the subterms are parameters.

2237
02:02:53,120 --> 02:02:56,480
However this representation does
not allow partial application of f.

2238
02:02:56,480 --> 02:02:59,600
So instead we use the special
@ symbol to represent

2239
02:02:59,600 --> 02:03:02,480
a function application
which takes a function

2240
02:03:02,480 --> 02:03:04,400
and its first parameter.

2241
02:03:04,920 --> 02:03:07,480
Control structures such as if-then-else

2242
02:03:07,480 --> 02:03:09,320
are simply represented as terms.

2243
02:03:10,080 --> 02:03:13,040
Finally let bindings
and anonymous functions

2244
02:03:13,040 --> 02:03:16,880
are erased living only
the bound body here k.

2245
02:03:17,640 --> 02:03:19,200
This is because the program's logic

2246
02:03:19,200 --> 02:03:22,400
is preserved elsewhere as
a term writing system.

2247
02:03:22,880 --> 02:03:26,520
A term rewriting system defines
a set of rewriting rules

2248
02:03:26,520 --> 02:03:29,120
between patterns that encode
the program's logic.

2249
02:03:29,640 --> 02:03:32,280
Here is an example
of the OCaml program

2250
02:03:32,280 --> 02:03:34,600
defining the sorted
predicate function on top.

2251
02:03:35,840 --> 02:03:38,920
Translated it into a rewriting
system on the bottom.

2252
02:03:38,920 --> 02:03:40,920
Because there are a lot of
function applications

2253
02:03:40,920 --> 02:03:42,520
you see a lot of @ symbols

2254
02:03:43,240 --> 02:03:44,400
which takes a lot of room

2255
02:03:44,400 --> 02:03:45,840
and is hard to read.

2256
02:03:46,600 --> 02:03:49,600
To simplify, we will write
space between the function

2257
02:03:49,600 --> 02:03:52,480
and it's parameters instead
of @ just like this.

2258
02:03:53,480 --> 02:03:56,560
Underlined elements are
the variables of the patterns.

2259
02:03:57,560 --> 02:04:00,080
We see that each pattern
matching rule on the top

2260
02:04:00,080 --> 02:04:03,720
is translated into the one or more
rewriting rules on the bottom.

2261
02:04:04,240 --> 02:04:06,680
Rewriting system can also
be used to describe

2262
02:04:06,680 --> 02:04:09,600
a regular language of
terms as tree automata.

2263
02:04:10,320 --> 02:04:12,640
Here is for instance on the right.

2264
02:04:12,640 --> 02:04:15,800
The tree automaton
recognizing every natural number

2265
02:04:15,800 --> 02:04:19,960
which can be used to partially
represent the int type in OCaml.

2266
02:04:20,440 --> 02:04:25,640
So rewriting rule defines
that 0 rewrites into int

2267
02:04:25,640 --> 02:04:27,080
because it is int

2268
02:04:27,080 --> 02:04:30,800
and s of int as you saw is also int.

2269
02:04:30,800 --> 02:04:34,880
In fact true automata can model
any algebraic data type of OCaml.

2270
02:04:34,880 --> 02:04:38,680
Here is the tree automata for
the following type of integer list.

2271
02:04:38,680 --> 02:04:41,680
It defines two more
rules for nil and cons

2272
02:04:41,680 --> 02:04:44,560
that closely match
the OCaml type definition.

2273
02:04:44,560 --> 02:04:47,120
Tree automata can even
represent regular language

2274
02:04:47,120 --> 02:04:49,600
that are not possible
to define in OCaml.

2275
02:04:49,600 --> 02:04:53,560
For instance, here is tree automata
recognizing even numbers.

2276
02:04:54,080 --> 02:04:56,600
This can be seen as
the subtype of int.

2277
02:04:56,600 --> 02:05:00,320
In our case we call it
a regular language type.

2278
02:05:00,320 --> 02:05:03,720
Now that we are familiarized
with rewriting systems

2279
02:05:03,720 --> 02:05:05,560
and regular language types

2280
02:05:05,560 --> 02:05:08,040
let's dive into our
verification technique.

2281
02:05:09,040 --> 02:05:11,760
Remember that in our initial example

2282
02:05:11,760 --> 02:05:16,200
the user wants to verify that
in his program for all list l

2283
02:05:16,200 --> 02:05:17,960
the output of sort l

2284
02:05:17,960 --> 02:05:20,640
is sorted according to
the corresponding predicate.

2285
02:05:21,640 --> 02:05:25,160
In our verification settings,
this problem is first translated

2286
02:05:25,160 --> 02:05:26,560
into a rewriting problem.

2287
02:05:27,280 --> 02:05:29,920
Here it states that for all term l

2288
02:05:29,920 --> 02:05:31,720
in the regular language of lists

2289
02:05:32,200 --> 02:05:33,720
and for all term v

2290
02:05:34,440 --> 02:05:36,600
if the input term rewrites into v

2291
02:05:36,600 --> 02:05:38,480
then v must be equal to true.

2292
02:05:38,960 --> 02:05:41,080
In particular, it cannot be false.

2293
02:05:41,840 --> 02:05:43,360
So list must be sorted.

2294
02:05:44,600 --> 02:05:47,960
The arrow relation is defined
by the term writing system

2295
02:05:47,960 --> 02:05:50,400
translating from the input program.

2296
02:05:52,400 --> 02:05:56,400
As we have seen earlier, regular
languages can be seen as types.

2297
02:05:57,640 --> 02:06:01,280
So we then translate our problem
into a type checking problem

2298
02:06:01,280 --> 02:06:04,320
where we need to check that
the following term is well typed

2299
02:06:04,320 --> 02:06:06,200
using regular language types.

2300
02:06:07,200 --> 02:06:11,680
The overall target type is a regular
language containing only the term true.

2301
02:06:12,680 --> 02:06:14,080
Note that to check that

2302
02:06:14,080 --> 02:06:18,720
we will need to infer the appropriate
intermediate type for sort L

2303
02:06:18,720 --> 02:06:20,040
since it is not given.

2304
02:06:21,800 --> 02:06:25,360
From the partially typed given
input there on the right.

2305
02:06:25,360 --> 02:06:28,320
the type checking algorithm
is defined as follows.

2306
02:06:29,080 --> 02:06:31,000
First we extract
the type signature

2307
02:06:31,000 --> 02:06:33,000
for the top most symbol, sorted.

2308
02:06:33,760 --> 02:06:36,160
We know the expected
output type, true.

2309
02:06:36,920 --> 02:06:40,800
Constant values such as some
comparison functions, CMP

2310
02:06:40,800 --> 02:06:42,120
are typed with themselves

2311
02:06:42,120 --> 02:06:45,360
and can be reported
directly on the signature.

2312
02:06:45,840 --> 02:06:47,720
We complete the blanks
in the signature

2313
02:06:47,720 --> 02:06:50,120
using our type inference procedure.

2314
02:06:50,120 --> 02:06:52,520
We find that to output true,

2315
02:06:52,520 --> 02:06:56,400
the list must be sorted which is
represented by the regular language

2316
02:06:56,400 --> 02:06:57,400
named sorted.

2317
02:06:57,880 --> 02:06:59,880
We propagate back what we have found

2318
02:06:59,880 --> 02:07:02,840
and check that it matches
the initial type information

2319
02:07:03,360 --> 02:07:05,040
and then we repeat,
the same procedure

2320
02:07:05,040 --> 02:07:06,600
with the subterm.

2321
02:07:06,600 --> 02:07:08,440
Here we need to check, sort l.

2322
02:07:08,960 --> 02:07:11,200
We again extract the template
signature for sort.

2323
02:07:11,200 --> 02:07:14,600
Here we already have all
the type information about sort.

2324
02:07:14,600 --> 02:07:16,040
We just need to check it.

2325
02:07:17,080 --> 02:07:20,520
The algorithm stops, when
all the types have been found

2326
02:07:20,520 --> 02:07:23,000
or when a type occur, when
there is a contradiction.

2327
02:07:23,760 --> 02:07:27,960
Of course everything is built on
top of type inference procedure

2328
02:07:27,960 --> 02:07:31,240
which has been able to find
the regular language of sorted lists.

2329
02:07:32,000 --> 02:07:33,960
Let's see how this procedure works.

2330
02:07:33,960 --> 02:07:35,480
So the problem is as follows.

2331
02:07:36,240 --> 02:07:39,600
As input we have rewriting
system of symbol f

2332
02:07:39,600 --> 02:07:42,360
and an output regular
language type o.

2333
02:07:42,360 --> 02:07:46,000
We want to find every
input type A1 to An

2334
02:07:46,000 --> 02:07:50,360
such that every term f(t1)
to tn is typed with O

2335
02:07:50,880 --> 02:07:53,360
when each ti is typed with AI.

2336
02:07:53,360 --> 02:07:56,000
This can be summarized with
the following abstract

2337
02:07:56,000 --> 02:07:57,600
writing rule for f.

2338
02:07:57,600 --> 02:08:00,760
For instance let's consider
the following rewriting system

2339
02:08:00,760 --> 02:08:02,720
defining the recursive function

2340
02:08:02,720 --> 02:08:06,840
even deciding if
a number is even or not.

2341
02:08:06,840 --> 02:08:09,800
We want to know for which
input regular language type

2342
02:08:09,800 --> 02:08:11,160
this function returns true.

2343
02:08:11,880 --> 02:08:15,480
So here our target output
type contains only true.

2344
02:08:16,240 --> 02:08:19,360
The expected output of
the procedure is a partial abstraction

2345
02:08:19,360 --> 02:08:21,400
of the input term rewriting system.

2346
02:08:21,400 --> 02:08:22,800
Focusing on even.

2347
02:08:22,800 --> 02:08:26,600
this tells us that even returns
true for every even number.

2348
02:08:27,320 --> 02:08:30,440
The new type Even is computed
during the analysis.

2349
02:08:31,200 --> 02:08:33,320
For recursive functions
such as this one

2350
02:08:33,320 --> 02:08:36,640
the type inference is done using
an invariant learning procedure.

2351
02:08:37,640 --> 02:08:40,160
In our paper, we defined
a counter-example

2352
02:08:40,160 --> 02:08:42,680
guided regular invariant
learning procedure

2353
02:08:42,680 --> 02:08:45,600
based on tree automata
completion and SMT solving.

2354
02:08:46,120 --> 02:08:47,920
It consists in repeating four steps

2355
02:08:48,400 --> 02:08:50,840
until it finds a solution
or a counter-example.

2356
02:08:51,320 --> 02:08:52,960
Step one we experiment.

2357
02:08:53,440 --> 02:08:56,600
We evaluate some terms of
the form of f of t1 to tn

2358
02:08:56,600 --> 02:09:01,960
where f is an analyzed symbol and t1
to tn some random input values.

2359
02:09:01,960 --> 02:09:05,360
We do that efficiently using
the tree automaton completion algorithm.

2360
02:09:05,880 --> 02:09:08,600
We then observe the result
of our experiments

2361
02:09:08,600 --> 02:09:12,520
and deduce some constraints on
the language we are learning.

2362
02:09:12,520 --> 02:09:16,040
We then make an hypothesis
by using an SMT solver

2363
02:09:16,040 --> 02:09:17,560
and we solve the constraints

2364
02:09:17,560 --> 02:09:20,120
we generate an abstraction
of the program execution.

2365
02:09:20,640 --> 02:09:22,920
This gives us possible
regular language solution.

2366
02:09:23,440 --> 02:09:25,920
We check the correctness
of the abstraction

2367
02:09:25,920 --> 02:09:27,280
by checking it's completeness

2368
02:09:27,280 --> 02:09:30,080
with regards to the rewriting
system and the input domain.

2369
02:09:30,560 --> 02:09:32,000
If it is, we are done,

2370
02:09:32,480 --> 02:09:35,440
and if it isn't we continue
by doing more experiments.

2371
02:09:36,440 --> 02:09:37,840
To illustrate the procedure,

2372
02:09:37,840 --> 02:09:40,000
let's take back
the even function example.

2373
02:09:40,760 --> 02:09:44,120
First we experiment with
the values 0, 1 and 2

2374
02:09:44,120 --> 02:09:49,080
that are recognized in this
automata by this state X0, X1, X2.

2375
02:09:49,080 --> 02:09:51,200
Using the tree automaton completion

2376
02:09:51,200 --> 02:09:54,880
we know that Even of 0
and Even of 2 rewrites to true.

2377
02:09:55,360 --> 02:09:57,360
This is encoded by
the following rules.

2378
02:09:58,600 --> 02:10:00,120
If we only use this information,

2379
02:10:00,120 --> 02:10:02,560
then we cannot observe
the constraints.

2380
02:10:02,560 --> 02:10:05,280
Every tested value
so far rewrites to true.

2381
02:10:05,760 --> 02:10:08,680
As a consequence,
if everything rewrites to true

2382
02:10:08,680 --> 02:10:13,480
then there is no point in
separating X0 from X1 from X2.

2383
02:10:14,000 --> 02:10:16,560
The solution to our
anti-constraint system

2384
02:10:16,560 --> 02:10:19,560
is to merge them together
into a new state tau zero

2385
02:10:20,160 --> 02:10:22,480
By applying the substitution
in the automaton

2386
02:10:22,480 --> 02:10:25,520
we get a candidate abstraction
for rewriting system

2387
02:10:25,520 --> 02:10:28,520
where even applied to any
number returns to true.

2388
02:10:28,520 --> 02:10:30,800
Of course this does not pass the test.

2389
02:10:30,800 --> 02:10:33,160
Every input value for
Even is represented

2390
02:10:33,160 --> 02:10:36,360
but some rewriting rules
are not used in particular

2391
02:10:36,360 --> 02:10:40,480
this abstraction misses the fact
that even of 1 rewrites to false.

2392
02:10:41,200 --> 02:10:42,960
So we can not stop here.

2393
02:10:42,960 --> 02:10:46,560
To continue we add Even of
one in our experiment.

2394
02:10:46,560 --> 02:10:48,080
It rewrites to false.

2395
02:10:48,840 --> 02:10:52,680
This time we observe that in
order to separate true from false

2396
02:10:52,680 --> 02:10:55,680
we need to have X0 different from X2

2397
02:10:55,680 --> 02:10:57,560
and X1 different from X2.

2398
02:10:57,560 --> 02:10:59,960
By solving this new constant system

2399
02:10:59,960 --> 02:11:01,440
we make a new hypothesis

2400
02:11:01,440 --> 02:11:04,840
in which X0 are X2 are
equals, away from X1.

2401
02:11:05,360 --> 02:11:08,920
If we look closely, we can notice
that this new potential abstraction

2402
02:11:08,920 --> 02:11:10,800
separates even and odd numbers

2403
02:11:10,800 --> 02:11:12,720
which is exactly what we want.

2404
02:11:13,240 --> 02:11:16,120
And this time every input
value is represented.

2405
02:11:16,120 --> 02:11:18,880
And every possible
rewriting rule is used.

2406
02:11:19,640 --> 02:11:21,360
So we can stop the analysis here.

2407
02:11:21,360 --> 02:11:25,080
We have found an abstraction that
gives us the correct input type

2408
02:11:25,080 --> 02:11:26,080
for the output true.

2409
02:11:26,840 --> 02:11:29,920
Which is new language even
organizing even numbers.

2410
02:11:30,600 --> 02:11:32,680
We have proved that this
procedure is regularly

2411
02:11:33,360 --> 02:11:36,600
complete as it can
learn any regular language

2412
02:11:36,600 --> 02:11:38,760
and complete in refutation.

2413
02:11:38,760 --> 02:11:42,440
We have implemented it in Ocaml
in a new version of Timbuk

2414
02:11:42,440 --> 02:11:45,920
and tested it on more
than 80 problems.

2415
02:11:45,920 --> 02:11:48,360
When possible, we
compared the performances

2416
02:11:48,360 --> 02:11:50,960
with the previous version of Timbuk.

2417
02:11:50,960 --> 02:11:53,000
We saw that it is on average,

2418
02:11:53,000 --> 02:11:54,760
a second slower than Timbuk Three

2419
02:11:54,760 --> 02:11:56,480
which can be explained
by an overhead

2420
02:11:56,480 --> 02:11:59,000
due to our typing facility.

2421
02:11:59,000 --> 02:12:02,680
However, in the subset of problems
on which we could compare,

2422
02:12:02,680 --> 02:12:07,080
our implementation could
handle 16% more problems

2423
02:12:07,080 --> 02:12:09,720
without timing out.

2424
02:12:09,720 --> 02:12:13,720
Surprisingly, the greatest
improvement is in the memory usage

2425
02:12:13,720 --> 02:12:15,320
where in the worst case,

2426
02:12:15,320 --> 02:12:19,000
Timbuk Three would consume
several gigabytes of memory

2427
02:12:19,000 --> 02:12:20,320
to solve some problems

2428
02:12:20,320 --> 02:12:24,240
where we never got
passed 40 megabytes.

2429
02:12:24,240 --> 02:12:27,920
This is mostly due to our
modular type system approach

2430
02:12:27,920 --> 02:12:32,560
that allows us to split
the problem into smaller parts.

2431
02:12:32,560 --> 02:12:35,640
Thank you for listening
and don't hesitate to download

2432
02:12:35,640 --> 02:12:38,640
the source code which is
available on our GitHub

2433
02:12:38,640 --> 02:12:40,120
at the address below.

2434
02:12:40,120 --> 02:12:45,120
(AUDIENCE APPLAUDS)

2435
02:12:47,840 --> 02:12:50,720
RICHARD: OK, thanks, Timothee,

2436
02:12:50,720 --> 02:12:54,360
who's available in the New York
time band for question and answer.

2437
02:12:54,360 --> 02:12:58,680
We are now down with Technical
Session number five of ICFP.

2438
02:12:58,680 --> 02:13:02,720
In the New York time band, we have
an ICFP coffee break coming up

2439
02:13:02,720 --> 02:13:07,720
and yeah, at 1:30, there's
a virtualization feedback meeting

2440
02:13:08,880 --> 02:13:10,600
with Benjamin Pierce, Jonathan Bell,

2441
02:13:10,600 --> 02:13:11,920
and Krista Lopez coming up.

2442
02:13:11,920 --> 02:13:14,160
So that's an exciting thing to join.

2443
02:13:14,160 --> 02:13:17,120
In the Asia time band
in one half hour,

2444
02:13:17,120 --> 02:13:18,520
there will be a session on mentoring

2445
02:13:18,520 --> 02:13:20,320
with Mathias Felleisen.

2446
02:13:20,320 --> 02:13:25,320
Thanks very much and I hope
you enjoy the rest of ICFP.

2447
02:14:30,600 --> 02:14:35,600
(UPBEAT FOLK MUSIC)

2448
02:17:12,360 --> 02:17:17,360
(INDIE ROCK MUSIC)

2449
02:20:52,840 --> 02:20:57,840
(ELECTRONIC POP MUSIC)

2450
02:25:38,320 --> 02:25:43,320
(ROCK MUSIC)

